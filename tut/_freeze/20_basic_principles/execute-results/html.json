{
  "hash": "f711a80d0b50b08704bacef94ccfe8d7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Basic statistical principles\nauthor: \"Murray Logan\"\ndate: \"16 July, 2024\"\nformat: \n  html:\n    toc: true\n    toc-float: true\n    page-layout: full\n    number-sections: true\n    number-depth: 3\n    embed-resources: true\n    code-fold: false\n    code-tools: true\n    code-summary: \"Show the code\"\n    code-line-numbers: true\n    code-block-border-left: \"#ccc\"\n    code-copy: true\n    highlight-style: atom-one\n    theme: [default, ../resources/tut-style.scss]\n    css: ../resources/tut-style.css\ncrossref:\n  fig-title: '**Figure**'\n  fig-labels: arabic\n  tbl-title: '**Table**'\n  tbl-labels: arabic\nengine: knitr\nbibliography: ../resources/references.bib\noutput_dir: \"docs\"\n---\n\n\n\n# Statistics\n\nStatistics is a branch of mathematical sciences that relates to the\ncollection, analysis, presentation and interpretation of data and is\ntherefore central to most scientific pursuits. Fundamental to\nstatistics is the concept that samples are collected and statistics\nare calculated to _estimate_ _populations_ and their\n_parameters_.\n\nStatistical populations can represent natural biological populations\n(such as the Victorian koala population), although more typically they\nreflect somewhat artificial constructs (e.g. Victorian male koalas). A\n**statistical population** strictly refers to all the possible\nobservations from which a sample (a subset) can be drawn and is the\nentity about which you wish to make conclusions.\n\nThe **population parameters** are the characteristics (such as\npopulation mean, variability etc) of the population that we are\ninterested in drawing conclusions about. Since it is usually not\npossible to observe an entire population, the population parameters\nmust be **estimated** from corresponding statistics calculated from a\nsubset of the population known as a sample (e.g sample mean,\nvariability etc). Provided the sample adequately represents the\npopulation (is sufficiently large and unbiased), the sample statistics\nshould be reliable estimates of the population parameters of interest.\n\nIt is primarily for this reason that most statistical procedures\nimpose certain sampling and distributional assumptions on the\ncollected data. For example, most statistical tests assume that the\nobservations have been drawn randomly from populations (to maximize\nthe likelihood that the sample will truly represent the population).\nAdditional terminology fundamental to the study of ecological\nstatistics are listed in the following table (in which the examples\npertain to a hypothetical research investigation into estimating the\nprotein content of koala milk).\n\n| Term          | Definition                                                                                                                                                                                 | Example                                                                                                                                                                                                                                                                                                        |\n|---------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| _Measurement_ | A single piece of recorded information reflecting a characteristic of interest (e.g. length of a leaf, pH of a water aliquot mass of an individual, number of individuals per quadrat etc) | Protein content of the milk of a single female koala                                                                                                                                                                                                                                                           |\n| _Observation_ | A single measured sampling or experimental unit (such as an individual, a quadrat, a site etc)                                                                                             | A small quantity of milk from a single koala                                                                                                                                                                                                                                                                   |\n| _Population_  | All the possible observations that could be measured and the unit of which wish to draw conclusions about (note a statistical population need not be a viable biological population)       | The milk of all female koalas                                                                                                                                                                                                                                                                                  |\n| _Sample_      | The (representative) subset of the population that are observed                                                                                                                            | A small quantity of milk collected from 15 captive female koalas.Note that such a sample may not actually reflect the defined population. Rather, it could be argued that such a sample reflects captive populations. Nevertheless, such extrapolations are common when field samples are difficult to obtain. |\n| _Variable_    | A set of measurements of the same type that comprise the sample. The characteristic that differs (varies) from observation to observation                                                  | The protein content of koala milk.                                                                                                                                                                                                                                                                             |\n\n: {.primary .bordered .sm .paramsTable tbl-colwidths=\"[20,50,30]\"}\n\n\nIn addition to estimating population parameters, various statistical\nfunctions (or _statistics_) are often calculated to express the\nrelative magnitude of trends within and between populations. For\nexample, the degree of difference between two populations is usually\ndescribed in classic frequentist statistics by a _t_-statistic.\n\nAnother important concept in statistics is the idea of _probability_.\nThe frequentist view of the probability of an event or outcome is the\nproportion of times that the event or outcome is expected to occur in\nthe long-run (after a large number of repeated sampling events). For\nmany statistical analyses, probabilities of occurrence are used as the\nbasis for conclusions, inferences and predictions.\n\nConsider the vague research question \"How much do Victorian male\nkoalas weigh?\". This could be interpreted as:\n\n- How much do each of the Victorian male koalas weigh individually?\n- What is the total mass of all Victorian male koalas added together?\n- What is the mass of the typical Victorian male koala?\n\nArguably, it is the last of these questions that is of most interest.\nWe might also be interested in the degree to which these weights\ndiffer from individual to individual and the frequency of individuals\nin different weight classes.\n\n\n# Probability theory\n\nProbability (the chance of a particular outcome per event) can be\nconsidered from two different perspectives;\n\n- as an objective representation of the relative frequency of times\n  that the outcome occurs from a long series (infinite) of events.\n  Hence, it can be calculated by counting the number of times that the\n  outcome occurs (the frequency) divided (normalized) by the total\n  number of events (the sample space) in which it could have occurred.\n  In order to relate this back to a hypothesis, we typically estimate\n  the expected frequencies of outcomes when the null hypothesis is\n  true. We will return to why there is a focus on a null hypothesis\n  rather than a hypothesis a little later.\n- as a somewhat subjective representation of the uncertainty of an\n  outcome. That is, how reasonable is an outcome given our previous\n  understandings and the newly observed data.\n\nThese two approaches differ substantially in their interpretation of\nprobability (long-run chances of outcomes under certain conditions vs\ndegree of belief). This can be represented diagrammatically. In simple\nprobability, the probability of an outcome (e.g. $P(A)$) is expressed\nrelative to a broad sample space.\n\n\n\n\n\n\n\n\n<table>\n<tbody>\n<tr>\n<td width=\"250px\">\n\nThe Probability of outcome A is the frequency of times outcome A\noccurs divided by the total number of times the outcome could occur\n(the sample space). The open symbols represent alternative outcomes.\n\n</td>\n<td width=\"200px\">\n\\begin{align*}\nP(A) &= \\frac{freq(A)}{freq(Total)}\\\\\nP(A) &= \\frac{5}{22}\\\\\n&= 0.227\n\\end{align*}\n</td>\n<td width=\"225px\">\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20_basic_principles_files/figure-html/probA-1.png){width=384}\n:::\n:::\n\n\n</td>\n</tr>\n\n<tr>\n<td>\nThe Probability of outcome B is the frequency of times outcome B occurs divided by the total number of times the outcome could occur (the sample space).\n</td>\n<td>\n\\begin{align*}\nP(B) &= \\frac{freq(B)}{freq(Total)}\\\\\nP(B) &= \\frac{7}{22}\\\\\n&= 0.318\n\\end{align*}\n</td>\n<td>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20_basic_principles_files/figure-html/probB-1.png){width=384}\n:::\n:::\n\n\n</td>\n</tr>\n<tr>\n<td>\nThe Probability of both outcome A AND outcome B is the frequency of times outcome A AND outcome B both occur together divided by the total number of times the outcome could occur (the sample space).\n</td>\n<td>\n\\begin{align*}\nP(AB) &= \\frac{freq(A\\&B)}{freq(Total)}\\\\\nP(AB) &= \\frac{2}{22}\\\\\n&= 0.091\n\\end{align*}\n</td>\n<td>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20_basic_principles_files/figure-html/probAUB-1.png){width=384}\n:::\n:::\n\n\n</td>\n</tr>\n\n</tbody>\n</table>\n\n\nConditional probability on the other hand, establishes the probability\nof a particular event conditional to (given the occurrence of) another\nevent and therefore alters the divisor sample space. The sample space\nis restricted to the occurrence of the unconditional outcome.\n\n<table>\n<tbody>\n<tr>\n<td width=\"250px\">\n\nThe probability of outcome A occurring given that outcome B also\noccurs (or has occurred) is the frequency of times that outcome A AND\noutcome B both occur divided by the frequency of times that outcome B\noccurs. The frequency of outcome B occurrences becomes the divisor.\n\n</td>\n<td width=\"200px\">\n\\begin{align*}\nP(A|B) &= \\frac{freq(A\\&B)}{freq(B)}\\\\\nP(A|B) &= \\frac{2}{7}\\\\\n&= 0.286\n\\end{align*}\n</td>\n<td width=\"225px\">\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20_basic_principles_files/figure-html/probA|B-1.png){width=384}\n:::\n:::\n\n\n</td>\n</tr>\n</tbody>\n</table>\n\nThe above representation of conditional probability can be expressed\ncompletely in terms of probability\n\n\\begin{align*}\nP(A|B) &= \\frac{freq(A\\&B)}{freq(B)}\\Leftrightarrow \\frac{P(AB)\\times freq(Total)}{P(B)\\times freq(Total)}\\\\\n&= \\frac{P(AB)}{P(B)}\n\\end{align*}\n\nMost probability statements take place in the context of a hypothesis\n(nor hypothesis). For example, frequentist probability is the\nprobability of the data given the null hypothesis. Hence, most\ninferential statistics involve conditional probability.\n\n# Distributions\n\nThe set of observations in a sample can be represented by a _sampling_\nor _frequency distribution_. A frequency distribution (or just\ndistribution) represents how often observations in certain ranges\noccur. For example, how many male koalas in the sample weigh between\n10 and 11kg, or how many weigh more than 12kg. Such a sampling\ndistribution can also be expressed in terms of the probability\n(long-run likelihood or chance) of encountering observations within\ncertain ranges.\n\nProbability distributions are also know as density distributions and\ntheir mathematical representations are known as density functions. For\ndiscrete outcomes (integers, such as the number of eggs laid by female\nsilver gulls [range from 0-8]), the density represents the frequency\nof a certain outcome (clutch size) divided by the total number of\nobservations (examined clutches). The following figures represent the\nfrequency (left) and density (right) of clutch sizes from 100 nests.\n\n::: {.columns}\n\n:::: {.column width=\"45%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20_basic_principles_files/figure-html/discreteDist-1.png){width=384}\n:::\n:::\n\n\n\n::::\n\n:::: {.column width=\"45%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20_basic_principles_files/figure-html/discreteDist1-1.png){width=384}\n:::\n:::\n\n\n\n::::\n:::\n\nFor continuous outcomes however, it is highly likely that all the\nobserved (sample) values are unique (at least in theory) and therefore\nall outcomes have a frequency of exactly 1. As the sample size\napproaches infinity, the probability of any single point value\ntherefore approaches zero.\n\nSo we instead break the continuum into small equal-sized chunks and\ncalculate the frequency of values within each chunk or bin (akin to a\nhistogram). To normalize these data such that the histogram represents\nan area of exactly one (necessary to be considered a probability\ndistribution), we divide by the chunk width.\n\nClearly the accuracy of the density (probability) will depend on the\nsize of the chunk selected. The smaller the chunk, the greater the\naccuracy. Alternatively, integrating the density function produces an\nexact solution. Probability from continuous distributions is thence\nbased on areas under the density function and is undefined for a\nsingle point along the curve.\n\nFor example, the probability of encountering a male koala weighing\nmore than 12kg is equal to the proportion of male koalas in the sample\nthat weighed greater than 12kg. It is then referred to as a\n**probability distribution**.\n\nWhen a frequency distribution can be described by a mathematical\nfunction, the probability distribution is a curve. The total area\nunder this curve is defined as 1 and thus, the area under sections of\nthe curve represent the probability of values falling in the\nassociated interval. Note, it is not possible to determine the\nprobability of discrete events (such as the probability of\nencountering a koala weighing 12.183kg) only ranges of values. Well,\nit is possible, it is just that it is infinitesimally small and\nmeaningless.\n\n## Continuous distributions\n\n### The normal (Gaussian) distribution\n\nIt has been a long observed mathematical phenomenon that the\naccumulation of a very large set of independent random influences tend\nto converge upon a central value (**central limit theorem**) and that\nthe distribution of such accumulated values follow a specific \"bell\nshaped\" curve called a _normal_ or _Gaussian_ distribution. The normal\ndistribution is a symmetrical distribution in which values close to\nthe center of the distribution are more likely and that progressively\nlarger and smaller values are less commonly encountered.\n\n:::: {.column width=\"48%\"}\n\n$$f(x;\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\left(\\frac{x-\\mu}{2\\sigma}\\right)^2}$$\n\nAt first, this might appear to be a very daunting formula. It\nessentially defines the density (frequency) of any value of $x$. The\nexact shape of the distribution is determined by just two parameters:\n\n- **$\\mu$ - the mean**. This defines the center of the distribution,\n  the location of the peak.\n- **$\\sigma^2$ - the variance** (or $\\sigma$, the standard deviation)\n  which defines the variability or spread of values around the mean.\n\n::::\n\n:::: {.column width=\"48%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20_basic_principles_files/figure-html/GaussianDistribution-1.png){width=4in}\n:::\n:::\n\n\n::::\n\n\nImportant properties of the Gaussian distribution:\n\n- There is no relationship between the distributions mean (location)\n  and variance - they are independent of one another.\n- It is symmetric and unbounded and thus defined for all real numbers\n  in the range of ($-\\infty$, $\\infty$).\n- Governed by central limits theorem\n  - averages tend to converge to a central limit\n\nAs many biological measurements (such as weights, lengths etc) are\ninfluenced by an almost infinite number of factors (many of which can\nbe considered independent and random), many biological variables also\nfollow a Gaussian distribution. The Gaussian distribution is\nparticularly well suited for representing the distribution variables\nwhose values are either\n\n- considerably larger (or smaller) than zero (e.g. koalas mass) or\n- have no theoretical limits (e.g. difference in masses between\n  sibling fledglings)\n\nEven discrete responses (such as counts that can only logically be\npositive integers) can occasionally be approximately described by a\nGaussian distribution, particularly if either the samples are very\nlarge and the values free from boundary conditions (such as being\nclose to a lower limit of 0), or else we are dealing with average\ncounts.\n\nSince many scientific variables behave according to the central limit\ntheorem, many of the common statistical procedures have been\nspecifically derived for (and thus assume) that the underlying\ndistribution from which the data are drawn is Gaussian. Specifically,\nparameter estimation, inference and hypothesis tests from simple\nparametric tests (regression, ANOVA etc) assume that the residuals\n(stochastic, unexplained components of data) are normally distributed\naround a mean of zero. The reliability of such tests is dependent on\nthe degree of conformity to this assumption of normality. Likewise,\nmany other statistical elements rely on normal distributions, and thus\nthe normal distribution (or variants thereof) is one of the most\nimportant mathematical distributions.\n\n### Log-normal distribution\n\nMany biological variables have a lower limit of zero (at least in\ntheory). For example, a koala cannot weigh less than 0kg or there\ncannot be less than 0mm of rain in a month. Such circumstances can\nresult in asymmetrical distributions that are highly truncated towards\nthe left with a long right tail.\n\nIn such cases, the mean and median present different values (the\nlatter arguably more reflective of the 'typical' value). These\ndistributions can often be described by a log-normal distribution.\nFurthermore, some variables do not naturally vary on a linear scale.\nFor example, growth rates or chemical concentrations might naturally\noperate on logarithmic or exponential scales. Consequently, when such\ndata are collected on a linear scale, they might be expected to follow\na non-normal (perhaps log-normal) distribution.\n\n:::: {.column width=\"48%\"}\n\n$$f(x;\\mu,\\sigma) = \\frac{1}{x\\sigma\\sqrt{2\\pi}} e^{-\\left(\\frac{ln x-\\mu}{2\\sigma^2}\\right)^2}$$\n\nAs with the Gaussian distribution, the exact shape of the log-normal distribution is determined by just two parameters:\n\n- **$\\mu$ - the mean**. This defines the center of the distribution,\n  the location of the peak.\n- **$\\sigma^2$ - the variance** (or $\\sigma$, the standard deviation)\n  which defines the variability or spread of values around the mean.\n\nHowever, $\\mu$ and $\\sigma^2$ are the mean and variance of $ln(x)$\nrather than $x$.\n\n::::\n\n:::: {.column width=\"48%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20_basic_principles_files/figure-html/Log_normalDistribution-1.png){width=4in}\n:::\n:::\n\n\n\n::::\n\nImportant properties of the log-normal distribution:\n\n- The variance is related (proportional) to the mean ($\\sigma^2 \\sim\n  \\mu^2$)\n- The log-normal distribution is skewed to the right as a result of\n  being bounded at 0, yet unbounded to the right ($0$, $\\infty$)\n- Also governed by central limits theorem except that it describes the\n  distribution of values that are the product (rather than sum) of a\n  large number of independent random factors.\n\n### t-distribution\n\nThe t-distribution, also known as the Student's t-distribution, is a\nprobability distribution that is similar to the standardised normal\ndistribution (mean of 0, standard deviation of 1) however it is better\nsuited for smaller sample sizes. It is characterized by its\nbell-shaped curve and heavier tails, making it suitable for modeling\ndata that deviates from normality. The t-distribution is often\nemployed in hypothesis testing and confidence interval estimation when\ndealing with small sample sizes or when the population standard\ndeviation is unknown. It provides a robust alternative to the normal\ndistribution in situations where the underlying data exhibit skewness\nor constraints.\n\n:::: {.column width=\"48%\"}\n$$\nf(t; \\mu, \\nu) = \\frac{\\Gamma(\\frac{\\nu + 1}{2})}{\\sqrt{\\pi \\nu} \\Gamma(\\frac{\\nu}{2})} \\left( 1 + \\frac{(t - \\mu)^2}{\\nu} \\right)^{-\\frac{\\nu + 1}{2}}\n$$\n\nWhere:\n\n- $f(t; \\mu, \\nu)$ represents the probability density function of the t-distribution.\n- $\\mu$ is the location parameter (mean).\n- $\\nu$ is the degrees of freedom parameter, which controls the shape of the distribution (fatter tails with lower $\\nu$).\n- $\\Gamma(⋅)$ is the gamma function.\n\nThis formula resembles the Gaussian distribution but includes an\nadditional term involving the degrees of freedom and a different power\nin the exponent. This difference reflects the heavier tails of the\nt-distribution compared to the bell-shaped normal distribution. This\nformula describes the shape of the t-distribution, which converges to\nthe standard normal distribution as the degrees of freedom increase.\n\n::::\n\n:::: {.column width=\"48%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20_basic_principles_files/figure-html/tDistribution-1.png){width=4in}\n:::\n:::\n\n\n::::\n\n\n### Gamma distribution\n\nThe Gamma distribution describes the distribution of _waiting\ntimes_ until a specific number of independent events (typically\ndeaths) have occurred. For example, if the average mortality rate is\none individual per five days (rate=1/5 or scale=5), then a Gamma\ndistribution could be used to describe the distribution of expected\nwaiting time before 10 individuals were dead.\n\n:::: {.column width=\"48%\"}\n\nThere are two parameterizations of the Gamma distribution\n\n- in terms of shape ($k$) and scale ($\\theta$)\n\n$$\nf(x;k,\\theta) = \\frac{1}{\\theta^k}\\frac{1}{\\gamma(k)}x^{k-1}e^{-\\frac{x}{\\theta}}\\\\\n\\text{for}~x\\gt 0~\\text{and}~k,\\theta\\gt 0\n$$\n\n- in terms of shape ($\\alpha$) and rate ($\\beta$)\n\n    $$\n    f(x;\\alpha,\\beta) = \\beta^\\alpha\\frac{1}{\\gamma(\\alpha)}x^{\\alpha-1}e^{-\\beta x}\\\\\n    \\text{for}~x\\gt 0~\\text{and}~\\alpha,\\beta\\gt 0\n    $$\n\n::::\n\n:::: {.column width=\"48%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20_basic_principles_files/figure-html/gammaDistribution-1.png){width=4in}\n:::\n:::\n\n\n\n::::\n\nIn addition to being used to describe the distribution of waiting\ntimes, the gamma distribution can also be used as an alternative to\nthe normal distribution when data (residuals) are skewed with a long\nright tail, such as when there is a relationship between mean and\nvariance. When such data are modeled with a normal distribution,\nillogical negative predicted values can occur. Such values are not\npossible from a Gamma distribution.\n\nThe Gamma distribution is also an important conjugate prior for the\nprecision (variance) of a normal distribution in Bayesian modeling.\nImportant properties of the Gamma distribution:\n\n- The **shape parameter** defines the number of events (for example,\n  10 deaths) and can technically be any positive number.\n  - shape values less than 1, the gamma distribution has a mode of 0\n  - shape values equal to 1, the gamma distribution is equivalent to\n    the exponential distribution\n  - shape values greater than 1, the distribution becomes increasingly\n    more symmetrical and approaches a normal distribution when the\n    shape parameter is large.\n\n- The **scale** or **rate** (rate=1/scale) parameter defines how often\n  (scale) or the rate at which events are expected to occur\n- The variance is proportional to the mean\n  ($variance=\\frac{scale}{mean}$, $variance=\\frac{mean^2}{shape}$)\n\n### Uniform distribution\n\nThe uniform distribution describes a square distribution within a\nspecific range.\n\n:::: {.column width=\"48%\"}\n$$f(x;a,b) = \\begin{cases}\n  \\frac{1}{b-a} & \\text{for } a \\leq x \\geq b,\\\\[1em]\n    0       & \\text{for } x \\lt a \\text{ or } x \\gt b\n\\end{cases}\n$$\n\n::::\n\n:::: {.column width=\"48%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20_basic_principles_files/figure-html/uniformDistribution,-1.png){width=4in}\n:::\n:::\n\n\n\n::::\n\nImportant properties of the uniform distribution:\n\n- Has a constant probability density within the range $a\\le x\\ge b$ of\n  $\\frac{1}{b-a}$ and zero outside of this range\n- Whilst this distribution is rarely employed in frequentist\n  statistics, it is occasionally used as an improper prior\n  distribution in Bayesian modeling.\n\n\n### Exponential distribution\n\nThe exponential distribution describes the distribution of waiting\ntimes for the occurrence a single discrete event (such as an\nindividual death) given a constant rate (probability of occurrence per\nunit of time) - for example, describing longevity or the time elapsed\nbetween events (such as whale sightings). It is also useful for\ndescribing the distribution of measurements that naturally attenuate\n(exponentially) such as light levels penetrating to increasing water\ndepths.\n\n:::: {.column width=\"48%\"}\n\n$$f(x;\\lambda) = \\lambda e^{-\\lambda x}$$\nThe uniform distribution is defined by a single parameter:\n\n- **$\\lambda$ - the rate**. The rate at which the event is expected to\n  occur. The larger the rate, the steeper the curve.\n\n::::\n\n:::: {.column width=\"48%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20_basic_principles_files/figure-html/exponentialDistribution,-1.png){width=4in}\n:::\n:::\n\n\n\n::::\n\nImportant properties of the uniform distribution:\n\n- It is bounded by 0 on the left and limitless on the right ($0$,\n  $\\infty$).\n- The mean and variance are both related to the rate\n  ($variance=\\frac{1}{\\lambda^2}$, $mean=\\frac{1}{\\lambda}$)\n\n### Beta distribution\n\nThe beta distribution describes the probability of success in a\nbinomial trial is the only continuous distribution defined within the\nrange that is bound at both ends ($0-1$). As it operates in the range\nof $0-1$, it is ideal for modeling proportions and percentages.\nHowever, it is also useful for modeling other continuous quantities on\na finite scale. The values are <i>transformed</i> (see <a\nhref=\"#Transformations\">Transformations</a>) from the arbitrary finite\nscale to the $0-1$ scale, modeling with a beta distribution and\nfinally the parameters are back-transformed into the original scale.\n\n:::: {.column width=\"48%\"}\n\n$$f(x;a,b) = \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}x^{a-1}(1-x)^{b-1}$$\n\nThe beta distribution is defined by two shape parameters:\n\n- **$a$ - shape parameter 1**. Number of successes in binomial trial\n  ($a-1$)\n- **$b$ - shape parameter 1**. Number of successes in binomial trial\n  ($b-1$)\n                  \n::::\n\n:::: {.column width=\"48%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20_basic_principles_files/figure-html/betaDistribution,-1.png){width=4in}\n:::\n:::\n\n\n::::\n\nThe beta distribution is also a conjugate prior for binomial, Bernoulli and geometric distributions.\nImportant properties of the beta distribution:\n\n- It is bounded by 0 on the left and limitless on the right ($0$,\n  $\\infty$).\n- When $a=b$, the distribution is symmetric about $x=0.5$\n- When $a=b=1$, the distribution is a uniform distribution with $a=0$\n  and $b=1$.\n- The location of the peak shifts towards 0 as $a<b$ and shifts\n  towards 1 as $a>b$.\n- The variance of the distribution is inversely proportional to the\n  total of $a+b$ (the number of trials).\n\n\n## Discrete distributions\n\n### Binomial distribution\n\nThe binomial distribution describes the number of '_successes_' out of\na total of $n$ independent trials each with a set probability. On any\ngiven trial, only two possible outcomes (binary) are possible (0 and\n1) - that is it is a _Bernoulli trial_. Importantly, the binomial\ndistribution is bounded at both ends - zero to the left and the trial\nsize on the right. Typical binomial include:\n\n- the number of surviving individuals from a pool of individuals\n- the number of infected individuals from a pool of individuals\n- the number of items of a particular class (e.g. males) from a pool\n  of items\n\n:::: {.column width=\"48%\"}\n\n$$f(x;n,p) = \\left(\\begin{array}{c}\nn\\\\x\n\\end{array}\\right)p^{x}(1-p)^{n-x}$$\n\nThe binomial distribution is defined by two shape parameters:\n\n- **$n$** - the total number of trials\n- **$p$** - the probability of success on any given trial. Defined\n  as any real number between 0 and 1.\n\nThe $\\left(\\begin{array}{c} n\\\\x \\end{array}\\right)$ component is a\n_normalizing constant_ that defines the number of ways of drawing $x$\nitems out of $n$ trials and also ensures that all probabilities add up\nto 1.\n\n::::\n\n:::: {.column width=\"48%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20_basic_principles_files/figure-html/binomialDistribution,-1.png){width=4in}\n:::\n:::\n\n\n\n::::\n\nImportant properties of the binomial distribution:\n\n- It is bounded by 0 on the left and by $n$ (the number of\n  trials/individuals/quadrats etc) on the right ($0$, $n$).\n- Variance is proportional to $n$ and related to the mean in that the\n  larger the sample size, the larger the variance.\n- Variance is greatest when $p=0.5$ and decreases as $p$ approaches 0\n  or 1.\n- When $n$ is large and $p$ is away from 0 or 1, the binomial\n  distribution approaches a normal distribution\n- When $n$ is large and $p$ is small, the binomial distribution\n  approaches a Poisson distribution/li>\n\n### Poisson distribution\n\nThe poisson distribution describes the number (counts) of independent\ndiscrete items or events (individuals, times, deaths) recorded for a\ngiven effort. The poisson distribution is defined by a single\nparameter ($\\lambda$) that describes the expected count (mean) as well\nas the variance in count. The poisson distribution is bounded at the\nlower end by zero, yet theoretically unbounded at the upper end\n($0$,$\\infty$).\n  \nThe poisson distribution is particularly appropriate for modeling\ncount data as they are always truncated at zero, have no upper limit\nand tend to get more variable with increasing mean.\n\n:::: {.column width=\"48%\"}\n\n$$f(x;\\lambda) = \\frac{e^{-\\lambda}\\lambda^x}{x!}$$\n\nThe poisson distribution is defined by a single parameter:\n\n- **$\\lambda$** - the expected value\n\n::::\n\n:::: {.column width=\"48%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20_basic_principles_files/figure-html/poissonDistribution,-1.png){width=4in}\n:::\n:::\n\n\n\n::::\n\nImportant properties of the binomial distribution:\n\n- It is bounded by 0 on the left and unbounded on the right ($0$, $\\infty$).\n- Mean and variance are both equal to $\\lambda$.\n- Assumes that the ratio of variance to mean (**Dispersion**) is 1 ($D=\\frac{var}{mean}=1$)\n- When $\\lambda$ is large, the binomial distribution approaches a normal distribution\n\n\n## Negative binomial distribution\n\nThe negative binomial distribution describes the expected number of\nfailures out of a sequence of $n$ independent trials before a success\nis obtained each with a set probability (typically 0.5). The negative\nbinomial is a useful alternative to the poisson distribution for\nmodeling count data for which the variance is greater than the mean\n(e.g. **overdispersed**, particularly when caused by a\nheterogeneous/patchy/clumped response). The negative binomial\ndistribution is bounded at the lower end by zero, yet theoretically\nunbounded at the upper end ($0$,$\\infty$).\n  \n:::: {.column width=\"48%\"}\n\nThere are two parameterizations of the Gamma distribution\n\n- in terms of the size ($n$) and probability ($p$)\n\n    $$f(x;n,p) = \\frac{(n+x-1)!}{(n-1!)x!}p^{n}(1-p)^x$$\n    \n    - **$n$** - the number of successes to occur before stopping the\n      count of failures. $n$ acts as a stopping point in that the\n      number of failures are counted until $n$ successes are\n      encountered.\n    - **$p$** - the probability of success of any single trial.\n\n  - in terms of mean $\\mu=n(1-p)/p$) and _overdispersion parameter_ or\n    _scaling factor_ ($\\omega$). This parameterization is more\n    meaningful in ecology. \n    \n    $$f(x;\\mu,\\omega) =\n    \\frac{\\Gamma(\\omega+x)}{\\Gamma(\\omega)x!}\\frac{(\\mu^x\\omega^\\omega}{(\\mu+\\omega)^{\\mu+\\omega}}$$\n\n    - **$\\mu$** - the mean (expected number of failures).\n    - **$\\omega$** - the dispersion or scaling factor.\n \n::::\n\n:::: {.column width=\"48%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20_basic_principles_files/figure-html/negativebinomialDistribution,-1.png){width=4in}\n:::\n:::\n\n\n\n::::\n\nImportant properties of the negative binomial distribution:\n\n- It is bounded by 0 on the left and unbounded on the right ($0$,\n  $\\infty$).\n- The variance is related to the mean ($\\sigma^2=\\mu+\\mu^2/\\omega$) -\n  variance increases with increasing mean.\n\n\n# Scale transformations\n\nThe above section on distributions illustrate the main distributions\nthat are useful in ecology. Provided data have been collected in an\nunbiased manner and from well defined populations, data usually follow\none of the above distributions. When data do not comply well to one of\nthe above distributions, it is often possible to transform the scale\nof those data so that they may be better approximated by one of these\ndistributions. For example, data measured on a percentage scale of 0\nto 100 could be easily transformed into a scale of 0-1 (for a beta\ndistribution), by dividing the observations by 100.\n\nEssentially, data transformation is the process of converting the\nscale in which the observations were measured into another scale. I\nwill demonstrate the principles of data transformation with two simple\nexamples. Firstly, to illustrate the legitimacy and commonness of data\ntransformations, imagine you had measured water temperature in a large\nnumber of streams. Let's assume that you measured the temperature in\n$\\,^{\\circ}\\mathrm{C}$. Supposing later you required the temperatures\nbe in $\\,^{\\circ}\\mathrm{F}$. You would not need to re-measure the\nstream temperatures. Rather, each of the temperatures could be\nconverted from one scale ($\\,^{\\circ}\\mathrm{C}$) to the other\n($\\,^{\\circ}\\mathrm{F}$). Such transformations are very common.\n\nImagine now that a botanist wanted to examine the leaf size of a\nparticular species. The botanist decides to measure the length of a\nrandom selection of leaves using a standard linear, metric ruler and\nthe distribution of sample observations are illustrated in the upper\nleft hand figure of the following.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20_basic_principles_files/figure-html/Transforms1-1.png){width=672}\n:::\n:::\n\n\n\nThe growth rate of leaves might be expected to be greatest in small\nleaves and decelerate with increasing leaf size. That is, the growth\nrate of leaves might be expected to be logarithmic rather than linear.\nAs a result, the distribution of leaf sizes using a linear scale might\nalso be expected to be non-normal (log-normal). If, instead of using a\nlinear scale, the botanist had used a logarithmic ruler, the\ndistribution of leaf sizes may have been more like that depicted in\nthe figure in the upper right corner.\n\nIf the distribution of observations is determined by the scale used to\nmeasure of the observations, and the choice of scale (in this case the\nruler) is somewhat arbitrary (a linear scale is commonly used because\nwe find it easier to understand), then it is justifiable to convert\nthe data from one scale to another after the data has been collected\nand explored. It is not necessary to re-measure the data in a\ndifferent scale. Therefore, to normalize the data, the botanist can\nsimply convert the data to logarithms.\n\nThe important points in the process of transformations are;\n\n- The order of the data has not been altered (a large leaf measured on\n  a linear scale is still a large leaf on a logarithmic scale), only\n  the spacing of the data has changed\n- Since the spacing of the data is purely dependent on the scale of\n  the measuring device, there is no reason why one scale is more\n  correct than any other scale\n- For the purpose of normalization, data can be converted from one\n  scale to another\n\nThe purpose of scale transformation is purely to normalize the data so\nas to satisfy the underlying assumptions of a statistical analysis. As\nsuch, it is possible to apply any function to the data. Nevertheless,\ncertain data types respond more favourably to certain transformations\ndue to characteristics of those data types. Common transformations\ninto an approximate normal distribution as well as the R syntax are\nprovided in the following table.\n\n\n\n| Nature of the data                   | Transformation           | R syntax               |\n|--------------------------------------|--------------------------|------------------------|\n| Measurements (lengths, weights, etc) | $log_e$ (natural log)    | `log(x)`               |\n|                                      | $log_{10}$ (log base 10) | `log(x, 10)`           |\n|                                      |                          | `log10(x)`             |\n|                                      | $log x+1$                | `log(x+1)`             |\n| Counts (number of individuals etc)   | $\\sqrt{~}$               | `sqrt(x)`              |\n| Percentages (must be proportions)    | $arcsin$                 | `asin(sqrt(x))*180/pi` |\n\n: {.primary .bordered .sm .paramsTable tbl-colwidths=\"[50,30,20]\"}\n\n\n::: {.callout-warning collapse=\"false\"}\n\nWhilst scale transformations of the kind outlined above are\nlegitimate, in general it is preferable that appropriate distributions\nbe selected for modelling rather than transforming data to adhere to\nspecific distributional requirements. For example, it is arguably more\nappropriate to model against a lognormal distribution than to model a\ngaussian distribution on log-transformed data.\n\nConsider the common yesteryear practice of log (or worse, square root)\ntransformation of count data (which are often skewed with a long right\ntail) to satisfy normality and homogeneity of variance assumptions of\nclassic statistical tests. A statistical model itself should reflect\nthe expected underlying data generation process. In applying a\ngaussian distribution, we are implying that the data were generated\nvia a gaussian process. However, this is not logical. From a gaussian\ndata generation process, all real values would be possible - such as a\ncount of 10.235.\n\nRather than log-transform the data to satisfy the gaussian model\nassumptions, if we assume a poisson distribution, in addition to\napplying a distribution for which the data a likely to fit better, we\nare implying that the counts have been generated via a poisson\nprocess - a much more likely situation.\n\n:::\n\n::: {.callout-important collapse=\"false\"}\n\nIt used to be reasonably common to apply square-root transformations\nto normalise count data. Square-root transformations were favoured\nover logarithmic transformations since count data often contained\nnumerous zero values and the logarithm of zero is illegal. \n\nGreat care and consideration must be applied prior to performing a\nsquare-root transformation in preparation for statistical model\nfitting. Typically, after fitting a model, various summations are\nproduced to provide insights into the model estimates and inferences.\nTo be meaningful, these insights are usually presented on the same\nscale and as original data. The back-transformation from a square-root\ntransformation is to square the data. However, this transformation\ndoes not apply equally across all ranges of data.\n\nConsider the following numbers:  $-4, -2, 0, 0.5, 1, 2$.\n\nThe smallest value here is the first value ($-4$) and the largest\nvalue is last value ($2$). However, once we square these numbers, they\nbecome $16, 4, 0, 0.25, 1, 4$.\n\n- the first value is now the largest value\n- typically, when squared, values increase, but not values between 0\n  and 1 - they decrease.\n\nSo the spacing and order can change dramatically and hence such\nback-transformations can produce values that are meaningless in the\ncontext of the study.\n\n:::\n\n# Estimates\n\n## Measures of location\n\nMeasures of location describe the center of a distribution and thus\ncharacterize the typical value of a population. There are many\ndifferent measures of location (see Table below), all of which yield\nidentical values (in the center of the distribution) when the\npopulation (and sample) follows an exactly symmetrical distribution.\nWhilst the mean is highly influenced by unusually large or small\nvalues (outliers) and skewed distributions, the median is more\n<b>robust</b>. The greater the degree of asymmetry and outliers, the\nmore disparate the different measures of location. \n\n\n| Parameter                           | Description                                                                                                                 | R syntax                                 |\n|-------------------------------------|-----------------------------------------------------------------------------------------------------------------------------|------------------------------------------|\n| _Estimates of location_             |                                                                                                                             |                                          |\n| Arithmetic mean ($\\mu$)             | the sum of the values divided by the number of values ($n$)                                                                 | `mean(x)`                                |\n| Trimmed mean                        | the arithmetic mean calculated after a fraction (typically 0.05 or $5\\%$) of the lower and upper values have been discarded | `mean(x, trim=0.05)`                     |\n| Winsorized mean                     | the arithmetic mean is calculated after the trimmed values are replaced by the upper and lower trimmed quantiles            | `library(psych)<br>winsor(x, trim=0.05)` |\n| Median                              | the middle value                                                                                                            | `median(x)`                              |\n| Minimum, maximum                    | the smallest and largest values                                                                                             | `min(x), max(x)`                         |\n| Estimates of spread                 |                                                                                                                             |                                          |\n| Variance ($\\sigma^2$)               | the average deviation (difference) of observations from the mean                                                            | `var(x)`                                 |\n| Standard deviation ($\\sigma$)       | square-root of the variance                                                                                                 | `sd(x)`                                  |\n| Median average deviation            | the median difference of observations from the median value                                                                 | `mad(x)`                                 |\n| Inter-quartile range                | the difference between the 75% and 25% ranked observations                                                                  | `IQR(x)`                                 |\n| Precision and confidence            |                                                                                                                             |                                          |\n| Standard error $\\bar{y}(s_{\\bar{y}})$ | the precision of an estimate $\\bar{y}$                                                                                      | `sd(x)/sqrt(length(x))`                  |\n| 95% confidence interval of $\\mu$    | the interval with a 95\\% probability of containing the true mean                                                            | `library(gmodels)<br>ci(x)`              |\n\n\n: {.primary .bordered .sm .paramsTable tbl-colwidths=\"[30,50,20]\"}\n\n## Measures of dispersion and variability\n\nIn addition to having an estimate of the typical value (center of a\ndistribution), it is often desirable to have an estimate of the spread\nof the values in the population. That is, do all Victorian male koalas\nweigh the same or do the weights differ substantially?\n\nIn its simplest form, the variability, or spread, of a population can\nbe characterized by its range (difference between maximum and minimum\nvalues). However, as ranges can only increase with increasing sample\nsize, sample ranges are likely to be a poor estimate of population\nspread.\n\n**Variance** ($s^2$) describes the typical deviation of values from\nthe typical (mean) value: $$s^2=\\sum{\\frac{(y_i-\\bar{y})^2}{n-1}}$$\nNote that by definition, the mean value must be in the center of all\nthe values, and thus the sum of the positive and negative deviations\nwill always be zero. Consequently, the deviances are squared prior to\nsumming. Unfortunately, this results in the units of the spread\nestimates being different to the units of location. **Standard\ndeviation** (the square-root of the variance) rectifies this issue.\n\nNote also, that population variance (and standard deviation) estimates\nare calculated with a denominator of $n-1$ rather than $n$. The reason\nfor this is that since the sample values are likely to be more similar\nto the sample mean (which is of course derived from these values) than\nto the fixed, yet unknown population mean, the sample variance will\nalways underestimate the population variance. That is, the sample\nvariance and standard deviations are biased estimates of the\npopulation parameters. Ideally, the mean and variance should be\nestimated from two different independent samples. However, this is not\npractical in most situations. Division by **n-1** rather than **n** is\nan attempt to partly offset these biases.\n\nThere are more robust (less sensitive to outliers) measures of spread\nincluding the inter-quartile range (difference between 75% and 25%\nranked observations) and the median absolute deviation (MAD: the\nmedian difference of observations from the median value).\n\n\n## Measures of the precision of estimates - standard errors and confidence intervals\n\nSince sample statistics are used to estimate population parameters, it\nis also desirable to have a measure of how good the estimates are\nlikely to be. For example, how well the sample mean is likely to\nrepresent the true population mean. The proximity of an estimated\nvalue to the true population value is its **accuracy**.\n\nClearly, as the true value of the population parameter is never known\n(hence the need for statistics), it is not possible to determine the\naccuracy of an estimate. Instead, we measure the **precision**\n(repeatability, consistency) of the estimate. Provided an estimate is\nrepeatable (likely to be obtained from repeated samples) and that the\nsample is a good, unbiased representative of the population, a precise\nestimate should also be accurate.\n\nStrictly, precision is measured as the degree of spread (standard\ndeviation) in a set of sample statistics (e.g. means) calculated from\nmultiple samples and is called the **standard error**. The standard\nerror can be estimated from a single sample by dividing the sample\nstandard deviation by the square-root of the sample size\n($\\frac{\\sigma}{\\sqrt{n}}$). The smaller the standard error of an\nestimate, the more precise the estimate is and thus the closer it is\nlikely to approximate the true population parameter.\n\nThe central limit theorem (which predicates that any set of averaged\nvalues drawn from an identical population will always converge towards\nbeing normally distributed) suggests that the distribution of repeated\nsample means should follow a normal distribution and thus can be\ndescribed by its overall mean and standard deviation (=standard\nerror). In fact, since the standard error of the mean is estimated\nfrom the same single sample as the mean, its distribution follows a\nspecial type of normal distribution called a **t**-distribution.\n\nIn accordance to the properties of a normal distribution (and thus a\n**t**-distribution with infinite degrees of freedom), 68.27% of the\nrepeated means fall between the true mean and $\\pm$ one sample\nstandard error (see Figure bellow). Put differently, we are 68.27%\npercent confident that the interval bound by the sample mean plus and\nminus one standard error will contain the true population mean. Of\ncourse, the smaller the sample size (lower the degrees of freedom),\nthe flatter the **t**-distribution and thus the smaller the level of\nconfidence for a given span of values (interval).\n\nThis concept can be easily extended to produce intervals associated\nwith other degrees of confidence (such as 95%) by determining the\npercentiles (and thus number of standard errors away from the mean)\nbetween which the nominated percentage (e.g. 95\\%) of the values lie.\nThe 95% confidence interval is thus defined as:\n\n$$P\\{\\bar{y}-t_{0.05(n-1)}s_{\\bar{y}}\\le\\mu\\le\\bar{y}+t_{0.05(n-1)}s_{\\bar{y}}\\}$$\n\nwhere $\\bar{y}$ is the sample mean, $s_{\\bar{y}}$ is the standard\nerror, $t_{0.05(n-1)}$ is the value of the 95\\% percentile of a\n\\textit{t} distribution with $n-1$ degrees of freedom, and $\\mu$ is\nthe unknown population mean.\n\nFor a 95% confidence interval, there is a 95% probability that the\ninterval will contain the true mean. Note, this interpretation is\nabout the interval, not the true population value, which remains fixed\n(albeit unknown). The smaller the interval, the more confidence is\nplaced in inferences about the estimated parameter.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20_basic_principles_files/figure-html/NormalDistribution,-1.png){width=9in}\n:::\n:::\n\n\n\nThe left hand figure above illustrates a Normal distribution\ndisplaying percentage quantiles (grey) and probabilities (areas under\nthe curve) associated with a range of standard deviations beyond the\nmean. The right hand figure displays 20 possible 95% confidence\nintervals from 20 samples ($n=30$) drawn from the one population. Bold\nintervals are those that do not include the true population mean. In\nthe long run, 5% of such intervals will not include the population\nmean ($\\mu$).\n\n# Degrees of freedom\n\nThe concept of degrees of freedom is sufficiently abstract and foreign\nto those new to statistical principles that it warrants special\nattention. The **degrees of freedom** refers to how many observations\nin a sample are \"free to vary\" (theoretically take on any value) when\ncalculating independent estimates of population parameters (such as\npopulation variance and standard deviation).\n\nIn order for any inferences about a population to be reliable, each\npopulation parameter estimate (such as the mean and the variance) must\nbe independent of one another. Yet they are usually all obtained from\na single sample and to estimate variance, a prior estimate of the mean\nis required. Consequently, mean and variance estimated from the same\nsample cannot strictly be independent of one another.\n\nWhen estimating the population variance (and thus standard deviation)\nfrom sample observations, not all of the observations can be\nconsidered independent of the estimate of population mean. The value\nof at least one of the observations in the sample is constrained (not\nfree to vary).\n\nIf, for example, there were four observations in a sample with a mean\nof 5, then the first three of these can theoretically take on any\nvalue, yet the forth value must be such that the sum of the values is\nstill 20.\n\nThe degrees of freedom therefore indicates how many\n\\textbf{independent} observations are involved in the estimation of a\npopulation parameter. A `cost' of a single degree of freedom is\nincurred for each prior estimate required in the calculation of a\npopulation parameter.\n\nThe shape of the probability distributions of coefficients (such as\nthose in linear models etc) and statistics depend on the number of\ndegrees of freedom associated with the estimates. The greater the\ndegrees of freedom, the narrower the probability distribution and thus\nthe greater the statistical power. Power is the probability of\ndetecting an effect if an effect genuinely occurs.\n\nDegrees of freedom (and thus power) are positively related to sample\nsize (the greater the number of replicates, the greater the degrees of\nfreedom and power) and negatively related to the number of variables\nand prior required parameters (the greater the number of parameters\nand variables, the lower the degrees of freedom and power).\n \n\n\n",
    "supporting": [
      "20_basic_principles_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}