{
  "hash": "bc54acf44903de82ba89615ba8ccb9f2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Different statistical philosophies\nauthor: \"Murray Logan\"\ndate: \"26 July, 2024\"\nformat: \n  html:\n    toc: true\n    toc-float: true\n    page-layout: full\n    number-sections: true\n    number-depth: 3\n    embed-resources: true\n    code-fold: false\n    code-tools: true\n    code-summary: \"Show the code\"\n    code-line-numbers: true\n    code-block-border-left: \"#ccc\"\n    code-copy: true\n    highlight-style: atom-one\n    theme: [default, ../resources/tut-style.scss]\n    css: ../resources/tut-style.css\ncrossref:\n  fig-title: '**Figure**'\n  fig-labels: arabic\n  tbl-title: '**Table**'\n  tbl-labels: arabic\nengine: knitr\nbibliography: ../resources/references.bib\noutput_dir: \"docs\"\n---\n\n\n\nThe two major opposing philosophies (frequentist and Bayesian) differ\nin how they consider population parameters and thus how\ninterpretations are framed.\n\n# Frequentist (classical)\n\nVariation in observed data allows the long-run frequency of different\noutcomes to be approximated (we can directly measure the likelihood of\nobtaining the data given a certain null hypothesis).\n\nIn the frequentist framework, population parameters (characteristics\nof the population) are considered fixed quantities (albeit unknown).\nThat is, there is one true state (e.g. mean) per population. The\nobserved data (and its characteristics - sample mean) however,\nrepresents just one possible outcome that could be collected from this\ntrue state. Multiple simultaneous data collection experiments would\nyield different outcomes. This notion that there is a single true\npopulation parameter and a distribution of possible sample outcomes is\nthe basis for a framework that conveniently allows a logical and\nrelatively simple algebraic link between data, hypotheses and\nprobability.\n\nTraditional (frequentist) statistics focus on determining the\nprobability of obtaining the collected data, given a set of parameters\n(hypothesis). In probability notation, this can be written as:\n$$P(D|H)$$ where $D$ represents the collected data and $H$ represents\nthe set of population parameters or hypothese(s).\n\nThe process of **inference** (hypothesis testing) is based on\napproximating long-run frequencies (probability) of repeated sampling\nfrom the stationary (non-varying) population(s). This approach to\nstatistics provides relatively simple analytical methods to\nobjectively evaluate research questions (albeit with rigid underlying\nassumptions) and thus gained enormous popularity across many\ndisciplines.\n\nThe price for mathematical convenience is that under this philosophy,\nthe associated statistical interpretations and conclusions are\nsomewhat counter-intuitive and not quite aligned with the desired\nmanner in which researchers would like to phrase research conclusions.\n**Importantly, conclusions are strictly about the data, not the\nparameters or hypotheses.** \n\nInference (probability and confidence intervals) is based on comparing\nthe one observed outcome (parameter estimate) to all other outcomes\nthat might be expected if the null hypothesis really was true.\nMoreover, as probabilities of point events always equal zero (e.g. the\nprobability of obtaining a mean of 13.5646 is infinitely small),\nprobability (the p-value) is calculated as the probability of\nobtaining data that is at least as extreme as that observed.\n \nAs a result of the somewhat obtuse association between hypotheses and\nthe collected data, frequentist statistical outcomes (particularly\np-values and confidence intervals) are often misused and\nmisinterpreted. Of particular note;\n\n- A frequentist p-value is the probability of rejecting the null\n  hypothesis, **it is not a measure of the magnitude of an effect or\n  the probability of a hypothesis being true**.\n- Given that a statistical null hypothesis can never actually be true\n  (e.g. a population slope is never going to be exactly zero), a\n  p-value is really just a **measure of whether the sample size is big\n  enough to detect a non-zero effect**.\n- A 95% confidence interval defines the proportion of repeated samples\n  (95/100) of a particular spread of values that are likely to contain\n  the true mean. **It is not a range of values for which you are 95%\n  confident the true mean lies between**.\n\n\n# Bayesian\n\nBy contrast, the Bayesian framework considers the observed data to be\nfixed and a truth (a real property of a population) whilst the\npopulation parameters (and therefore also the hypotheses) are\nconsidered to have a distribution of possible values. Consequently,\ninferential statements can be made directly about the probability of\nhypotheses and parameters. Furthermore, outcomes depend on the\nobserved data and not other more extreme (unobserved) data.\n\nBound up in this framework is the manner in which Bayesian philosophy\ntreats knowledge and probability. Rather than being a long-run\nfrequency of repeated outcomes (which never actually occur) as it is\nin frequentist statistics, probability is considered a degree of\nbelief in an outcome. It naturally acknowledges that belief is an\niterative process by which knowledge is continually refined on the\nbasis of new information. In the purest sense, our updated (posterior)\nbelief about a parameter or hypothesis is the weighted product of our\nprior belief in the parameter or hypothesis and the likelihood of\nobtaining the data given the parameter or hypothesis, such that our\nprior beliefs are reevaluated in light of the collected data.\n\nThe fundamental distinction between Bayesian and frequentist\nstatistics is the opposing perspectives and interpretations of\nprobability. Whereas, frequestists focus on the probability of the\ndata given the (null) hypothesis ($P(D|H)$), Bayesians focus on the\nprobability of the hypothesis given the data ($P(H|D)$).\n\nWhilst the characteristics of a single sample (in particular, its\nvariability) permit direct numerical insights (likelihood) into the\ncharacteristics of the data given a specific hypothesis (and hence\nfrequentist probability), this is not the case for Bayesian\nhypotheses.\n\nNevertheless, further manipulations of conditional probability rules\nreveal a potential pathway to yield insights about a hypothesis given\na collected sample (and thus Bayesian probability).\n\nRecall the conditional probability law: \n\n$$P(A|B) = \\frac{P(AB)}{P(B)}~ \\text{and equivalently, } P(B|A) = \\frac{P(BA)}{PAB)}$$ \n\nThese can be transformed to express in terms of $P(AB)$:\n\n$$P(AB) = P(A|B)\\times P(B), \\text{ and } P(BA) = P(B|A)\\times P(A)$$\n\nSince $P(AB)$ is the same as $P(BA)$,\n\n\\begin{align*} P(A|B)\\times P(B) &= P(B|A)\\times\nP(A)\\\\ P(A|B) &= \\frac{P(B|A)\\times P(A)}{P(B)} \\end{align*}\n\nThis probability statement (the general Bayes' rule) relates the\nconditional probability of outcome $A$ given $B$ to the conditional\nprobability of $B$ given $A$. If we now substitute outcome $A$ for $H$\n(hypothesis) and outcome $B$ for $D$ (data), it becomes clearer how\nBayes' rule can be used to examine the probability of a parameter or\nhypothesis given a single sample of data. \n\n$$P(H|D) =\n\\frac{P(D|H)\\times P(H)}{P(D)}$$\n\nwhere $P(H|D)$ is the <b>posterior\nprobability</b> of the hypothesis (our beliefs about the hypothesis\nafter inspiration from the newly collected data), $P(D|H)$ is the\n<b>likelihood</b> of the data, $P(H)$ is the <b>prior probability</b>\nof the hypothesis (our prior beliefs about the hypotheses before\ncollecting the data) and $P(D)$ is the <b>normalizing constant</b>.\n\nBayesian statistics offer the following advantages;\n\n- Interpretive simplicity:\n  - Since probability statements pertain to population parameters (or\n    hypotheses), drawn conclusions are directly compatible with\n    research or management questions.\n- Computationally robustness:\n  - Design balance (unequal sample sizes) is not relevant\n  - Multicollinearity is not relevant\n  - There is no need to have expected counts greater than 5 in\n    contingency analyses\n- Inferential flexibility:\n  - The stationary joint posterior distribution reflects the relative\n    credibility of all combinations of parameter values and from which\n    we can explore any number and combination of inferences. For\n    example, because this stationary joint posterior distribution\n    never changes no matter what perspective (number and type of\n    questions) it is examined from, we can explore all combinations of\n    pairwise comparisons without requiring adjustments designed to\n    protect against inflating type II errors. We simply derive samples\n    of each new parameter (e.g. difference between two groups) from\n    the existing parameter samples thereby obtaining the posterior\n    distribution of these new parameters.\n  - As we have the posterior density distributions for each parameter,\n    we have inherent credible intervals for any parameter. That is we\n    can state the probability that a parameter values lies within a\n    particular range. We can also state the probability that two\n    groups are different.\n  - We get the covariances between parameters and therefore we can\n    assess interactions in multiple regression\n\nDespite all the merits of the Bayesian philosophy (and that its roots\npre-date the frequentist movement), widespread adoption has been\nhindered by two substantial factors;\n\n  - Proponents have argued that the incorporation (indeed necessity)\n    of prior beliefs introduces subjectivity into otherwise objective\n    pursuits. Bayesians however, claim that considering so many other\n    aspects of experimental design (choice of subjects and treatment\n    levels etc) and indeed the research questions asked are\n    discretionary, objectivity is really a fallacy. Rather than being\n    a negative, true Bayesians consider the incorporation of priors to\n    be a strength. Nevertheless, it is possible to define vague or\n    non-informative priors\n  - Expressing probability as a function of the parameters and\n    hypotheses rather than as a function of the data relies on\n    applying an extension of conditional probability (called Bayes'\n    rule) that is only tractable (solved with simple algebra) for the\n    most trivial examples. Hence it is only through advances in modern\n    computing power that Bayesian analyses have become feasible.<br>\n    For example, when only a discrete set of hypotheses are possible,\n    then $P(D)$ essentially becomes the sum of all these possible\n    scenarios (each $P(DH_i)$):\n\n    $$P(D) = \\sum{P(D|H)P(H)}$$\n\n    $$P(H|D) = \\frac{P(D|H)\\times P(H)}{\\sum{P(D|H_i)P(H)}}$$\n\n    However, if there are potentially an infinite number of possible\n    hypotheses (the typical case, at least in theory) this sum is\n    replaced with an integral:\n\n    $$P(H|D) = \\frac{P(D|H)\\times P(H)}{\\int P(D|H_i)P(H)dH}$$\n\nWhilst it is not always possible to integrate over all the possible\nscenarios, modern computing now permits more brute force solutions in\nwhich very large numbers of samples can be drawn from the parameter\nspace which in turn can be used to recreate the exact posterior\nprobability distribution.\n  \n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}