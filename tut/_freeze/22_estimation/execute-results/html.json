{
  "hash": "d5ff3af0b46e72ef01f5986d6e73ca80",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Estimation and inference\nauthor: \"Murray Logan\"\ndate: \"26 July, 2024\"\nformat: \n  html:\n    toc: true\n    toc-float: true\n    page-layout: full\n    number-sections: true\n    number-depth: 3\n    embed-resources: true\n    code-fold: false\n    code-tools: true\n    code-summary: \"Show the code\"\n    code-line-numbers: true\n    code-block-border-left: \"#ccc\"\n    code-copy: true\n    highlight-style: atom-one\n    theme: [default, ../resources/tut-style.scss]\n    css: ../resources/tut-style.css\ncrossref:\n  fig-title: '**Figure**'\n  fig-labels: arabic\n  tbl-title: '**Table**'\n  tbl-labels: arabic\nengine: knitr\nbibliography: ../resources/references.bib\noutput_dir: \"docs\"\n---\n\n\n\n\n# Least squares\n\nLeast squares (LS) parameter estimation is achieved by simply\nminimizing the (sum) overall (square) differences between the observed\nsample values and those values calculated (predicted) from an equation\ncontaining those estimated parameter(s). For example, the least\nsquares estimate of the population mean ($\\mu$) is a value that\nminimizes the differences between the sample values and this estimated\nmean.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](22_estimation_files/figure-html/OLS-1.png){width=672}\n:::\n:::\n\n\n\n\nIt is clear in the figure above that the sum of the (squared) length\nof the grey lines that join the observed data to the candidate\nparameter value of 7 is greater than the sum of the (squared) length\nof the black lines that join the observed data to the candidate\nparameter value of 10.\n\nAs the differences between observed and predicted are squared\n(otherwise they would sum to 0), it is assumed that these differences\n(residuals) are symmetrically distributed around 0. For this reason,\nthey are almost exclusively aligned to the Normal (Gaussian)\ndistribution (as nearly every other useful sampling distribution has\nthe potential to be asymmetric and/or invalid for negative values).\n\nLeast squares estimation has no inherent basis for testing hypotheses\nor constructing confidence. Instead, inference is achieved via\nmanipulating the properties of the residual variance.\n\n# Maximum likelihood\n\nThe maximum likelihood (ML) approach estimates one or more population\nparameters such that the (log) likelihood of obtaining the observed\nsample values from such populations with the nominated probability\ndistribution is maximized. That is, the maximum likelihood parameter\nestimates are the parameter values that make the observed data most\nlikely.\n\nLikelihood of observed data, given a specific set of distribution\nparameters (for example the mean $\\mu$ and standard deviation\n$\\sigma^2$ of a normal distribution) is the product of the individual\ndensities of each observation for those parameters (and distribution).\n\n$$\\mathscr{L}(D|P) = \\Pi~f(D_i; P_A, P_B)$$\n$$\\mathscr{L}(x_1, x_2, ..., X_i|\\mu, \\sigma^2) = \\Pi~f(x_i;\\mu,\\sigma^2)$$\n\nComputationally, this involves accumulating the probabilities of\nobtaining each observation for a range of possible population\nparameter estimates, and using integration to determine the parameter\nvalue(s) that maximize the likelihood. A simplified example of this\nprocess is represented in the following figure.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](22_estimation_files/figure-html/ML-1.png){width=672}\n:::\n:::\n\n\n\nLikelihood can accommodate any exponential probability distribution\n(such as Normal, binomial, Poisson, gamma or negative binomial). When\nthe nominated probability distribution is Normal (as in the above\nfigure), ML estimators for linear model parameters have exact\ncomputational solutions and are identical to LS solutions.\n\nGiven enough time, it is possible to determine the exact combination\nof parameter values (for the nominated distribution) from amongst all\npossible combinations that maximizes the log-likelihood of observing\nthe collected data by pure brute force (i.e. calculating the\nlog-likelihood from all parameter combinations). For simple models\n(with few parameters), maximum likelihood estimates can also be\nobtained algebraically by integrating to the first partial derivative\n(the local minimum of which is zero). Unfortunately, as models become\nincreasingly more complex (with more parameters), so too does the\ncomplexity of the integration. Once the parameter space has more than\ntwo parameters, this algebraic solution becomes non-viable and complex\noptimization routines (such as Newton-Raphson optimization) are\nnecessary. The Newton-Raphson method is an optimization routine that\niteratively approaches the root (zero value) of a function by taking\nthe intercept of the tangent of first derivatives of successive root\nestimates.\n\nUnlike least squares, the maximum likelihood estimation framework also\nprovides standard errors and confidence intervals for estimations\n(since the negative log-likelihood follows a $\\chi^2$) and therefore\ncan provide a basis for statistical inference (for those seeking to\napply frequentist interpretation).\n\nA drawback of this procedure is that it is known to be unstable near\nhorizontal asymptotic. Indeed this is one of the main reasons why\nlog-likelihood is preferred over likelihood (log-likelihood functions\nare steeper and have narrower curvatures and thus have more prominent\nmaxims). It is also known to yield biased parameter estimates (the\nseverity of which is inversely related to sample size in a manner\nanalogous to the degree of bias in estimates of standard deviation).\nAnother major draw back of this method is that it typically requires\nstrong assumptions about the underlying distributions of the\nparameters.\n\n# Bayesian\n\nThe Bayesian approach yields (posterior probability) distributions for\neach of the model parameters. Therefore, a point estimate of each\nparameter is just the mean (or median or mode) of its posterior\ndistribution.\n\nRecall from the previous tutorial that via the use of very simple and\nwell established laws of conditional probability we can derive Bayes'\nrule:\n\n$$P(H|D) = \\frac{P(D|H)\\times P(H)}{P(D)}$$\n\nwhich establishes the relationship between the probability of the\nparameters given the data and the likelihood of the data given the\nparameters. In the above rule, $P(H|D)$ (or $P(y|p)$ - probability of\ndata given a parameter), the likelihood of the data, is some density\nfunction that describes the likelihood of the observed data for a\nrange of possible parameter combinations and $P(H)$ ($P(p)$) is the\nprior probability (or expected characteristics) of the parameters.\n\nIn essence, the posterior probability distribution is a weighted\naverage of the information contained in the current data (the\nlikelihood) and the prior expectations based on previous studies or\nintuition.\n\n\\begin{align*}\n{posterior~probability} &\\sim {likelihood} \\times {prior~probability}\\\\\np(H|D) &\\sim \\mathscr{L}(D|H)\\times P(H)\n\\end{align*}\n\nTo qualify as a probability distribution from which certain properties\n(such as probabilities and confidence intervals) can be gleaned, a\ndistribution (continuous density function) must integrate to one. That\nis, the total area (or volume) under the function must equal 1. Hence,\nin order to convert the product of the likelihood and the prior\nprobability into a true probability distribution it is necessary to\nnormalize this product to the appropriate sample space.\n\nFrom Bayes' rule, this is achieved by division with $P(D)$, the\nnormalizing constant or **\"marginal likelihood\"**. $P(D)$ is the\nprobability of our data considering the model and is determined by\nsumming across all possible parameter values weighted by the strength\nof belief of those parameters.\n\n$$P(H|D) = \\frac{\\mathscr{L}(D|H)\\times P(H)}{P(H)dH}$$\n\n  When there are only a small number of possible parameter (hypothesis\nvalues), the calculations are relatively straight forward. For\nexample, if the parameter can only be one of two states ('Male'\n'Female'), then the equation becomes: \n\n$$P(F|D) = \\frac{P(F|H)\\times P(F)}{P(F|H)\\times P(F) + P(M|H)\\times P(M)}$$ \n\nwhere $F$ and $M$ represent parameter values of 'Female' and 'Male'\nrespectively.<br> However, when $H$ is continuous and the number of\npossible parameter values (hypotheses) is very large or infinite, it\nis necessary to integrate over the entire parameter space.\n\nBoth frequentist and Bayesian approaches can both start with start\nwith maximum likelihood. However, whilst maximum likelihood parameter\nestimates are the result of maximizing across all possible parameter\nvalues, Bayesian estimates are the result of integrating across all\npossible parameter values.\n\n$$P(H|D) = \\frac{\\mathscr{L}(D|H)\\times P(H)}{\\int P(D|H)P(H)dH}$$\n\nUnfortunately, integration of irregular high-dimensional functions\n(the more parameters, the more dimensions) is usually intractable.\n\nIndeed, the lack of a simple mathematical solution was one of the main\nreasons that Bayesian statistics initially failed to gain traction.\n\nOne way to keep the math simple is to use likelihood functions with\n**\"conjugate\"** prior functions to ensure that the prior function is\nof the same form as the prior function. It is also possible to\nsubstitute the functions with approximations that simplify the\nalgebra.\n\n## Markov Chain Monte Carlo sampling\n\nMarkov Chain Monte Carlo (MCMC) algorithms provide a powerful, yet\nrelatively simple means of generating samples from high-dimensional\ndistributions. Whilst there are numerous specific MCMC algorithms,\nthey all essentially combine a randomization routine (Monte Carlo\ncomponent) with a routine that determines whether or not to accept or\nreject randomizations (Markov Chain component) in a way that ensures\nthat the samples are drawn from multidimensional space proportional to\ntheir likelihoods such that the distribution of these samples\n(posterior probability distribution) reflects their likelihood in\nhigh-dimensional space. Given a sufficiently large number of\nsimulations, the resulting samples should exactly describe the target\nprobability distribution and thereby allow the full range of parameter\ncharacteristics to be derived.\n\nTo illustrate the mechanics of MCMC sampling, I will contrive a very\nsimple scenario in which we have two parameters ($\\alpha$ and\n$\\beta$). I will assume infinitely vague priors on both parameters and\nthus the density function from which to draw samples will simply be\nthe likelihood density function for these parameters. For simplicity\nsake, this likelihood function will be a multivariate normal\ndistribution for the two parameters with values of $\\alpha=0$\n($var=1$) and $\\beta=5$ ($var=2$). The following two graphics provide\ntwo alternative views (perspective view and contour view) of this\nlikelihood density function.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](22_estimation_files/figure-html/LDF-1.png){width=864}\n:::\n:::\n\n\n\nThe specific MCMC algorithm that I will describe is the\nMetropolis-Hastings algorithm and I will do so as much as possible via\nvisual aids. Initially a location (coordinates) in multi-dimensional\nspace (in this case 2-D) corresponding to a particular set of\nparameter values is chosen as the starting location. We will start\nwith the coordinates (&alpha;=0.5, &beta;=-0.5), yet make the point\nthat if we collect enough samples, the importance of the initial\nstarting configuration will diminish.\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](22_estimation_files/figure-html/sim1-1.png){width=864}\n:::\n:::\n\n\n\nThe values of the parameters sampled from this joint distribution are\nthen stored.\n\nNow lets move to another random sampling location. To do so, we will\nmove a random multivariate distance and direction from the current\nlocation based on the coordinates of a single random location drawn\nfrom a multivariate normal distribution centered around zero. The\nratio of probabilities (heights on the perspective diagram)\ncorresponding to the new and previous locations is then calculated and\nthis ratio effectively becomes the probability of accepting the new\nlocation. For example, the probability associated with the initial\nlocation was approximately 0.037 and the probability associated with\nthe new location is 0.019.\n\nThe probability of moving is therefore 0.019/0.037=0.513. There is\njust over 50% chance of moving. A random number between 0 and 1 is\nthen drawn from a uniform distribution. If this number is greater than\nthe ratio described above, the new location is rejected and the\nprevious location is resampled. However, if the number is less than\nthe ratio, then the new location is accepted and parameter values are\nstored from this new location.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](22_estimation_files/figure-html/sim2-1.png){width=864}\n:::\n:::\n\n\n\nThe sampling chain continues until a predefined number of iterations\nhave occurred.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](22_estimation_files/figure-html/sim3-1.png){width=864}\n:::\n:::\n\n\n\nWith 1000 samples collected\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](22_estimation_files/figure-html/sim4-1.png){width=864}\n:::\n:::\n\n\n\nThe collected samples, in this case 1000 samples for $\\alpha$ and 1000\nfor $\\beta$, can then be used to calculate a range of characteristics.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncolnames(params)<-c(\"a\",\"b\")\napply(params,2,mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          a           b \n-0.03090054  5.22433258 \n```\n\n\n:::\n\n```{.r .cell-code}\napply(params,2,sd)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        a         b \n0.8792583 1.4364185 \n```\n\n\n:::\n:::\n\n\n\nRecall that the true parameter means and standard deviations were\n($\\alpha=0, var=1 \\Rightarrow sd=1; \\beta=5, var=2 \\Rightarrow\nsd=1.414$). Whilst the means and standard deviations from the\nsimulated samples are similar to these values, they are in no way\nidentical. There are numerous potential reasons for this. We will now\nexplore these. It is worth noting at this point that we are in the\nunique position of being able to identify that the parameter estimates\ndo not yet match closely the true parameter values. Normally this\nwould not be case. However, there are various diagnostics that we can\nemploy to help us evaluate whether the chain has yielded a stationary\nposterior distribution or not.\n\n- Perhaps the posterior distribution (represented by the red dots) had\n  not yet stabilized. That is, perhaps the shape of the posterior\n  distribution was still changing slightly with each additional sample\n  collected. The chain had not yet completely sampled the entire\n  surface in approximate proportion to the densities. If so, then\n  collecting substantially more samples could address this. Lets try\n  10,000.\n\n::: {.indented}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](22_estimation_files/figure-html/sim6-1.png){width=864}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncolnames(params)<-c(\"a\",\"b\")\napply(params,2,mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          a           b \n-0.02836663  4.99912614 \n```\n\n\n:::\n\n```{.r .cell-code}\napply(params,2,sd)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       a        b \n1.010878 1.422975 \n```\n\n\n:::\n:::\n\n\n\nThe resulting estimates are now very accurate. Given sufficient\nindependent MCMC samples, exact solutions can be obtained (c.f.\nfrequentist approximates that become increasingly more dubious with\nincreasing model complexity and diminishing levels of replication).\n\nThe \"sharper\" the features (and more isolated islands) in the\nmultidimensional profile, the more samples will be required in order\nto ensure that the Markov chain traverses (mixes across) all of the\nfeatures. One of the best ways of evaluating whether or not sufficient\nsamples have been collected is with a trace plot. Trace plots\nessentially display the iterative history of MCMC sequences. Ideally,\nthe trace plot should show no trends, just consistent random noise\naround a stable baseline. Following are three trace plots that depict\nan adequate number samples, inadequate number of samples and\ninsufficient mixing across all features.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](22_estimation_files/figure-html/sim7-1.png){width=864}\n:::\n:::\n\n\n\n:::\n\n\t\t\t  \n- The drawback of a Markov Chain is that the samples are not\n  independent of one another. In a sense, its strength (the properties\n  that allow it to simulate in proportion to the density) can also be\n  a weakness in that consecutive samples are likely to be very similar\n  (non-independent) to one another. Summary statistics based on\n  non-independent samples tend to be biased and can be\n  unrepresentative.\n\n::: {.indented}\n\nIt turns out that there is a very simple remedy for this however.\nRather than store every single sample, we can thin the sample set by\nskipping over (not storing) some of the samples. For example, we could\nnominate to store every second, or fifth or tenth etc sample. The\nresulting samples will thence be more independent of one another. Note\nhowever, that the sample will be smaller by a factor of the thinning\nvalue and thus it may be necessary to perform more iterations.\n\nTo illustrate autocorrelation and thinning, we will run the MCMC\nsimulations twice, the first time with no thinning and a second time\nwith a thinning factor of 10. Non-thinned (stored) samples are\ndepicted as circles with black borders. Associated with each chain of\nsimulations, is an autocorrelation function (ACF) plot which\nrepresents the degree of correlation between all pairs of samples\nseparated by progressively larger lags (number of samples).\n\n**Thinning factor = 1**\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](22_estimation_files/figure-html/sim8-1.png){width=864}\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Thinning factor = 10**\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](22_estimation_files/figure-html/sim8b-1.png){width=864}\n:::\n:::\n\n\n\nFor the un-thinned samples, the autocorrelation at lag 1 (adjoining\npairs of samples) is over 0.8 (80%) indicating a very high degree of\nnon-independence. Ideally, we would like to aim for a lag 1\nautocorrelation value no greater than 0.1. According to the ACF plot\nassociated with the un-thinned simulation chain, to achieve a lag 1\nautocorrelation value of no greater than 0.1 would require a sample\nseparation (thinning) of greater than 10. Indeed, the second\nsimulation chain had a thinning factor of 10 (only retained every 10th\nsample) and still has a lag 1 autocorrelation value greater than 0.1\n(although this is may now be because it is based on a much smaller\nsample size). To be sure, we could re-simulate the chain again with a\n15-fold thinning but with 10 times more iterations (a total of\n10,000).\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](22_estimation_files/figure-html/sim9-1.png){width=864}\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n:::\n\n- Although the importance of the starting configuration (initial\n  coordinates) should eventually diminish as the stochastic walk\n  traverses more and more of the profile, for shorter chains, the\n  initial the initial path traversed is likely to exert a pull on the\n  sample values. Of course, substantially increasing the number of\n  iterations of the chain will substantially increase the time it\n  takes to run the chain.\n\n::: {.indented}\nHence, it is common to define a **burnin** interval at the start of\nthe chain. This defines the number of initial iterations that are\nignored (samples not retained). Generally, ignoring the first 10% of\niterations should be sufficient. In the current artificial example,\nthis is unlikely to be an issue as the density distribution has very\nblunt, connected features.\n\nEventually (given enough iterations), the MCMC chain should sample the\nentire density profile and the resulting posterior distribution should\nconverge (produce relatively invariant outcomes) on a stationary\ndistribution (the target distribution).\n\nUnlike models fitted via maximum likelihood (which stop iterating when\nthe improvement in log-likelihood does not exceed a pre-defined\nvalue - at which point they have usually converged), MCMC chains do\nnot have a stopping criterion. Rather, the analyst must indicate the\nnumber of iterations (simulations) to perform. The number of\niterations is always a compromise between convergence and time. The\nmore iterations, the better the mixing and coverage, the greater the\nlikelihood of convergence, yet the greater the time it takes to\ncomplete the analysis.\n\n- **Trace plots** - as described above, trace plots for each parameter\n  offer visual indicators of issues with the mixing and coverage of\n  the samples collected by the chain as well as the presence of a\n  starting configuration impact. In addition to the lack of drift, the\n  residuals (sometimes called units) should be centered around 0.\n  Positive displacement of the residuals can be an indication of\n  zero-inflation (response having more zeros that the modelling\n  distribution should expect).\n\n\n- **Plots of the posterior distribution** (density or histograms) of\n  each parameter. The basic shape of these plots should follow the\n  approximate expected joint probability distribution (between family\n  modeled in determining likelihood and prior). <i>Conjugate</i>\n  priors are typically used to simplify the expected posterior\n  distribution.\n\n:::: {.indented} \n\nIn our contrived example, this should be a normal distribution. Hence,\nobviously non-normal (particularly bi-modal) distributions would\nsuggest that the chain has not yet fully mixed and converged on a\nstationary distribution. Specifically, it would imply that the\nsampling chain had focused heavily on one region of the parameter\nspace before moving and focusing on another region. In the case of\nexpected non-normal distributions, the density plots can also serve to\nremind us that certain measures of location and spread (such as mean\nand variance) might be inadequate characterizations of the population\nparameters.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](22_estimation_files/figure-html/sim10-1.png){width=8in}\n:::\n:::\n\n\n::::\n\n- **Autocorrelation** - correlation between successive samples (as a\n  result of way the Markov Chain spreads) are useful for determining\n  the appropriate amount of thinning required to generate a well mixed\n  series of samples. Autocorrelation can be explored visually (in a\n  plot such as that illustrated earlier) or numerically by simply\n  exploring the autocorrelation values at a range of lags.\n\n\n- The **Geweke diagnostic** (and Geweke-Brooks plot) compares the\n  means (by a standard Z-score) of two non-overlapping sections of the\n  chain (by default the first 10% and the last 50%) to explore the\n  likelihood that the samples from the two sections come from the same\n  stationary distribution. In the plot, successively larger numbers of\n  iterations are dropped from the start of the samples (up to half of\n  the chain) and the resulting Z-scores are plotted against the first\n  iteration in the starting segment.\n\n:::: {.indented} \n\nSubstantial deviations of the Z-score from zero (those outside the\nhorizontal critical confidence limits) imply poor mixing and a lack of\nconvergence. The Geweke-Brooks plot is also particularly useful at\ndiagnosing a lack of mixing due o the initial configuration and how\nlong the burnin phase should be.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](22_estimation_files/figure-html/sim11-1.png){width=8in}\n:::\n:::\n\n\n\n::::\n\n- The **Gelman and Rubin diagnostic** essentially calculates the ratio\n  of the total variation within and between multiple chains to the\n  within chain variability (this ratio is called the Potential Scale\n  Reduction Factor). As the chain progresses (and samples move towards\n  convergence) the variability between chains should diminish such\n  that the scale reduction factor essentially measures the ratio of\n  the within chain variability to itself. At this point (when the\n  potential scale reduction factor is 1), it is likely that any one\n  chain should have converged. When the potential scale reduction\n  factor is greater than 1.1 (or more conservatively 1.2), we should\n  run the chain(s) for longer to improve convergence.\n\n:::: {.indented} \n\nClearly, in order to calculate the Gelman and Rubin diagnostic and\nplot, it is necessary to run multiple chains. When doing so, it is\nadvisable that different starting configurations be used for each\nchain.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](22_estimation_files/figure-html/sim12-1.png){width=8in}\n:::\n\n::: {.cell-output-display}\n![](22_estimation_files/figure-html/sim12-2.png){width=8in}\n:::\n:::\n\n\n\n::::\n\n- A **Raftery and Lewis diagnostic** estimates the number of\n  iterations and the burnin length required to have a given\n  probability (95%) of posterior quantile (0.025) accuracy (0.005). In\n  the following, I will show the Raftery and Lewis diagnostic from a\n  sample length of 1000 (which it considers below the minimum\n  requirements for the quantile, probability and accuracy you have\n  nominated) and this will be followed by diagnostics from a longer\n  chain (40,000 iterations with a thinning factor of 10). In the\n  latter case, for each parameter it lists the burnin length ($M$),\n  number of MCMC iterations ($N$), minimum number of (uncorrelated)\n  samples to construct this plot and $I$ (the proportional increase in\n  necessary iterations due to autocorrelation). Values of $I$ greater\n  than 5 indicate substantial autocorrelation. \n\n:::: {.indented} \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(9)  \nraftery.diag(params)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nQuantile (q) = 0.025\nAccuracy (r) = +/- 0.005\nProbability (s) = 0.95 \n\nYou need a sample size of at least 3746 with these values of q, r and s\n```\n\n\n:::\n\n```{.r .cell-code}\nparams1<-mcmcsim1(n=40000, start.x=0.5, start.y=3,tex=FALSE,lns=FALSE,fls=FALSE,thin=10)\n```\n\n::: {.cell-output-display}\n![](22_estimation_files/figure-html/sim13-1.png){width=5in}\n:::\n\n```{.r .cell-code}\nraftery.diag(params1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nQuantile (q) = 0.025\nAccuracy (r) = +/- 0.005\nProbability (s) = 0.95 \n                                       \n Burn-in  Total Lower bound  Dependence\n (M)      (N)   (Nmin)       factor (I)\n 4        4782  3746         1.28      \n 5        5894  3746         1.57      \n```\n\n\n:::\n:::\n\n\n\n::::\n\n- The <b>Heidelberg and Welch diagnostic</b> tests a null hypothesis\n  that the Markov chain is from a stationary distribution. The test\n  evaluates the accuracy of the estimated means after incrementally\n  discarding more and more (10%, 20%, 30% up to 50%) of the chain. The\n  point at which the null hypothesis is accepted marks the number of\n  iterations required to keep. If the null hypothesis is not accepted\n  by the time 50% of the chain has been discarded, it suggests that\n  more MCMC iterations are required to reach a stationary distribution\n  and to estimate the mean with sufficient accuracy. For the portion\n  of the chain that passed (null hypothesis accepted), the sample mean\n  and 95% confidence interval are calculated.\n\n:::: {.indented} \n\nIf the ratio of the mean to 95% interval is greater than 0.1, then the\nmean is considered accurate otherwise a longer overall chain length is\nrecommended. Note, this diagnostic is misleading for residuals or any\nother parameters whose means are close to zero. Whilst this diagnostic\ncan help trim chains to gain speed, in practice it is more useful for\njust verifying that the collected chain is long enough. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(9) \npar(mfrow=c(1,3),mar=c(4,4,1,1))\nheidel.diag(params)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                                \n  Stationarity start     p-value\n  test         iteration        \na passed       1         0.326  \nb passed       1         0.713  \n                            \n  Halfwidth Mean   Halfwidth\n  test                      \na failed    0.0262 0.0902   \nb passed    5.0083 0.1438   \n```\n\n\n:::\n\n```{.r .cell-code}\nheidel.diag(params1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                                   \n     Stationarity start     p-value\n     test         iteration        \n[,1] passed       1         0.245  \n[,2] passed       1         0.912  \n                               \n     Halfwidth Mean   Halfwidth\n     test                      \n[,1] failed    -0.017 0.0395   \n[,2] passed     5.003 0.0683   \n```\n\n\n:::\n:::\n\n\n\n::::\n\n:::\n\nAlthough MCMC algorithms have their roots in the Bayesian paradigm,\nthey are not exclusively bound to Bayesian analyses. Philosophical\narguments aside, MCMC sampling algorithms provide very powerful ways\nto tackling otherwise intractable statistical models. Indeed, it is\nbecoming increasingly more common for MCMC sampling to employ vague\n(non-informative) priors to fit models that are otherwise not\nanalytically compliant or convergent.\n\n## Priors\n\nBayesian analyses incorporate prior expectations about the parameters\nbeing estimated (fixed effects, random effects and residuals). As with\nthe likelihoods and posteriors, the priors are distributions (rather\nthan single values) thereby reflecting the typical prior expectations\nas the variability (spread).\n\nWhilst proponents consider the ability to evaluate information within\na broader and more substantive context is one of the great strengths\nof the Bayesian framework, opponents claim that the use or reliance of\nprior information has the potential to bias the outcomes. To an\nextent, both camps have valid points (which is perhaps why the\nphilosophical debates continue). However, there is a practical\ncompromise.\n\nIt is possible, and indeed advisable to employ a range of priors\nincluding non-informative (or null, vague, weakly informative) priors.\nStrickly, non-informative priors (such as a uniform distribution which\nis purely flat over its entire range) have no impact on the outcomes\nas they simply multiply the likelihood by a constant. Hence,\nnon-informative flat priors result in parameter estimates that are\nalmost identical to those of equivalent traditional analyses.\n\nWeakly informative priors (which are typically more preferable) impose\na broad structure (e.g. the prior of a parameter is normally\ndistributed with a certain mean and typically very large standard\ndeviation). Whilst in the absence of any other prior knowledge, it is\ncommon to use a normal distribution with a mean of zero and very large\nstandard deviation. Note however, that such a prior distribution will\nhave the effect of 'shrinking' estimates towards zero. Such priors are\nvery useful when no other priors are sensible and justifiable. By\napplying a range of priors, it is possible to evaluate the impact of\ndifferent priors and this in itself can be a valuable form of\nanalysis.\n\nEven when no prior information is available, priors must still be\ndefined to make use of MCMC sampling techniques. In such cases,\nnon-informative priors are typically defined. The other retort to the\nsubjectivity accusation is that science is an inherently subjective\nactivity in the first place. To deny there is no subjectivity in other\naspects of sampling design and data collection is purely fanciful. At\nleast in the case of selecting priors, we can (by using a range of\npriors) at least evaluate this source of bias.\n\n### Conjugate priors\n\nAlthough priors can be specified from any distribution, historically\nthe choice of distribution was such that the posterior distribution\ncould be calculated via relatively simple hand calculations. Recall\nthat the posterior distribution is equal to the product of the\nlikelihood function ($P(D|H)$) and the prior ($P(H)$) normalized\n(divided by) the probability of the data ($P(D)$) and that $P(D)$ is\ndetermined by integrating across all possible parameter (hypothesis)\nvalues. \n\n$$P(H|D) = \\frac{P(D|H)\\times P(H)}{\\int P(D|H)P(H)dH}$$\n\nCalculating the integral ($\\int P(D|H)P(H)dH$) is generally more\nmanageable when the posterior distribution ($P(H|D$) is the same\ndistribution family (same distribution type, yet possibly different\nvalues of the parameters) as the prior distribution ($P(H)$). The\nposterior and prior distributions are thence called **conjugate\ndistributions** and the prior is considered the **conjugate\nprior** to the model likelihood. \n\n\n| Observations                          | Likelihood distribution | Parameter      | Conjugate prior | Prior hyperparameters |\n|---------------------------------------|-------------------------|----------------|-----------------|-----------------------|\n| Normal (measurements)                 | Normal (Gaussian)       | $\\mu$          | normal          | $\\mu, \\tau$           |\n|                                       |                         | $\\tau$         | gamma           | $r, \\mu$              |\n| Skewed normal (measurements)          | Log-normal              | $\\mu$          | normal          | $\\mu, \\tau$           |\n|                                       |                         | $\\tau$         | gamma           | $r, \\mu$              |\n| Binary (presence/absence, dead/alive) | Binomial                | $logit(p)$     | beta            | $a, b$                |\n|                                       |                         | $probit(p)$    | gamma           | $a, b$                |\n|                                       |                         | $cloglog(p)$   | gamma           | $a, b$                |\n| Poisson                               | Poisson                 | $log(\\lambda)$ | gamma           | $r, \\mu$              |\n|                                       | Negative binomial       | $logit(p)$     | beta            | $a, b$                |\n| Percentages                           | Beta                    | $a$            | beta            | $a, b$                |\n|                                       |                         | $b$            | beta            | $a, b$                |\n\n: {.primary .bordered .sm .paramsTable tbl-colwidths=\"[20,20,20,20,20]\"}\n\nNote, vague gamma priors has a is relatively flat density over most of\nits range, yet tend to have a sharp spike at the origin. This can\ncause problems, particularly in multilevel models. Consequently,\nuniform priors are recommended as an alternative to gamma priors.\n\nThe need for the conjugacy as a mathematical convenience is now\nunnecessary thanks to MCMC sampling techniques. Nevertheless,\nconjugacy does make it easier to anticipate the distribution of the\nposterior(s) and thus help identify and diagnose issues of mixing and\nconvergence (via a lack of conformity to the anticipated\ndistribution).\n\n### Priors on variance\n\nNumerous non-informative priors for variance have been suggested\nincluding:\n\n- flat uniform prior distribution $U(0, A)$, where\n  $A\\rightarrow\\infty$\n- non-informative _inverse-gamma_ $Gamma(\\epsilon, \\epsilon)$, where\n  $\\epsilon$ is set to a low value such as 0.001, 1.0E-06 etc)\n- half-cauchy distribution $HC(\\alpha)$ where the scale parameter,\n  $\\alpha$ is typically 5 or 25.\n  \n@Gelman-2006-515 warns that the use of the inverse-gamma family of\nnon-informative priors are very sensitive to $\\epsilon$ particularly\nwhen variance is close to zero and this may lead to unintentionally\ninformative priors. When the number of groups (treatments or\nvarying/random effects) is large (more than 5), @Gelman-2006-515\nadvocated the use of either uniform or half-cauchy priors. Yet when\nthe number of groups is low, @Gelman-2006-515 indicates that uniform\npriors have a tendency to result in inflated variance estimates.\nConsequently, half-cauchy priors are generally recommended for\nvariances.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}