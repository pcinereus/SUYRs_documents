{
  "hash": "cc159033141d62efeebde11e5eeff3e8",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Bayesian generalised linear models\nauthor: \"Murray Logan\"\ndate: \"22 July, 2024\"\nformat: \n  html:\n    toc: true\n    toc-float: true\n    page-layout: full\n    number-sections: true\n    number-depth: 3\n    embed-resources: true\n    code-fold: false\n    code-tools: true\n    code-summary: \"Show the code\"\n    code-line-numbers: true\n    code-block-border-left: \"#ccc\"\n    code-copy: true\n    highlight-style: atom-one\n    theme: [default, ../resources/tut-style.scss]\n    css: ../resources/tut-style.css\ncrossref:\n  fig-title: '**Figure**'\n  fig-labels: arabic\n  tbl-title: '**Table**'\n  tbl-labels: arabic\nengine: knitr\nbibliography: ../resources/references.bib\noutput_dir: \"docs\"\n---\n\n\n\n\n\n# Preparations\n\nLoad the necessary libraries\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)   #for data wrangling and plotting\nlibrary(DHARMa)      #for simulated residuals\nlibrary(performance) #for model diagnostics\nlibrary(see)         #for model diagnostics\nlibrary(brms)        #for Bayesian models\nlibrary(tidybayes)   #for exploring Bayesian PKPDmodels\nlibrary(rstan)       #for diagnostics plots\nlibrary(bayesplot)   #for diagnostic plots\nlibrary(patchwork)   #for arranging multiple plots\nlibrary(gridGraphics)#for arranging multiple plots - needed for some patchwork plots\nlibrary(HDInterval)  #for HPD intervals\nlibrary(bayestestR)  #for ROPE\nlibrary(emmeans)     #for estimated marginal means\nlibrary(standist)    #for plotting distributions\nlibrary(cmdstanr)    #for the backend\nsource(\"helperfunctions.R\")\n```\n:::\n\n\n\nMany biologists and ecologists get a little twitchy and nervous around\nmathematical and statistical formulae and nomenclature. Whilst it is\npossible to perform basic statistics without too much regard for the\nactual equation (model) being employed, as the complexity of the\nanalysis increases, the need to understand the underlying model\nbecomes increasingly important. Moreover, model specification in BUGS\n(the language used to program Bayesian modelling) aligns very closely\nto the underlying formulae. Hence a good understanding of the\nunderlying model is vital to be able to create a sensible Bayesian\nmodel. Consequently, I will always present the linear model formulae\nalong with the analysis. If you start to feel some form of disorder\nstarting to develop, you might like to run through the Tutorials and\nWorkshops twice (the first time ignoring the formulae). \n\n::: {.callout-note}\nThis tutorial will introduce the concept of Bayesian (generalised)\nlinear models and demonstrate how to fit simple models to a set of\nsimple fabricated data sets, each representing major data types\nencountered in ecological research. Subsequent tutorials will build on\nthese fundamentals with increasingly more complex data and models.\n:::\n\n\n\n\n# A philosophical note\n\nTo introduce the philosophical and mathematical differences between\nclassical (frequentist) and Bayesian statistics, @Wade-2000 presented\na provocative yet compelling trend analysis of two hypothetical\npopulations. The temporal trend of one of the populations shows very\nlittle variability from a very subtle linear decline. By contrast, the\nsecond population appears to decline more dramatically, yet has\nsubstantially more variability.\n\n@Wade-2000 neatly illustrates the contrasting conclusions\n(particularly with respect to interpreting probability) that would be\ndrawn by the frequentist and Bayesian approaches and in so doing\nhighlights how and why the Bayesian approach provides outcomes that\nare more aligned with management requirements.\n\nThis tutorial will start by replicating the demonstration of @Wade-2000.\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.columns}\n\n:::: {.column width=\"32%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/PopA-1.png){width=288}\n:::\n:::\n\n\n\n<p style=\"margin-left:30pt;\">\nn: 10<br>\nSlope: -0.1022<br>\n_t_: -2.3252<br>\n_p-value_: 0.0485<br>\n</p>\n\n::::\n\n:::: {.column width=\"32%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/PopB-1.png){width=288}\n:::\n:::\n\n\n\n<p style=\"margin-left:30pt;\">\nn: 10<br>\nSlope: -10.2318<br>\n_t_: -2.2115<br>\n_p-value_: 0.0579<br>\n</p>\n\n::::\n\n:::: {.column width=\"32%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/PopC-1.png){width=288}\n:::\n:::\n\n\n\n<p style=\"margin-left:30pt;\">\nn: 100<br>\nSlope: -10.4713<br>\n_t_: -6.6457<br>\n_p-value_: 0<br>\n</p>\n\n::::\n\n:::\n\n\nFrom a traditional frequentist perspective, we would conclude that\nthere is a 'significant' relationship in Population A and C\n($p<0.05$), yet not in Population B ($p>0.05$). Note, Population B and\nC were both generated from the same random distribution, it is just\nthat Population C has a substantially higher number of observations.\n\nThe above illustrates a couple of things\n\n- statistical significance does not necessarily translate into\n  biological importance. The percentage of decline for Population A is\n  0.46 where as the percentage of decline\n  for Population B is 45.26. That is Population B is declining\n  at nearly 10 times the rate of Population A. That sounds rather\n  important, yet on the basis of the hypothesis test, we would dismiss\n  the decline in Population B.\n\n- that a _p_-value is just the probability of detecting an effect or\n  relationship - what is the probability that the sample size is large\n  enough to pick up a difference.\n\nLet us now look at it from a Bayesian perspective. I will just provide\nthe posterior distributions (densities scaled to 0-1 so that they can\nbe plotted together) for the slope for each population.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/Pop_densities-1.png){width=864}\n:::\n:::\n\n\n\nFocusing on Populations A and B, we would conclude:\n\n- the mean (plus or minus CI) slopes for Population A and B are \n  -0.1 (-0.21,0) and -10.14\n  (-20.12,0.37) respectively.\n\n- the Bayesian approach allows us to query the posterior distribution\n  is many other ways in order to ask sensible biological questions.\n  For example, we might consider that a rate of change of 5% or\n  greater represents an important biological impact. For Population A\n  and B, the probability that the rate is 5% or greater is \n  0 and 0.86\n  respectively.\n\n# Review of (generalised) linear models\n\nI would highly recommend reviewing the information in [the tutorial on\ngeneralised linear models](30_glm.html), particularly the sections\ndescribe linear models, assumption checking and generalised linear\nmodels (GLM). Whilst there are philosophical differences between\nfrequentist and Bayesian statistics that have implications for how\nmodels are fit and interpreted, model choice and assumption checking\nprinciples are common between the two approaches. Hence, many of these\ntopics will be assumed, and not fully described in the current\ntutorial.\n\nRecall from [the tutorial on generalised linear models](30_glm.html)\nthat simple linear regression is a linear modelling process that\nmodels a single response variable against one or more predictors with\na linear combination of coefficients and (in the case of a Gaussian\nmodel) can be expressed as:\n\n::: {.columns}\n\n:::: {.column width=\"48%\"}\n$$y_i = \\beta_0+ \\beta_1 x_i+\\epsilon_i \\hspace{1cm}\\epsilon\\sim{}N(0,\\sigma^2)$$\n\nwhere:\n\n- $y_i$ is the response value for each of the $i$ observations\n- $\\beta_0$ is the y-intercept (value of $y$ when $x=0$)\n- $\\beta_1$ is the slope (rate of chance in $y$ per unit chance in\n  $x$)\n- $x_i$ is the predictor value for each of the $i$ observations\n- $\\epsilon_i$ is the residual value of each of the $i$ observations.\n  A residual is the difference between the observed value and the\n  value expected by the model.\n\n- $\\epsilon\\sim{}N(0,\\sigma^2)$ indicates that the residuals are\n  normally distributed with a constant amount of variance\n\n::::\n\n:::: {.column width=\"48%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/residuals,-1.png){width=288}\n:::\n:::\n\n\n::::\n\n:::\n\nThe above can be re-expressed and generalised as:\n\n$$\n\\begin{align}\ny_i&\\sim{}Dist(\\mu_i, ...) \\\\\ng(\\mu_i) &= \\beta_0+ \\beta_1 x_i\n\\end{align}\n$$\n\nwhere:\n\n- $Dist$ represents a distribution from the exponential family (such\n  as Gaussian, Poisson, Binomial, etc)\n- $...$ represents additional parameters relevant to the nominated\n  distribution (such as $\\sigma^2$: Gaussian, $n$: Binomial and\n  $\\phi$: Negative Binomial, etc)\n- $g()$ represents the link function (e.g. log: Poisson, logit:\n  Binomial, etc)\n\nThe reliability of any model depends on the degree to which the data\nadheres to the model assumptions. Hence, as with frequentist models,\nexploratory data analysis (EDA) is a vital component of Bayesian\nmodelling and since the model structures are similar between\nfrequentist and Bayesian approaches, so too is EDA.\n\n# Bayesian (generalised) linear models\n\nFor the purpose of introduction, we will start by exploring a Gaussian\nmodel with a very simple fabricated data set representing the\nrelationship between a response ($y$) and a continuous predictor ($x =\n[1,2,3,4,5,6,7,8,9,10]$. The fabricated data set will comprise 10\nobservations each drawn from normal distributions with a set standard\ndeviation of 4. The means of the 10 populations will be determined by\nthe following equation: \n\n$$ \\mu_i = 2 + 5\\times x_i $$\n\nLet generate these data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(234)\ndat <- data.frame(x = 1:10) |>\n    mutate(y = round(rnorm(n = 10, mean = 2 + (5 * x), sd = 4), digits = 2))\ndat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    x     y\n1   1  9.64\n2   2  3.79\n3   3 11.00\n4   4 27.88\n5   5 32.84\n6   6 32.56\n7   7 37.84\n8   8 29.86\n9   9 45.05\n10 10 47.65\n```\n\n\n:::\n:::\n\n\n\n\n\nThe model will will be fitting will be:\n\n$$\n\\begin{align}\ny_i&\\sim{}N(\\mu_i, \\sigma^2)\\\\\n\\mu_i &= \\beta_0+ \\beta_1 x_i\n\\end{align}\n$$\n\nThe parameters that we are going to attempt to estimate are the\ny-intercept ($\\beta_0$), the slope ($\\beta_1$) and the underlying\nvariance ($\\sigma^2$). Recall (from tutorials on [statistical\nphilosophies](21_statistical_philosophies.html) and\n[estimation](22_estimation.html))) that Bayesian models calculate\n**posterior** predictions ($P(H|D)$) from likelihood ($P(D|H)$) and\n**prior** expectations ($P(H)$). Therefore, in preparation for fitting\na Bayesian model, we must consider what our prior expectations are for\n_all_ parameters.\n\nThe individual responses ($y_i$, observed yields) are each expected to\nhave been **independently** drawn from normal (**Gaussian**)\ndistributions ($\\mathcal{N}$). These distributions represent all the\npossible values of $y$ we could have obtained at the specific ($i^{th}$)\nlevel of $x$. Hence the $i^{th}$ $y$ observation is expected\nto have been drawn from a normal distribution with a mean of $\\mu_i$.\n\nAlthough each distribution is expected to come from populations that\ndiffer in their means, we assume that all of these distributions have\nthe **same variance** ($\\sigma^2$).\n\n## Priors\n\nWe need to supply priors for each of the parameters to be estimated\n($\\beta_0$, $\\beta_1$ and $\\sigma$). Whilst we want these priors to be\nsufficiently vague as to not influence the outcomes of the analysis\n(and thus be equivalent to the frequentist analysis), we do not want\nthe priors to be so vague (wide) that they permit the MCMC sampler to\ndrift off into parameter space that is both illogical as well as\nnumerically awkward.\n\nProffering sensible priors is one of the most difficult aspects of\nperforming Bayesian analyses. For instances where there are some\nprevious knowledge available and a desire to incorporate those data,\nthe difficulty is in how to ensure that the information is\nincorporated correctly. However, for instances where there are no\nprevious relevant information and so a desire to have the posteriors\ndriven entirely by the new data, the difficulty is in how to define\npriors that are both vague enough (not bias results in their\ndirection) and yet not so vague as to allow the MCMC sampler to drift\noff into unsupported regions (and thus get stuck and yield spurious\nestimates).\n\nFor early implementations of MCMC sampling routines (such as\nMetropolis Hasting and Gibbs), it was fairly common to see very vague\npriors being defined.  For example, the priors on effects, were\ntypically normal priors with mean of 0 and variance of `1e+06`\n(1,00,000).  These are very vague priors.  Yet for some samplers\n(e.g. NUTS), such vague priors can encourage poor behaviour of the\nsampler - particularly if the posterior is complex. It is now\ngenerally advised that priors should (where possible) be somewhat\n**weakly informative** and to some extent, represent the bounds of\nwhat are feasible and sensible estimates.\n\nThe degree to which priors __influence__ an outcome (whether by having\na pulling effect on the estimates or by encouraging the sampler to\ndrift off into unsupported regions of the posterior) is dependent on:\n\n- the relative sparsity of the data - the larger the data, the less\n  weight the priors have and thus less influence they exert.\n- the complexity of the model (and thus posterior) - the more\n  parameters, the more sensitive the sampler is to the priors.\n\nThe sampled posterior is the product of both the likelihood and the\nprior - all of which are multidimensional.  For most applications, it\nwould be vertically impossible to define a sensible multidimensional\nprior.  Hence, our only option is to define priors on individual\nparameters (e.g. the intercept, slope(s), variance etc) and to hope\nthat if they are individually sensible, they will remain collectively\nsensible.\n\nSo having (hopefully) impressed upon the notion that priors are an\nimportant consideration, I will now attempt to synthesise some of the\napproaches that can be employed to arrive at weakly informative priors\nthat have been gleaned from various sources.  Largely, this advice has\ncome from the following resources:\n\n- https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations\n- http://svmiller.com/blog/2021/02/thinking-about-your-priors-bayesian-analysis/\n\nI will outline some of the current main recommendations before\nsummarising some approaches in a table.\n\n- weakly informative priors should contain enough information so as to\n  regularise (discourage unreasonable parameter estimates whilst\n  allowing all reasonable estimates).\n- for effects parameters on scaled (standardised) data, an argument\n  could be made for a normal distribution with a standard deviation of\n  1 (e.g. `normal(0,1)`), although some prefer a _t_ distribution with\n  3 degrees of freedom and standard deviation of 1 (e.g.\n  `student_t(3,0,1)`) - apparently a flatter _t_ is a more robust\n  prior than a normal as an uninformative prior...\n- for un-scaled data, the above priors can be scaled by using the\n  standard deviation of the data as the prior standard deviation\n  (e.g. `student_t(3,0,sd(y))`, or `sudent_t(3,0,sd(y)/sd(x))`)\n- for priors of hierachical standard deviations, priors should\n  encourage shrinkage towards 0 (particularly if the number of groups\n  is small, since otherwise, the sampler will tend to be more\n  responsive to \"noise\").\n\nIn this tutorial series, we will perform Bayesian analysis in the STAN\nlanguage via an R interface. Two popular interfaces that greatly\nsimplify the specification of Bayesian models are\n[brms](https://paul-buerkner.github.io/brms/) and\n[rstanarm](https://mc-stan.org/rstanarm/). We will exclusively focus\non the former as it is far more flexible.\n\n| Family            | Parameter                            | brms                            | rstanarm                    |\n|-------------------|--------------------------------------|---------------------------------|-----------------------------|\n| Gaussian          | Intercept                            | `student_t(3,median(y),mad(y))` | `normal(mean(y),2.5*sd(y))` |\n|                   | 'Population effects' (slopes, betas) | flat, improper priors           | `normal(0,2.5*sd(y)/sd(x))` |\n|                   | Sigma                                | `student_t(3,0,mad(y))`         | `exponential(1/sd(y))`      |\n|                   | 'Group-level effects'                | `student_t(3,0,mad(y))`         | `decov(1,1,1,1)`            |\n|                   | Correlation on group-level effects   | `ljk_corr_cholesky(1)`          |                             |\n| Poisson           | Intercept                            | `student_t(3,median(y),mad(y))` | `normal(mean(y),2.5*sd(y))` |\n|                   | 'Population effects' (slopes, betas) | flat, improper priors           | `normal(0,2.5*sd(y)/sd(x))` |\n|                   | 'Group-level effects'                | `student_t(3,0,mad(y))`         | `decov(1,1,1,1)`            |\n|                   | Correlation on group-level effects   | `ljk_corr_cholesky(1)`          |                             |\n| Negative binomial | Intercept                            | `student_t(3,median(y),mad(y))` | `normal(mean(y),2.5*sd(y))` |\n|                   | 'Population effects' (slopes, betas) | flat, improper priors           | `normal(0,2.5*sd(y)/sd(x))` |\n|                   | Shape                                | `gamma(0.01, 0.01)`             | `exponential(1/sd(y))`      |\n|                   | 'Group-level effects'                | `student_t(3,0,mad(y))`         | `decov(1,1,1,1)`            |\n|                   | Correlation on group-level effects   | `ljk_corr_cholesky(1)`          |                             |\n\n: {.primary .bordered .sm .paramsTable}\n\nNotes:\n\n`brms`\n\nhttps://github.com/paul-buerkner/brms/blob/c2b24475d727c8afd8bfc95947c18793b8ce2892/R/priors.R\n\n1. In the above, for non-Gaussian families, `y` is first transformed\n  according to the family link.  If the family link is `log`, then 0.1\n  is first added to 0 values.\n2. in `brms` the minimum standard deviation for the Intercept prior is\n   `2.5`\n3. in `brms` the minimum standard deviation for group-level priors is\n   `10`.\n\n`rstanarm`\n\nhttp://mc-stan.org/rstanarm/articles/priors.html\n\n1. in `rstanarm` priors on standard deviation and correlation\n   associated with group-level effects are packaged up into a single\n   prior (`decov` which is a decomposition of the variance and\n   covariance matrix).\n\nIn my experience, I find that the above priors tend to be a little bit\ntoo wide for many ecological applications and I often prefer to use\n1.5 rather than 2.5 as the multiplier.\n\n:::: {.callout-note}\n\nIn Bayesian models, centering of predictors offers huge numerical advantages.  So important is it to center that `brms` automatically centers any continuous predictors for you.  However, since the user has not necessarily centered the predictors, the user might misinterpret the outputs from a `brms` model.  Consequently, when fitting a model, `brms` also generates y-intercept values that are consistent with un-centered values and these are the estimates returned to the user.\n\nNevertheless, I would recommend that you should always explicitly\ncenter continuous predictors to provide more meaningful\ninterpretations of the y-intercept. I would also highly recommend\nstandardising continuous predictors - this will not only help speed up\nand stabilise the model, it will simplify the spefication of priors -\nsee the specific examples later in this tutorial.\n\n::::\n\nBased on the above, for our fabricated data, lets assign the following\npriors:\n\n- $\\beta_0$: Normal prior centred at 31.21 with a\n  variance of 15.17\n  - mean: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dat$y |> median() |> round(2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 31.21\n    ```\n    \n    \n    :::\n    :::\n\n\n  - variance: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dat$y |> mad() |> round(2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 15.17\n    ```\n    \n    \n    :::\n    :::\n\n\n- $\\beta_1$: _t_ distribution (3 degrees of freedom) prior centred at\n  0 with a variance of 4.09\n  - mean: since effects are differences and we may not want to\n    pre-determine the direction (polarity) of any trends, it is\n    typical to define effects priors with means of 0\n  - variance: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    mad(dat$y) / mad(dat$x) |> round(2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 4.090138\n    ```\n    \n    \n    :::\n    :::\n\n\n- $\\sigma$: (half) _t_ distribution (3 degrees of freedom) centred at\n  0 with a variance of 15.17\n  - variance: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dat$y |> mad() |> round(2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 15.17\n    ```\n    \n    \n    :::\n    :::\n\n\n\nNote, again, when fitting models through either `rstanarm` or `brms`,\nthe priors assume that the predictor(s) have been centred and are to\nbe applied on the link scale. In this case the link scale is an\nidentity.\n\n\n\nSimilar logic can be applied for models that employ different\ndistributions. In the following sections, we will define numerous sets\nof data (each of which represents different major forms of ecological\ndata) and see how we can set appropriate priors in each case. In\nworking through these example, it is worth reflecting on how much\nsimpler prior specification is if we use standardised predictors.\n\n\n\n# Example data\n\nThis tutorial will blend theoretical discussions with actual\ncalculations and model fits. I believe that by bridging the divide\nbetween theory and application, we all gain better understanding. The\napplied components of this tutorial will be motivated by numerous\nfabricated data sets. The advantage of simulated data over real data\nis that with simulated data, we know the 'truth' and can therefore\ngauge the accuracy of estimates.\n\nThe motivating examples are:\n\n- **Example 1** - simulated samples drawn from a Gaussian (normal)\n  distribution reminiscent of data collected on measurements (such as\n  body mass)\n- **Example 2** - simulated Gaussian samples drawn three different\n  populations representing three different treatment levels (e.g. body\n  masses of three different species)\n- **Example 3** - simulated samples drawn from a Poisson distribution\n  reminiscent of count data (such as number of individuals of a\n  species within quadrats)\n- **Example 4** - simulated samples drawn from a Negative Binomial\n  distribution reminiscent of over-dispersed count data (such as\n  number of individuals of a species that tends to aggregate in\n  groups)\n- **Example 5** - simulated samples drawn from a Bernoulli (binomial\n  with $n = 1$) distribution reminiscent of binary data (such as the\n  presence/absence of a species within sites)\n- **Example 6** - simulated samples drawn from a Binomial distribution\n  reminiscent of proportional data (such as counts of a particular\n  taxa out of a total number of individuals)\n\n::: {.panel-tabset}\n\n## Example 1 (Gaussian data)\n\nLets formally simulate the data illustrated above. The underlying\nprocess dictates that on average a one unit change in the predictor\n(`x`) will be associated with a five unit change in response (`y`) and\nwhen the predictor has a value of 0, the response will typically be 2.\nHence, the response (`y`) will be related to the predictor (`x`) via\nthe following:\n\n$$\ny = 2 + 5x\n$$\n\nThis is a deterministic model, it has no uncertainty. In order to\nsimulate actual data, we need to add some random noise. We will assume\nthat the residuals are drawn from a Gaussian distribution with a mean\nof zero and standard deviation of 4. The predictor will comprise of 10\nuniformly distributed integer values between 1 and 10. We will round\nthe response to two decimal places.\n\nFor repeatability, a seed will be employed on the random number\ngenerator. Note, the smaller the dataset, the less it is likely to\nrepresent the underlying deterministic equation, so we should keep\nthis in mind when we look at how closely our estimated parameters\napproximate the 'true' values. Hence, the seed has been chosen to\nyield data that maintain a general trend that is consistent with the\ndefining parameters.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(234)\ndat <- data.frame(x = 1:10) |>\n    mutate(y = round(2 + 5*x + rnorm(n = 10, mean = 0, sd = 4), digits = 2))\ndat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    x     y\n1   1  9.64\n2   2  3.79\n3   3 11.00\n4   4 27.88\n5   5 32.84\n6   6 32.56\n7   7 37.84\n8   8 29.86\n9   9 45.05\n10 10 47.65\n```\n\n\n:::\n\n```{.r .cell-code}\nggplot(data = dat) + \ngeom_point(aes(y = y, x = x))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/sim-1.png){width=288}\n:::\n:::\n\n\n\nWe will use these data in two ways. Firstly, to estimate the mean and\nvariance of the reponse (`y`) ignoring the predcitor (`x`) and\nsecondly to estimate the relationship between the reponse and\npredictor.\n\nFor the former, we know that the mean and variance of the response\n(`y`) can be calculated as:\n\n$$\n\\begin{align}\n\\bar{y} =& \\frac{1}{n}\\sum^n_{i=1}y_i\\\\\nvar(y) =& \\frac{1}{n}\\sum^n_{i=1}(y-\\bar{y})^2\\\\\nsd(y) =& \\sqrt{var(y)}\n\\end{align}\n$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(dat$y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 27.811\n```\n\n\n:::\n\n```{.r .cell-code}\nvar(dat$y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 225.9111\n```\n\n\n:::\n\n```{.r .cell-code}\nsd(dat$y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 15.03034\n```\n\n\n:::\n:::\n\n\n\n## Example 2 (categorical predictor)\n\nAs previously described, categorical predictors are transformed into\ndummy codes prior to the fitting of the linear model. We will simulate\na small data set with a single categorical predictor comprising a\ncontrol and two treatment levels ('mediam', 'high'). To simplify\nthings we will assume a Gaussian distribution, however most of the\nmodelling steps would be the same regardless of the chosen\ndistribution.\n\nThe data will be drawn from three Gaussian distributions with a\nstandard deviation of 4 and means of 20, 15 and 10. We will draw a\ntotal of 12 observations, four from each of the three populations.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nbeta_0 <- 20\nbeta <- c(-2, -10)\nsigma <- 4\nn <- 12\nx <- gl(3, 4, 12, labels = c('control', 'medium', 'high'))\ny <- (model.matrix(~x) %*% c(beta_0, beta)) + rnorm(12, 0, sigma)\ndat2 <- data.frame(x = x, y = y)\ndat2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         x         y\n1  control 17.758097\n2  control 19.079290\n3  control 26.234833\n4  control 20.282034\n5   medium 18.517151\n6   medium 24.860260\n7   medium 19.843665\n8   medium 12.939755\n9     high  7.252589\n10    high  8.217352\n11    high 14.896327\n12    high 11.439255\n```\n\n\n:::\n\n```{.r .cell-code}\nggplot(data = dat2) + \ngeom_point(aes(y = y, x = x)) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/sim2-1.png){width=288}\n:::\n:::\n\n\n\n## Example 3 (Poisson data)\n\nThe Poisson distribution is only parameterized by a single parameter\n($\\lambda$) which represents both the mean and variance. Furthermore,\nPoisson data can only be positive integers.  \n\nUnlike simple trend between two Gaussian or uniform distributions,\nmodelling against a Poisson distribution alters the scale to\nlogarithms. This needs to be taken into account when we simulate the\ndata. The parameters that we used to simulate the underlying processes\nneed to either be on a logarithmic scale, or else converted to a\nlogarithmic scale prior to using them for generating the random data.\n\nMoreover, for any model that involves a non-identity link function\n(such as a logarithmic link function for Poisson models), 'slope' is\nonly constant on the scale of the link function. When it is back\ntransformed onto the natural scale (scale of the data), it takes on a\ndifferent meaning and interpretation.\n\nWe will chose $\\beta_0$ to represent a value of 1 when `x=0`. As for\nthe 'effect' of the predictor on the response, lets say that for every\none unit increase in the predictor the response increases by 40% (on\nthe natural scale). Hence, on the log scale, the slope will be\n$log(1.5)=$ 0.3364722.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nbeta <- c(1, 1.40)\nbeta <- log(beta)\nn <- 10\ndat3 <- data.frame(x=seq(from = 1, to = 10, len = n)) |>\n    mutate(y = rpois(n, lambda = exp(beta[1] + beta[2]*x)))\ndat3\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    x  y\n1   1  1\n2   2  3\n3   3  2\n4   4  6\n5   5  9\n6   6  3\n7   7 10\n8   8 15\n9   9 28\n10 10 31\n```\n\n\n:::\n\n```{.r .cell-code}\nggplot(data = dat3) + \ngeom_point(aes(y = y, x = x)) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/sim3-1.png){width=288}\n:::\n:::\n\n\n\n## Example 4 (NB data)\n\nIn theory, count data should follow a Poisson distribution and\ntherefore have properties like mean equal to variance (e.g.\n$\\textnormal{Dispersion}=\\frac{\\sigma}{\\mu}=1$). However as simple\nlinear models are low dimensional representations of a system, it is\noften unlikely that such a simple model can capture all the\nvariability in the response (counts). For example, if we were\nmodelling the abundance of a species of intertidal snail within\nquadrats in relation to water depth, it is highly likely that water\ndepth alone drives snail abundance. There are countless other\ninfluences that the model has not accounted for. As a result, the\nobserved data might be more variable than a Poisson (of a particular\nmean) would expect and in such cases, the model is over-dispersed\n(more variance than expected).\n\nOver dispersed models under-estimate the variability and thus\nprecision in estimates resulting in inflated confidence in outcomes\n(elevated Type I errors).\n\nThere are numerous causes of over-dispersed count data (one of which\nis eluded to above). These are:\n\n- additional sources of variability not being accounted for in the\n  model (see above)\n- when the items being counted aggregate together. Although the\n  underlying items may have been generated by a Poisson process, the\n  items clump together. When the items are counted, they are more\n  likely to be in either in relatively low or relatively high\n  numbers - hence the data are more varied than would be expected from\n  their overall mean.\n- imperfect detection resulting in excessive zeros. Again the\n  underlying items may have been generated by a Poisson process,\n  however detecting and counting the items might not be completely\n  straight forward (particularly for more cryptic items). Hence, the\n  researcher may have recorded no individuals in a quadrat and yet\n  there was one or more present, they were just not obvious and were\n  not detected. That is, layered over the Poisson process is another\n  process that determines the detectability. So while the Poisson\n  might expect a certain proportion of zeros, the observed data might\n  have a substantially higher proportion of zeros - and thus higher\n  variance.\n\nThis example will generate data that is drawn from a negative binomial\ndistribution so as to broadly represent any one of the above causes.\n\nWe will chose $\\beta_0$ to represent a value of 1 when `x=0`. As for\nthe 'effect' of the predictor on the response, lets say that for every\none unit increase in the predictor the response increases by 40% (on\nthe natural scale). Hence, on the log scale, the slope will be\n$log(1.5)=$ 0.3364722. Finally, the dispersion parameter (ratio of\nvariance to mean) will be 10.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(234)\nbeta <- c(1, 1.40)\nbeta <- log(beta)\nn <- 10\nsize <- 10\ndat4 <- data.frame(x = seq(from = 1, to = 10, len = n)) |>\n    mutate(\n        mu = exp(beta[1] + beta[2] * x),\n        y = rnbinom(n, size = size, mu =  mu)\n    )\ndat4\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    x        mu  y\n1   1  1.400000  0\n2   2  1.960000  3\n3   3  2.744000  7\n4   4  3.841600  3\n5   5  5.378240  5\n6   6  7.529536  9\n7   7 10.541350 13\n8   8 14.757891 10\n9   9 20.661047 17\n10 10 28.925465 26\n```\n\n\n:::\n\n```{.r .cell-code}\nggplot(data = dat4) + \ngeom_point(aes(y = y, x = x)) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/sim4-1.png){width=288}\n:::\n:::\n\n\n\n## Example 5 (Binary data)\n\nBinary data (presence/absence, dead/alive, yes/no, heads/tails, etc)\npose unique challenges for linear modeling. Linear regression,\ndesigned for continuous outcomes, may not be directly applicable to\nbinary responses. The nature of binary data violates assumptions of\nnormality and homoscedasticity, which are fundamental to linear\nregression. Furthermore, linear models may predict probabilities\noutside the [0, 1] range, leading to unrealistic predictions.\n\nThis example will generate data that is drawn from a bernoulli\ndistribution so as to broadly represent presence/absence data.\n\nWe will chose $\\beta_0$ to represent the odds of a value of 1 when\n$x=0$ equal to $0.02$. This is equivalent to a probability of $y$\nbeing zero when $x=0$ of $\\frac{0.02}{1+0.02}=0.0196$. E.g., at low\n$x$, the response is likely to be close to 0. For every one unit\nincrease in $x$, we will stipulate a 2 times increase in odds that\nthe expected response is equal to 1.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(234)\nbeta <- c(0.02, 2)\nbeta <- log(beta)\nn <- 10\ndat5 <- data.frame(x = seq(from = 1, to = 10, len = n)) |>\n    mutate(\n        y = as.numeric(rbernoulli(n, p = plogis(beta[1] + beta[2] * x)))\n    )\ndat5\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    x y\n1   1 0\n2   2 0\n3   3 0\n4   4 1\n5   5 0\n6   6 1\n7   7 1\n8   8 1\n9   9 1\n10 10 1\n```\n\n\n:::\n\n```{.r .cell-code}\nggplot(data = dat5) + \ngeom_point(aes(y = y, x = x)) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/sim5-1.png){width=288}\n:::\n:::\n\n\n\n## Example 6 (Binomial data)\n\nSimilar to binary data, proportional (binomial) data tend to violate\nnormality and homogeneity of variance (particularly as mean\nproportions approach either 0% or 100%.\n\nThis example will generate data that is drawn from a binomial\ndistribution so as to broadly represent proportion data.\n\nWe will chose $\\beta_0$ to represent the odds of a particular trial\n(e.g. an individual) being of a particular type (e.g. species 1) when\n$x=0$ and to equal to $0.02$. This is equivalent to a probability of\n$y$ being of the focal type when $x=0$ of\n$\\frac{0.02}{1+0.02}=0.0196$. E.g., at low $x$, the the probability\nthat an individual is taxa 1 is likely to be close to 0. For every one\nunit increase in $x$, we will stipulate a 2.5 times increase in odds\nthat the expected response is equal to 1.\n\nFor this example, we will also convert the counts into proportions\n($y2$) by division with the number of trials ($5$).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nbeta <- c(0.02, 2.5)\nbeta <- log(beta)\nn <- 10\ntrials <- 5\ndat6 <- data.frame(x = seq(from = 1, to = 10, len = n)) |>\n    mutate(\n      count = as.numeric(rbinom(n, size = trials, prob = plogis(beta[1] + beta[2] * x))),\n      total = trials,\n      y = count/total\n    )\ndat6\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    x count total   y\n1   1     0     5 0.0\n2   2     1     5 0.2\n3   3     1     5 0.2\n4   4     4     5 0.8\n5   5     2     5 0.4\n6   6     5     5 1.0\n7   7     5     5 1.0\n8   8     4     5 0.8\n9   9     5     5 1.0\n10 10     5     5 1.0\n```\n\n\n:::\n\n```{.r .cell-code}\nggplot(data = dat6) + \ngeom_point(aes(y = y, x = x)) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/sim6-1.png){width=288}\n:::\n:::\n\n\n\n\n:::\n\n\n# Exploratory data analysis\n\nStatistical models utilize data and the inherent statistical\nproperties of distributions to discern patterns, relationships, and\ntrends, enabling the extraction of meaningful insights, predictions,\nor inferences about the phenomena under investigation. To do so,\nstatistical models make assumptions about the likely distributions\nfrom which the data were collected. Consequently, the reliability and\nvalidity of any statistical model depend upon adherence to these\nunderlying assumptions.\n\nExploratory Data Analysis (EDA) and assumption checking therefore play\npivotal roles in the process of statistical analysis, offering\nessential tools to glean insights, assess the reliability of\nstatistical methods, and ensure the validity of conclusions drawn from\ndata. EDA involves visually and statistically examining datasets to\nunderstand their underlying patterns, distributions, and potential\noutliers. These initial steps provides an intuitive understanding of\nthe data's structure and guides subsequent analyses. By scrutinizing\nassumptions, such as normality, homoscedasticity, and independence,\nresearchers can identify potential limitations or violations that may\nimpact the accuracy and reliability of their findings.\n\nExploratory Data Analysis within the context of ecological statistical\nmodels usually comprise a set of targetted graphical summaries. They\nare not to be considered definitive diagnostics of the model\nassumptions, but rather a first pass to assess the obvious violations\nprior to the fitting of models. More definitive diagnostics can only\nbe achieved after a model has been fit. \n\nIn addition to graphical summaries, there are numerous statistical\ntests to help explore possible violations of various statistical\nassumptions. These tests are less commonly used in ecology since they\nare often more sensitive to deviations from ideal than are the models\nthat we are seeking to ensure. \n\nSimple classic regression models are often the easiest models to fit\nand interpret and as such often represent a standard by which other\nalternate models are gauged. As you will see later in this tutorial,\nsuch models can actually be fit using closed form (exact solution)\nmatrix algebra that can be performed by hand. Nevertheless, and\nperhaps as a result, they also impose some of the strictest\nassumptions. Although these collective assumptions are specific to\ngaussian models, they do provide a good introduction to model\nassumptions in general, so we will use them to motivate the more wider\ndiscussion.\n\nSimple (gaussian) linear models (represented below) make the following\nassumptions:\n\n::: {.columns}\n\n:::: {.column width=\"65%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/model1-1.png){width=576}\n:::\n:::\n\n\n \n\n::::\n\n:::: {.column width=\"35%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/assumptions,-1.png){width=288}\n:::\n:::\n\n\n\n::::\n\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n### Notes on the data depicted above\nThe data depicted above where generated using the following R codes:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(234)\nx <- 1:10\ny <- 2 + (5*x) + rnorm(10,0,4)\n```\n:::\n\n\n\nThe observations represent\n\n- single observations drawn from 10 normal populations\n- each population had a standard deviation of 4\n- the mean of each population varied linearly according to the value of x ($2 + 5x$)\n:::\n\n- **normality**: the residuals (and thus observations) must be drawn\n  from populations that are normal distribution. _The right hand figure\n  underlays the ficticious normally distributed populations from which\n  the observed values have been sampled_.\n  \n:::: {.indented}\n\n::: {.callout-note collapse=\"true\"}\n### More information about assessing normality\n\nEstimation and inference testing in linear regression assumes that the\nresponse is normally distributed in each of the populations. In this\ncase, the populations are all possible measurements that could be\ncollected at each level of $x$ - hence there are 16 populations.\nTypically however, we only collect a single observation from each\npopulation (as is also the case here). How then can be evaluate\nwhether each of these populations are likely to have been normal?\n\n::::: {.columns}\n\n:::::: {.column width=\"45%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/assumptions1a-1.png){width=384}\n:::\n:::\n\n\n\n::::::\n\n:::::: {.column width=\"45%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/assumptions1b-1.png){width=384}\n:::\n:::\n\n\n\n::::::\n:::::\n\nFor a given response, the population distributions should follow much\nthe same distribution shapes. Therefore provided the single samples\nfrom each population are unbiased representations of those\npopulations, a boxplot of all observations should reflect the\npopulation distributions.\n\nThe two figures above show the relationships between the individual\npopulation distributions and the overall distribution. The left hand\nfigure shows a distribution drawn from single representatives of each\nof the 16 populations. Since the 16 individual populations were\nnormally distributed, the distribution of the 16 observations is also\nnormal.\n\nBy contrast, the right hand figure shows 16 log-normally distributed\npopulations and the resulting distribution of 16 single observations\ndrawn from these populations. The overall boxplot mirrors each of the\nindividual population distributions.\n\nWhilst traditionally, non-normal data would typically be\n**normalised** via a scale transformation (such as a logarithmic\ntransformation), these days it is arguably more appropriate to attempt\nto match the data to a more suitable distribution (see later in this\ntutorial).\n\nYou may have noticed that we have only explored the distribution of\nthe response (y-axis). What about the distribution of the predictor\n(independent, x-axis) variable, does it matter? The distribution\nassumption applies to the residuals (which as purely in the direction\nof the y-axis). Indeed technically, it is assumed that there is no\nuncertainty associated with the predictor variable. They are assumed\nto be set and thus there is no error associated with the values\nobserved. Whilst this might not always be reasonable, it is an\nassumption.\n\nGiven that the predictor values are expected to be _set_ rather than\n_measured_, we actually assume that they are **uniformly**\ndistributed. In practice, the exact distribution of predictor values\nis not that important provided it is reasonably symmetrical and no\noutliers (unusually small or large values) are created as a result of\nthe distribution.\n\nAs with exploring the distribution of the response variable, boxplots,\nhistograms and density plots can be useful means of exploring the\ndistribution of predictor variable(s). When such diagnostics reveal\ndistributional issues, scale transformations (such as logarithmic\ntransformations) are appropriate.\n\n:::\n\n::::\n\n- **homogeneity of variance**: the residuals (and thus observations)\n  must be drawn from populations that are equally varied. The model as\n  shown only estimates a single variance ($\\sigma^2$) parameter - it is\n  assumed that this is a good overall representation of all underlying\n  populations. _The right hand figure underlays the ficticious normally\n  distributed and equally varied populations from which the\n  observations have been sampled_.\n  \n  Moreover, since the expected values (obtained by solving the\n  deterministic component of the model) and the variance must be\n  estimated from the same data, they need to be independent (not\n  related one another)\n\n:::: {.indented}\n\n::: {.callout-note collapse=\"true\"}\n### More information about assessing homogeneity of variance\n\nSimple linear regression also assumes that each of the populations are\nequally varied. Actually, it is the prospect of a relationship between\nthe mean and variance of y-values across x-values that is of the\ngreatest concern. Strictly the assumption is that the distribution of\ny values at each x value are equally varied and that there is no\nrelationship between mean and variance.\n\nHowever, as we only have a single y-value for each x-value, it is\ndifficult to directly determine whether the assumption of homogeneity\nof variance is likely to have been violated (mean of one value is\nmeaningless and variability can't be assessed from a single value).\nThe figure below depicts the ideal (and almost never realistic)\nsituation in which (left hand figure) the populations are all equally\nvaried. The middle figure simulates drawing a single observation from\neach of the populations. When the populations are equally varied, the\nspread of observed values around the trend line is fairly even - that\nis, there is no trend in the spread of values along the line.\n\nIf we then plot the residuals (difference between observed values and\nthose predicted by the trendline) against the predict values, there is\na definite lack of pattern. This lack of pattern is indicative of a\nlack of issues with homogeneity of variance.\n\n::::: {.columns}\n\n:::::: {.column width=\"32%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/assumptions2a-1.png){width=240}\n:::\n:::\n\n\n\n::::::\n\n:::::: {.column width=\"32%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/assumptions2b-1.png){width=240}\n:::\n:::\n\n\n\n::::::\n\n:::::: {.column width=\"32%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/assumptions2c-1.png){width=240}\n:::\n:::\n\n\n\n::::::\n:::::\n\n\nIf we now contrast the above to a situation where the population\nvariance is related to the mean (unequal variance), we see that the\nobservations drawn from these populations are not evenly distributed\nalong the trendline (they get more spread out as the mean predicted\nvalue increase). This pattern is emphasized in the residual plot which\ndisplays a characteristic \"wedge\"-shape pattern.\n\n::::: {.columns}\n\n:::::: {.column width=\"32%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/assumptions2d-1.png){width=240}\n:::\n:::\n\n\n\n::::::\n\n:::::: {.column width=\"32%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/assumptions2e-1.png){width=240}\n:::\n:::\n\n\n\n::::::\n\n:::::: {.column width=\"32%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/assumptions2f-1.png){width=240}\n:::\n:::\n\n\n\n::::::\n:::::\n\nHence looking at the spread of values around a trendline on a\nscatterplot of $y$ against $x$ is a useful way of identifying gross\nviolations of homogeneity of variance. Residual plots provide an even\nbetter diagnostic. The presence of a wedge shape is indicative that\nthe population mean and variance are related.\n:::\n\n::::\n\n- **linearity**: the underlying relationships must be simple linear\n  trends, since the line of best fit through the data (of which the\n  slope is estimated) is linear. _The right hand figure depicts a\n  linear trend through the underlying populations_.\n\n:::: {.indented}\n\n::: {.callout-note collapse=\"true\"}\n### More information about assessing linearity\n\nIt is important to disclose the meaning of the word \"linear\" in the\nterm \"linear regression\". Technically, it refers to a _linear_\ncombination of regression coefficients. For example, the following are\nexamples of linear models:\n\n- $y_i = \\beta_0 + \\beta_1 x_i$\n- $y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 z_i$\n- $y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x^2_i$\n\nAll the coefficients ($\\beta_0$, $\\beta_1$, $\\beta_2$) are linear\nterms. Note that the last of the above examples, is a linear model,\nhowever it describes a non-linear trend. Contrast the above models\nwith the following non-linear model:\n\n- $y_i = \\beta_0 + x_i^{\\beta_1}$\n\nIn that case, the coefficients are not linear combinations (one of\nthem is a power term).\n\n\nThat said, a simple linear regression usually fits a straight (linear)\nline through the data. Therefore, prior to fitting such a model, it is\nnecessary to establish whether this really is the most sensible way of\ndescribing the relationship. That is, does the relationship appear to\nbe linearly related or could some other non-linear function describe\nthe relationship better. Scatterplots and residual plots are useful\ndiagnostics.\n\nTo see how a residual plot could be useful, consider the following.\nThe first row of figures illustrate the residuals resulting from data\ndrawn from a linear trend. The residuals are effectively random noise.\nBy contrast, the second row show the residuals resulting from data\ndrawn from a non-normal relationship that have nevertheless been\nmodelled as a linear trend. There is still a clear pattern remaining\nin the residuals.\n\n::::: {.columns}\n\n:::::: {.column width=\"32%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/assumptions2a-1.png){width=240}\n:::\n:::\n\n\n\n::::::\n\n:::::: {.column width=\"32%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/assumptions2b-1.png){width=240}\n:::\n:::\n\n\n\n::::::\n\n:::::: {.column width=\"32%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/assumptions2c-1.png){width=240}\n:::\n:::\n\n\n\n::::::\n:::::\n\n::::: {.columns}\n\n:::::: {.column width=\"32%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/assumptions2a2-1.png){width=240}\n:::\n:::\n\n\n\n::::::\n\n:::::: {.column width=\"32%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/assumptions2b2-1.png){width=240}\n:::\n:::\n\n\n\n::::::\n\n:::::: {.column width=\"32%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/assumptions2c2-1.png){width=240}\n:::\n:::\n\n\n\n::::::\n:::::\n\nThe above might be an obvious and somewhat overly contrived example,\nyet it does illustrate the point - that a pattern in the residuals\ncould point to a mis-specified model.\n\nIf non-linearity does exist (as in the second case above) , then\nfitting a straight line through what is obviously not a straight\nrelationship is likely to poorly represent the true nature of the\nrelationship. There are numerous causes of non-linearity:\n\n1. underlying distributional issues can result in non-linearity. For\n   example, if we are assuming a gaussian distribution and the data\n   are non-normal, often the relationships will appear non-linear.\n   Addressing the distributional issues can therefore resolve the\n   linearity issues\n2. the underlying relationship might truly be non-linear in which case\n   this should be reflected in some way by the model formula. If the\n   model formula fails to describe the non-linear trend, then problems\n   will persist.\n3. the model proposed is missing an important covariate that might\n   help standardise the data in a way that results in linearity\n\n\n:::\n\n::::\n\n- **independence**: the residuals (and thus observations) must be\n  independently drawn from the populations. That is, the correlation\n  between all the observations is assumed to be 0 (off-diagonals in\n  the covariance matrix). More practically, there should be no\n  pattern to the correlations between observations.\n\n  Random sampling and random treatment assignment are experimental\n  design elements that are intended to mitigate many types of sampling\n  biases that cause dependencies between observations. Nevertheless,\n  there are aspects of sampling designs that are either logistically\n  difficult to randomise or in some cases not logically possible. For\n  example, the residuals from observations sampled closer together in\n  space and time will likely be more similar to one another than those\n  of observations more spaced apart. Since neither space nor time can\n  be randomised, data collected from sampling designs that involve\n  sampling over space and/or time need to be assess for spatial and\n  temporal dependencies. These concepts will be explored in the\n  context of introducing susceptible designs in a later tutorial.\n\nThe above is only a very brief overview of the model assumptions that\napply to just one specific model (simple linear gaussian regression).\nFor the remainder of this section, we will graphically explore the two\nmotivating example data sets so as gain insights into what\ndistributional assumptions might be most valid, and thus help guide\nmodelling choices. Similarly, for subsequent tutorials in this series\n(that introduce progressively more complex models), all associated\nassumptions will be explored and detailed.\n\n::: {.panel-tabset}\n\n## Example 1 (Gaussian data)\n\n:::: {.panel-tabset}\n\n### Normality\n\n::::: {.columns}\n:::::: {.column width=\"48%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat |> \n  ggplot(aes(y = y)) +\n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/eda1a-1.png){width=288}\n:::\n:::\n\n\n\n:::::: \n\n:::::: {.column width=\"48%\"}\n**Conclusions**\n\n- there is no strong evidence of non-normality\n- to be convincing evidence of non-normality, each segment of the\n  boxplot should get progressively larger\n\n:::::: \n::::: \n\n### Homogeneity of variance\n\n::::: {.columns}\n:::::: {.column width=\"48%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat |> \n  ggplot(aes(y = y, x = x)) +\n  geom_smooth(method = \"lm\") +\n  geom_point()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/eda1b-1.png){width=288}\n:::\n:::\n\n\n\n:::::: \n\n:::::: {.column width=\"48%\"}\n**Conclusions**\n\n- the spread of values around the trendline seems fairly even (hence\n  it there is no evidence of non-homogeneity\n\n:::::: \n::::: \n\n### Linearity\n\n::::: {.columns}\n:::::: {.column width=\"48%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat |> \n  ggplot(aes(y = y, x = x)) +\n  geom_smooth(method = \"lm\") +\n  geom_point()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/eda1c-1.png){width=288}\n:::\n:::\n\n\n\n:::::: \n\n:::::: {.column width=\"48%\"}\n**Conclusions**\n\n- the data seems well represented by the linear trendline.\n  Furthermore, the lowess smoother does not appear to have a\n  consistent shift trajectory.\n\n:::::: \n::::: \n::::\n\n\n**Conclusions**\n\n- there are no obvious violations of the linear regression model assumptions\n- we can now fit the suggested model\n- full confirmation about the model's goodness of fit should be\n  reserved until after exploring the additional diagnostics that are\n  only available after fitting the model.\n\n## Example 2 (categorical predictor)\n\n:::: {.panel-tabset}\n\n### Normality\n\n::::: {.columns}\n:::::: {.column width=\"48%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2 |> \n  ggplot(aes(y = y, x = x)) +\n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/eda2a-1.png){width=288}\n:::\n:::\n\n\n::::::\n:::::: {.column width=\"48%\"}\n**Conclusions**\n\n- there is no consitent evidence of non-normality across all groups\n- even though the control group demonstrates some evidence of\n  non-normality\n:::::: \n:::::\n\n### Homogeneity of variance\n\n::::: {.columns}\n:::::: {.column width=\"48%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2 |> \n  ggplot(aes(y = y, x = x)) +\n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/eda2b-1.png){width=288}\n:::\n:::\n\n\n\n:::::: \n\n:::::: {.column width=\"48%\"}\n**Conclusions**\n\n- the spread of noise in each group seems reasonably similar\n- more importantly, there does not seem to be a relationship between\n  the mean (as approximated by the position of the boxplots along the\n  y-axis) and the variance (as approximated by the spread of the\n  boxplots).\n- that is, the size of the boxplots do not vary with the elevation of\n  the boxplots.\n\n:::::: \n::::: \n\n### Linearity\n\nLinearity is not an issue for categorical predictors since it is\neffectively fitting separate lines between pairs of points (and a line\nbetween two points can only ever be linear)....\n\n::::\n\n**Conclusions**\n\n- no evidence of non-normality\n- no evidence of non-homogeneity of variance\n\n\n## Example 3 (Poisson data)\n\n:::: {.panel-tabset}\n\n### Normality\n\n::::: {.columns}\n:::::: {.column width=\"48%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3 |> \n  ggplot(aes(y = y)) +\n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/eda3a-1.png){width=288}\n:::\n:::\n\n\n\n:::::: \n\n:::::: {.column width=\"48%\"}\n**Conclusions**\n\n- there is strong evidence of non-normality\n- each segment of the boxplot should get progressively larger\n\n:::::: \n::::: \n\n### Homogeneity of variance\n\n::::: {.columns}\n:::::: {.column width=\"48%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3 |> \n  ggplot(aes(y = y, x = x)) +\n  geom_smooth(method = \"lm\") +\n  geom_point()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/eda3b-1.png){width=288}\n:::\n:::\n\n\n\n:::::: \n\n:::::: {.column width=\"48%\"}\n**Conclusions**\n\n- the spread of noise does not look random along the line of best fit.\n- homogeneity of variance is difficult to assess in the presence of\n  distributional issues (such as non-normality in this case) as they\n  can result in non-linearity (apparent here)\n\n:::::: \n::::: \n\n### Linearity\n\n::::: {.columns}\n:::::: {.column width=\"48%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3 |> \n  ggplot(aes(y = y, x = x)) +\n  geom_smooth(method = \"lm\") +\n  geom_smooth(colour = \"red\") +\n  geom_point()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/eda3c-1.png){width=288}\n:::\n:::\n\n\n\n:::::: \n\n:::::: {.column width=\"48%\"}\n**Conclusions**\n\n- the data do not appear to be linear\n- the red line is a loess smoother and it is clear that the data are\n  not linear\n\n:::::: \n::::: \n::::\n\n\n**Conclusions**\n\n- there are obvious violations of the linear regression model assumptions\n- we should consider a different model that does not assume normality\n\n\n## Example 4 (NB data)\n\n:::: {.panel-tabset}\n\n### Normality\n\n::::: {.columns}\n:::::: {.column width=\"48%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4 |> \n  ggplot(aes(y = y)) +\n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/eda4a-1.png){width=288}\n:::\n:::\n\n\n\n:::::: \n\n:::::: {.column width=\"48%\"}\n**Conclusions**\n\n- there is strong evidence of non-normality\n- each segment of the boxplot should get progressively larger\n\n:::::: \n::::: \n\n### Homogeneity of variance\n\n::::: {.columns}\n:::::: {.column width=\"48%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4 |> \n  ggplot(aes(y = y, x = x)) +\n  geom_smooth(method = \"lm\") +\n  geom_point()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/eda4b-1.png){width=288}\n:::\n:::\n\n\n\n:::::: \n\n:::::: {.column width=\"48%\"}\n**Conclusions**\n\n- the spread of noise does not look random along the line of best fit.\n- homogeneity of variance is difficult to assess in the presence of\n  distributional issues (such as non-normality in this case) as they\n  can result in non-linearity (apparent here)\n\n:::::: \n::::: \n\n### Linearity\n\n::::: {.columns}\n:::::: {.column width=\"48%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4 |> \n  ggplot(aes(y = y, x = x)) +\n  geom_smooth(method = \"lm\") +\n  geom_smooth(colour = \"red\") +\n  geom_point()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/eda4c-1.png){width=288}\n:::\n:::\n\n\n\n:::::: \n\n:::::: {.column width=\"48%\"}\n**Conclusions**\n\n- the data do not appear to be linear\n- the red line is a loess smoother and it is clear that the data are\n  not linear\n\n:::::: \n::::: \n::::\n\n\n**Conclusions**\n\n- there are obvious violations of the linear regression model assumptions\n- we should consider a different model that does not assume normality\n\n## Example 5 (Binary data)\n\n:::: {.panel-tabset}\n\n### Normality\n\n::::: {.columns}\n:::::: {.column width=\"48%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5 |> \n  ggplot(aes(y = y)) +\n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/eda5a-1.png){width=288}\n:::\n:::\n\n\n\n:::::: \n\n:::::: {.column width=\"48%\"}\n**Conclusions**\n\n- clearly a set of 0's and 1's cant be normally distributied.\n\n:::::: \n::::: \n\n### Homogeneity of variance\n\n::::: {.columns}\n:::::: {.column width=\"48%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5 |> \n  ggplot(aes(y = y, x = x)) +\n  geom_smooth(method = \"lm\") +\n  geom_point()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/eda5b-1.png){width=288}\n:::\n:::\n\n\n\n:::::: \n\n:::::: {.column width=\"48%\"}\n**Conclusions**\n\n- the spread of noise does not look random (or equal) along the line\n  of best fit.\n\n:::::: \n::::: \n\n### Linearity\n\n::::: {.columns}\n:::::: {.column width=\"48%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5 |> \n  ggplot(aes(y = y, x = x)) +\n  geom_smooth(method = \"lm\") +\n  geom_smooth(colour = \"red\") +\n  geom_point()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/eda5c-1.png){width=288}\n:::\n:::\n\n\n\n:::::: \n\n:::::: {.column width=\"48%\"}\n**Conclusions**\n\n- the data are clearly not linear\n- the red line is a loess smoother and it is clear that the data are\n  not linear\n\n:::::: \n::::: \n::::\n\n\n**Conclusions**\n\n- there are obvious violations of the linear regression model assumptions\n- we should consider a different model that does not assume normality\n\n## Example 6 (Binomial data)\n\n:::: {.panel-tabset}\n\n### Normality\n\n::::: {.columns}\n:::::: {.column width=\"48%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6 |> \n  ggplot(aes(y = y)) +\n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/eda6a-1.png){width=288}\n:::\n:::\n\n\n\n:::::: \n\n:::::: {.column width=\"48%\"}\n**Conclusions**\n\n- distribution is not normal and is truncated\n\n:::::: \n::::: \n\n### Homogeneity of variance\n\n::::: {.columns}\n:::::: {.column width=\"48%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6 |> \n  ggplot(aes(y = y, x = x)) +\n  geom_smooth(method = \"lm\") +\n  geom_point()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/eda6b-1.png){width=288}\n:::\n:::\n\n\n\n:::::: \n\n:::::: {.column width=\"48%\"}\n**Conclusions**\n\n- the spread of noise does not look random (or equal) along the line\n  of best fit.\n\n:::::: \n::::: \n\n### Linearity\n\n::::: {.columns}\n:::::: {.column width=\"48%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6 |> \n  ggplot(aes(y = y, x = x)) +\n  geom_smooth(method = \"lm\") +\n  geom_smooth(colour = \"red\") +\n  geom_point()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/eda6c-1.png){width=288}\n:::\n:::\n\n\n\n:::::: \n\n:::::: {.column width=\"48%\"}\n**Conclusions**\n\n- although there is no evidence of non-linearity from this small data\n  set, it is worth noting that the line of best fit does extend\n  outside the logical response range [0.1] within the range of\n  observed $x$ values. That is, a simple linear model would predict\n  proportions higher than 100% at high values of $x$\n- this is a common issue with binomial data and is often addressed by\n  fitting a logistic regression model\n\n:::::: \n::::: \n::::\n\n\n**Conclusions**\n\n- there are obvious violations of the linear regression model assumptions\n- we should consider a different model that does not assume normality\n\n:::\n\n# Fitting models\n\n\n:::: {.panel-tabset}\n\n\n### Example 1 (Gaussian data)\n\n::::::: {.panel-tabset}\n#### Raw predictor\n::::: {.panel-tabset}\n\n##### Define priors\n\nOne way to assess the priors is to have the MCMC sampler sample purely\nfrom the prior predictive distribution without conditioning on the\nobserved data. Doing so provides a glimpse at the range of predictions\npossible under the priors. On the one hand, wide ranging predictions\nwould ensure that the priors are unlikely to influence the actual\npredictions once they are conditioned on the data. On the other hand,\nif they are too wide, the sampler is being permitted to traverse into\nregions of parameter space that are not logically possible in the\ncontext of the actual underlying ecological context. Not only could\nthis mean that illogical parameter estimates are possible, when the\nsampler is traversing regions of parameter space that are not\nsupported by the actual data, the sampler can become unstable and have\ndifficulty.\n\nIn `brms`, we can inform the sampler to draw from the prior predictive\ndistribution instead of conditioning on the response, by running the\nmodel with the `sample_prior = 'only'` argument. Unfortunately, this\ncannot be applied when there are flat priors (since the posteriors\nwill necessarily extend to negative and positive infinity). Therefore,\nin order to use this useful routine, we need to make sure that we have\ndefined a proper prior for all parameters.\n\nEarlier we suggested the following priors might be useful:\n\n- $\\beta_0$: Normal prior centred at 31.21 with a\n  variance of 15.17\n  - mean: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dat$y |> median() |> round(2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 31.21\n    ```\n    \n    \n    :::\n    :::\n\n\n  - variance: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dat$y |> mad() |> round(2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 15.17\n    ```\n    \n    \n    :::\n    :::\n\n\n- $\\beta_1$: _t_ distribution (3 degrees of freedom) prior centred at\n  0 with a variance of 4.09\n  - mean: since effects are differences and we may not want to\n    pre-determine the direction (polarity) of any trends, it is\n    typical to define effects priors with means of 0\n  - variance: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    mad(dat$y) / mad(dat$x) |> round(2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 4.090138\n    ```\n    \n    \n    :::\n    :::\n\n\n \n- $\\sigma$: (half) _t_ distribution (3 degrees of freedom) centred at\n  0 with a variance of 15.17\n  - variance: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dat$y |> mad() |> round(2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 15.17\n    ```\n    \n    \n    :::\n    :::\n\n\n\nIt might be use usefull to understand what some of these distributions\nlook like. For example, we have used both a normal (Gaussian)\ndistribution and a flatter _t_ distribution for y-intercept and slope\nrespectively. This was a somewhat arbitrary choice. We could easily\nhave gone with either normal or _t_ distributions for all of the above\nparameters. To visualise prior distributions for the slope based on\nboth normal and _t_ distributions:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstandist::visualize(\"normal(0, 4.09)\", \"student_t(3, 0, 4.09)\", xlim = c(-20, 20))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/normal_prior-1.png){width=384}\n:::\n:::\n\n\n\nEvidently, the _t_ distribution (with 3 degrees of freedom) is wider\nthan the normal distribution. The former should be more robust to data\nwith values that are less concentrated around the mean.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npriors <- prior(normal(31.21, 15.17), class = \"Intercept\") +\n    prior(student_t(3, 0, 4.09), class = \"b\") +\n    prior(student_t(3, 0, 15.17), class = \"sigma\")\n```\n:::\n\n\n\n\n##### Fit the model\n$$\n\\begin{align}\ny_i \\sim{}& N(\\mu_i, \\sigma^2)\\\\\n\\mu_i =& \\beta_0 + \\beta_1 x_i\\\\\n\\end{align}\n$$\n\n1. start by fitting the model and sampling from the priors only\n\n::: {.indented}\n    \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- data.frame(y = rnorm(10), x = rnorm(10))\nbrm(y ~ x, data = dat, backend = \"rstan\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 7e-06 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.013 seconds (Warm-up)\nChain 1:                0.013 seconds (Sampling)\nChain 1:                0.026 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 3e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.013 seconds (Warm-up)\nChain 2:                0.013 seconds (Sampling)\nChain 2:                0.026 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 3e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.013 seconds (Warm-up)\nChain 3:                0.013 seconds (Sampling)\nChain 3:                0.026 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 3e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.013 seconds (Warm-up)\nChain 4:                0.013 seconds (Sampling)\nChain 4:                0.026 seconds (Total)\nChain 4: \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ x \n   Data: dat (Number of observations: 10) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.27      0.37    -0.45     1.01 1.00     2264     1483\nx            -0.10      0.44    -0.98     0.82 1.00     2351     1997\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.09      0.33     0.66     1.92 1.00     2182     2260\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1a.brm <- brm(bf(y ~ x),\n                data=dat,\n                prior=priors,\n                sample_prior = 'only', \n                iter = 5000,\n                warmup = 1000,\n                chains = 3, cores = 3,\n                thin = 5,\n                backend = \"rstan\",\n                refresh = 0)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n\n\n:::\n\n2. explore the range of posterior predictions resulting from the\n   priors alone\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1a.brm |>\n    conditional_effects() |>\n    plot(points = TRUE)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects_1a-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions:** \n\n- the grey ribbon above represents the credible range of the posterior\n  predictions\n- this ribbon is substantially wider than the observed data (black\n  points), suggesting that the priors are not overly narrow\n- the range of posterior predictions are not widely unreasonable\n  (although we could argue that negative predictions might be\n  illogical) since the maximums are not multiple orders of magnitude\n  above the observed data.\n\n:::\n\n3. now refit the model such that it samples from both the priors and\n   likelihood (that is allow the data to have an impact on the\n   estimates)\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1a.brm2 <- update(dat1a.brm,\n    sample_prior = \"yes\"\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe desired updates require recompiling the model\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n\n4. re-explore the range of posterior predictions resulting from the\n   fitted model\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1a.brm2 |>\n    conditional_effects() |>\n    plot(points = TRUE)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects2_1a-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions:** \n\n- the range of the posteriors are now substantially reduced (now that\n  the model includes both data and priors)\n- this suggests that the patterns are being driven predominantly by the data\n\n:::\n\n5. compare the priors and posteriors to further confirm that the\n   priors are not overly influential\n   \n::: {.indented}\n\nWhen we have indicated that the posterior should be informed by both\nthe prior and the posterior, both prior (governed by priors alone) and\nposterior (governed by both priors and data/likelihood) draws are\nreturned.  These can be compared by exploring the probabilities\nassociated with specific hypotheses - the most obvious of which is\nthat of no effect (that the parameter = 0).\n\nWhen doing so, ideally the posterior should be distinct from the\nprior. By distinct, I mean that the posteriors should be clearly\ndistinguishable from the priors. Ideally, the priors should be less\nvariable than the priors.  If this is not the case, it might suggest\nthat the posteriors are being too strongly driven by the priors.\n\n\n\n::: {.cell labels='brms_prior_posterior_1a'}\n\n```{.r .cell-code}\ndat1a.brm2 |> tidybayes::get_variables()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"     \"b_x\"             \"sigma\"           \"Intercept\"      \n [5] \"prior_Intercept\" \"prior_b\"         \"prior_sigma\"     \"lprior\"         \n [9] \"lp__\"            \"accept_stat__\"   \"stepsize__\"      \"treedepth__\"    \n[13] \"n_leapfrog__\"    \"divergent__\"     \"energy__\"       \n```\n\n\n:::\n\n```{.r .cell-code}\ndat1a.brm2 |> hypothesis(\"x = 0\") |> plot()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/unnamed-chunk-65-1.png){width=384}\n:::\n\n```{.r .cell-code}\ndat1a.brm2 |> hypothesis(\"sigma = 0\", class = \"\") |> plot()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/unnamed-chunk-65-2.png){width=384}\n:::\n:::\n\n\n\nUnfortunately, it is not possible to do this comparison sensibly for\nthe intercept.  The reason for this is that the prior for intercept\nwas applied to an intercept that is associated with centred\ncontinuous predictors (predictors are centred behind the scenes).\nSince we did not centre the predictor, the intercept returned is as if\nuncentred.  Hence, the prior and posterior are on different scales.\n \n\n\n::: {.cell mhidden='true'}\n\n```{.r .cell-code}\ndat1a.brm2 |> tidybayes::get_variables()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"     \"b_x\"             \"sigma\"           \"Intercept\"      \n [5] \"prior_Intercept\" \"prior_b\"         \"prior_sigma\"     \"lprior\"         \n [9] \"lp__\"            \"accept_stat__\"   \"stepsize__\"      \"treedepth__\"    \n[13] \"n_leapfrog__\"    \"divergent__\"     \"energy__\"       \n```\n\n\n:::\n\n```{.r .cell-code}\ndat1a.brm2 |> SUYR_prior_and_posterior()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/fitModel2k-1.png){width=768}\n:::\n:::\n\n\n\n**Conclusions:**\n\n- each of the priors are substantially wider than the posteriors\n- the posteriors are definitely distinct from their respective priors\n  and thus we can conclude that the priors and not driving the\n  posteriors (e.g. they are not influencing the outcomes)\n- the priors are simply regularising the parameters such that they are\n  only sampled from plausible regions\n\n:::\n\n:::::\n\n#### Centered predictor\n\n::::: {.panel-tabset}\n\n##### Define priors\n\nUnless you explicitly direct `brm` to include a user-defined\nintercept, the priors on the default intercept should assume that the\npredictor(s) are centered (because `brm` will automatically center all\ncontinuous predictors).\n\nLets try the following priors:\n\n- $\\beta_0$: Normal prior centred at 0.61 with a\n  variance of 0.48\n  - mean: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dat$y |> median() |> round(2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 0.61\n    ```\n    \n    \n    :::\n    :::\n\n\n  - variance: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dat$y |> mad() |> round(2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 0.48\n    ```\n    \n    \n    :::\n    :::\n\n\n- $\\beta_1$: _t_ distribution (3 degrees of freedom) prior centered at\n  0 with a variance of 0.75\n  - mean: since effects are differences and we may not want to\n    pre-determine the direction (polarity) of any trends, it is\n    typical to define effects priors with means of 0\n  - variance: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    mad(dat$y) / mad(dat$x) |> round(2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 0.7518247\n    ```\n    \n    \n    :::\n    :::\n\n\n \n- $\\sigma$: (half) _t_ distribution (3 degrees of freedom) centred at\n  0 with a variance of 0.48\n  - variance: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dat$y |> mad() |> round(2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 0.48\n    ```\n    \n    \n    :::\n    :::\n\n::: {.cell}\n\n```{.r .cell-code}\npriors <- prior(normal(31.21, 15.17), class = \"Intercept\") +\n    prior(student_t(3, 0, 4.09), class = \"b\") +\n    prior(student_t(3, 0, 15.17), class = \"sigma\")\n```\n:::\n\n\n\n\n##### Fit the model\n$$\n\\begin{align}\ny_i \\sim{}& N(\\mu_i, \\sigma^2)\\\\\n\\mu_i =& \\beta_0 + \\beta_1 (x_i - \\bar{x})\\\\\n\\end{align}\n$$\n\n1. start by fitting the model and sampling from the priors only\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1b.brm <- brm(bf(y ~ scale(x, scale = FALSE)),\n                data=dat,\n                prior=priors,\n                sample_prior = 'only', \n                iter = 5000,\n                warmup = 1000,\n                chains = 3, cores = 3,\n                thin = 5,\n                backend = \"rstan\",\n                refresh = 0)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n\n\n:::\n\n2. explore the range of posterior predictions resulting from the\n   priors alone\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1b.brm |>\n    conditional_effects() |>\n    plot(points = TRUE)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects_1b-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions:** \n\n- the grey ribbon above represents the credible range of the posterior\n  predictions\n- this ribbon is substantially wider than the observed data (black\n  points), suggesting that the priors are not overly narrow\n- the range of posterior predictions are not widely unreasonable\n  (although we could argue that negative predictions might be\n  illogical) since the maximums are not multiple orders of magnitude\n  above the observed data.\n\n:::\n\n3. now refit the model such that it samples from both the priors and\n   likelihood (that is allow the data to have an impact on the\n   estimates)\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1b.brm2 <- update(dat1b.brm,\n    sample_prior = \"yes\"\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe desired updates require recompiling the model\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n\n4. re-explore the range of posterior predictions resulting from the\n   fitted model\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1b.brm2 |>\n    conditional_effects() |>\n    plot(points = TRUE) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects2_1b-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions:** \n\n- the range of the posteriors are now substantially reduced (now that\n  the model includes both data and priors)\n- this suggests that the patterns are being driven predominantly by the data\n\n:::\n\n5. compare the priors and posteriors to further confirm that the\n   priors are not overly influential\n   \n::: {.indented}\n\nWhen we have indicated that the posterior should be informed by both\nthe prior and the posterior, both prior (governed by priors alone) and\nposterior (governed by both priors and data/likelihood) draws are\nreturned.  These can be compared by exploring the probabilities\nassociated with specific hypotheses - the most obvious of which is\nthat of no effect (that the parameter = 0).\n\nWhen doing so, ideally the posterior should be distinct from the\nprior. By distinct, I mean that the posteriors should be clearly\ndistinguishable from the priors. Ideally, the priors should be less\nvariable than the priors.  If this is not the case, it might suggest\nthat the posteriors are being too strongly driven by the priors.\n\n\n\n::: {.cell labels='brms_prior_posterior_1b'}\n\n```{.r .cell-code}\ndat1b.brm2 |> tidybayes::get_variables() \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"          \"b_scalexscaleEQFALSE\" \"sigma\"               \n [4] \"Intercept\"            \"prior_Intercept\"      \"prior_b\"             \n [7] \"prior_sigma\"          \"lprior\"               \"lp__\"                \n[10] \"accept_stat__\"        \"stepsize__\"           \"treedepth__\"         \n[13] \"n_leapfrog__\"         \"divergent__\"          \"energy__\"            \n```\n\n\n:::\n\n```{.r .cell-code}\ndat1b.brm2 |> hypothesis(\"scalexscaleEQFALSE = 0\") |> plot()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/unnamed-chunk-75-1.png){width=384}\n:::\n\n```{.r .cell-code}\ndat1b.brm2 |> hypothesis(\"sigma = 0\", class = \"\") |> plot()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/unnamed-chunk-75-2.png){width=384}\n:::\n:::\n\n\n\nUnfortunately, it is not possible to do this comparison sensibly for\nthe intercept.  The reason for this is that the prior for intercept\nwas applied to an intercept that is associated with centred\ncontinuous predictors (predictors are centred behind the scenes).\nSince we did not centre the predictor, the intercept returned is as if\nuncentred.  Hence, the prior and posterior are on different scales.\n \n\n\n::: {.cell mhidden='true'}\n\n```{.r .cell-code}\ndat1b.brm2 |> tidybayes::get_variables() \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"          \"b_scalexscaleEQFALSE\" \"sigma\"               \n [4] \"Intercept\"            \"prior_Intercept\"      \"prior_b\"             \n [7] \"prior_sigma\"          \"lprior\"               \"lp__\"                \n[10] \"accept_stat__\"        \"stepsize__\"           \"treedepth__\"         \n[13] \"n_leapfrog__\"         \"divergent__\"          \"energy__\"            \n```\n\n\n:::\n\n```{.r .cell-code}\ndat1b.brm2 |> SUYR_prior_and_posterior()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/fitModel1b-1.png){width=768}\n:::\n:::\n\n\n\n**Conclusions:**\n\n- each of the priors are substantially wider than the posteriors\n- the posteriors are definitely distinct from their respective priors\n  and thus we can conclude that the priors and not driving the\n  posteriors (e.g. they are not influencing the outcomes)\n- the priors are simply regularising the parameters such that they are\n  only sampled from plausible regions\n\n:::\n\n:::::\n\n#### Standardised predictor\n\n::::: {.panel-tabset}\n\n##### Define priors\n\nWhen the predictor is standardised, it simplifies prior definition\nbecause we no longer need to consider the scale of the predictor.\n\nLets try the following priors:\n\n- $\\beta_0$: Normal prior centred at 0.61 with a\n  variance of 0.48\n  - mean: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dat$y |> median() |> round(2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 0.61\n    ```\n    \n    \n    :::\n    :::\n\n\n  - variance: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dat$y |> mad() |> round(2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 0.48\n    ```\n    \n    \n    :::\n    :::\n\n\n- $\\beta_1$: _t_ distribution (3 degrees of freedom) prior centered at\n  0 with a variance of 0.48\n  - mean: since effects are differences and we may not want to\n    pre-determine the direction (polarity) of any trends, it is\n    typical to define effects priors with means of 0\n  - variance: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    mad(dat$y) |> round(2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 0.48\n    ```\n    \n    \n    :::\n    :::\n\n\n \n- $\\sigma$: (half) _t_ distribution (3 degrees of freedom) centred at\n  0 with a variance of 0.48\n  - variance: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dat$y |> mad() |> round(2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 0.48\n    ```\n    \n    \n    :::\n    :::\n\n::: {.cell}\n\n```{.r .cell-code}\npriors <- prior(normal(31.21, 15.17), class = \"Intercept\") +\n    prior(student_t(3, 0, 15.17), class = \"b\") +\n    prior(student_t(3, 0, 15.17), class = \"sigma\")\n```\n:::\n\n\n\n\n##### Fit the model\n\n$$\n\\begin{align}\ny_i \\sim{}& N(\\mu_i, \\sigma^2)\\\\\n\\mu_i =& \\beta_0 + \\beta_1 (x_i - \\bar{x})/\\sigma_{x}\\\\\n\\end{align}\n$$\n\n1. start by fitting the model and sampling from the priors only\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1c.brm <- brm(bf(y ~ scale(x)),\n                data=dat,\n                prior=priors,\n                sample_prior = 'only', \n                iter = 5000,\n                warmup = 1000,\n                chains = 3, cores = 3,\n                thin = 5,\n                backend = \"rstan\",\n                refresh = 0)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n\n\n:::\n\n2. explore the range of posterior predictions resulting from the\n   priors alone\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1c.brm |>\n    conditional_effects() |>\n    plot(points = TRUE)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects_1c-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions:** \n\n- the grey ribbon above represents the credible range of the posterior\n  predictions\n- this ribbon is substantially wider than the observed data (black\n  points), suggesting that the priors are not overly narrow\n- the range of posterior predictions are not widely unreasonable\n  (although we could argue that negative predictions might be\n  illogical) since the maximums are not multiple orders of magnitude\n  above the observed data.\n\n:::\n\n3. now refit the model such that it samples from both the priors and\n   likelihood (that is allow the data to have an impact on the\n   estimates)\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1c.brm2 <- update(dat1c.brm,\n    sample_prior = \"yes\"\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe desired updates require recompiling the model\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n\n\n:::\n\n4. re-explore the range of posterior predictions resulting from the\n   fitted model\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1c.brm2 |>\n    conditional_effects() |>\n    plot(points = TRUE)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects2_1c-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions:** \n\n- the range of the posteriors are now substantially reduced (now that\n  the model includes both data and priors)\n- this suggests that the patterns are being driven predominantly by the data\n\n:::\n\n5. compare the priors and posteriors to further confirm that the\n   priors are not overly influential\n   \n::: {.indented}\n\nWhen we have indicated that the posterior should be informed by both\nthe prior and the posterior, both prior (governed by priors alone) and\nposterior (governed by both priors and data/likelihood) draws are\nreturned.  These can be compared by exploring the probabilities\nassociated with specific hypotheses - the most obvious of which is\nthat of no effect (that the parameter = 0).\n\nWhen doing so, ideally the posterior should be distinct from the\nprior. By distinct, I mean that the posteriors should be clearly\ndistinguishable from the priors. Ideally, the priors should be less\nvariable than the priors.  If this is not the case, it might suggest\nthat the posteriors are being too strongly driven by the priors.\n\n\n\n::: {.cell labels='brms_prior_posterior_1c'}\n\n```{.r .cell-code}\ndat1c.brm2 |> tidybayes::get_variables()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"     \"b_scalex\"        \"sigma\"           \"Intercept\"      \n [5] \"prior_Intercept\" \"prior_b\"         \"prior_sigma\"     \"lprior\"         \n [9] \"lp__\"            \"accept_stat__\"   \"stepsize__\"      \"treedepth__\"    \n[13] \"n_leapfrog__\"    \"divergent__\"     \"energy__\"       \n```\n\n\n:::\n\n```{.r .cell-code}\ndat1c.brm2 |> hypothesis(\"scalex = 0\") |> plot()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/unnamed-chunk-85-1.png){width=384}\n:::\n\n```{.r .cell-code}\ndat1c.brm2 |> hypothesis(\"sigma = 0\", class = \"\") |> plot()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/unnamed-chunk-85-2.png){width=384}\n:::\n:::\n\n\n\nUnfortunately, it is not possible to do this comparison sensibly for\nthe intercept.  The reason for this is that the prior for intercept\nwas applied to an intercept that is associated with centred\ncontinuous predictors (predictors are centred behind the scenes).\nSince we did not centre the predictor, the intercept returned is as if\nuncentred.  Hence, the prior and posterior are on different scales.\n \n\n\n::: {.cell mhidden='true'}\n\n```{.r .cell-code}\ndat1c.brm2 |> tidybayes::get_variables()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"     \"b_scalex\"        \"sigma\"           \"Intercept\"      \n [5] \"prior_Intercept\" \"prior_b\"         \"prior_sigma\"     \"lprior\"         \n [9] \"lp__\"            \"accept_stat__\"   \"stepsize__\"      \"treedepth__\"    \n[13] \"n_leapfrog__\"    \"divergent__\"     \"energy__\"       \n```\n\n\n:::\n\n```{.r .cell-code}\ndat1c.brm2 |> SUYR_prior_and_posterior()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/fitModel1c-1.png){width=768}\n:::\n:::\n\n\n\n**Conclusions:**\n\n- each of the priors are substantially wider than the posteriors\n- the posteriors are definitely distinct from their respective priors\n  and thus we can conclude that the priors and not driving the\n  posteriors (e.g. they are not influencing the outcomes)\n- the priors are simply regularising the parameters such that they are\n  only sampled from plausible regions\n\n:::\n\n:::::\n\n:::::::\n\n### Example 2 (categorical predictor)\n\n::::::: {.panel-tabset}\n#### Treatment contrasts\n::::: {.panel-tabset}\n\n##### Define priors\n\nLets try the following priors:\n\n\n\n::: {.cell}\n\n:::\n\n\n\n- $\\beta_0$: Normal prior centred at 19.68 with a\n  variance of 1.87\n  - mean: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dat2 |>\n      group_by(x) |>\n        summarise(across(y, list(med = median, sd = sd, mad = mad)))\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    # A tibble: 3 × 4\n      x       y_med  y_sd y_mad\n      <fct>   <dbl> <dbl> <dbl>\n    1 control 19.7   3.74  1.87\n    2 medium  19.2   4.90  4.70\n    3 high     9.83  3.46  3.10\n    ```\n    \n    \n    :::\n    :::\n\n\n  - variance: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dat2 |>\n      group_by(x) |>\n        summarise(across(y, list(med = median, sd = sd, mad = mad)))\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    # A tibble: 3 × 4\n      x       y_med  y_sd y_mad\n      <fct>   <dbl> <dbl> <dbl>\n    1 control 19.7   3.74  1.87\n    2 medium  19.2   4.90  4.70\n    3 high     9.83  3.46  3.10\n    ```\n    \n    \n    :::\n    :::\n\n\n- $\\beta_1$: _t_ distribution (3 degrees of freedom) prior centered at\n  0 with a variance of 9.35\n  - mean: since effects are differences and we may not want to\n    pre-determine the direction (polarity) of any trends, it is\n    typical to define effects priors with means of 0\n  - variance: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dat2 |>\n      group_by(x) |>\n      summarise(across(y, median)) |>\n      pull(y) |>\n      diff() |>\n      abs() |>\n      max()\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 9.352104\n    ```\n    \n    \n    :::\n    :::\n\n\n \n- $\\sigma$: (half) _t_ distribution (3 degrees of freedom) centred at\n  0 with a variance of 4.7\n  - variance: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dat2 |>\n    group_by(x) |>\n    summarise(across(y, mad)) |>\n    pull(y) |>\n    max()\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 4.702147\n    ```\n    \n    \n    :::\n    :::\n\n::: {.cell}\n\n```{.r .cell-code}\npriors <- prior(normal(19.68, 1.87), class = \"Intercept\") +\n    prior(student_t(3, 0, 6.35), class = \"b\") +\n    prior(student_t(3, 0, 4.7), class = \"sigma\")\n```\n:::\n\n\n\n##### Fit the model\n\n$$\n\\begin{align}\ny_i \\sim{}& N(\\mu_i, \\sigma^2)\\\\\n\\mu_i =& \\beta_0 + \\sum{\\beta_j x_{ij}}\\\\\n\\end{align}\n$$\n\n1. start by fitting the model and sampling from the priors only\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2a.brm <- brm(bf(y ~ x),\n                data=dat2,\n                prior=priors,\n                sample_prior = 'only', \n                iter = 5000,\n                warmup = 1000,\n                chains = 3, cores = 3,\n                thin = 5,\n                backend = \"rstan\",\n                refresh = 0) \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n\n\n:::\n\n2. explore the range of posterior predictions resulting from the\n   priors alone\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2a.brm |>\n    conditional_effects() |>\n    plot(points = TRUE) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects_2a-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions:** \n\n- the grey ribbon above represents the credible range of the posterior\n  predictions\n- this ribbon is substantially wider than the observed data (black\n  points), suggesting that the priors are not overly narrow\n- the range of posterior predictions are not widely unreasonable\n  (although we could argue that negative predictions might be\n  illogical) since the maximums are not multiple orders of magnitude\n  above the observed data.\n\n:::\n\n3. now refit the model such that it samples from both the priors and\n   likelihood (that is allow the data to have an impact on the\n   estimates)\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2a.brm2 <- update(dat2a.brm,\n    sample_prior = \"yes\"\n) \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe desired updates require recompiling the model\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n\n\n:::\n\n4. re-explore the range of posterior predictions resulting from the\n   fitted model\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2a.brm2 |>\n    conditional_effects() |>\n    plot(points = TRUE) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects2_2a-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions:** \n\n- the range of the posteriors are now substantially reduced (now that\n  the model includes both data and priors)\n- this suggests that the patterns are being driven predominantly by the data\n\n:::\n\n5. compare the priors and posteriors to further confirm that the\n   priors are not overly influential\n   \n::: {.indented}\n\nWhen we have indicated that the posterior should be informed by both\nthe prior and the posterior, both prior (governed by priors alone) and\nposterior (governed by both priors and data/likelihood) draws are\nreturned.  These can be compared by exploring the probabilities\nassociated with specific hypotheses - the most obvious of which is\nthat of no effect (that the parameter = 0).\n\nWhen doing so, ideally the posterior should be distinct from the\nprior. By distinct, I mean that the posteriors should be clearly\ndistinguishable from the priors. Ideally, the priors should be less\nvariable than the priors.  If this is not the case, it might suggest\nthat the posteriors are being too strongly driven by the priors.\n\n\n\n::: {.cell labels='brms_prior_posterior_2a'}\n\n```{.r .cell-code}\ndat2a.brm2 |> tidybayes::get_variables()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"     \"b_xmedium\"       \"b_xhigh\"         \"sigma\"          \n [5] \"Intercept\"       \"prior_Intercept\" \"prior_b\"         \"prior_sigma\"    \n [9] \"lprior\"          \"lp__\"            \"accept_stat__\"   \"stepsize__\"     \n[13] \"treedepth__\"     \"n_leapfrog__\"    \"divergent__\"     \"energy__\"       \n```\n\n\n:::\n\n```{.r .cell-code}\ndat2a.brm2 |> hypothesis(\"xmedium = 0\") |> plot()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/unnamed-chunk-96-1.png){width=384}\n:::\n\n```{.r .cell-code}\ndat2a.brm2 |> hypothesis(\"sigma = 0\", class = \"\") |> plot() \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/unnamed-chunk-96-2.png){width=384}\n:::\n:::\n\n\n\nUnfortunately, it is not possible to do this comparison sensibly for\nthe intercept.  The reason for this is that the prior for intercept\nwas applied to an intercept that is associated with centred\ncontinuous predictors (predictors are centred behind the scenes).\nSince we did not centre the predictor, the intercept returned is as if\nuncentred.  Hence, the prior and posterior are on different scales.\n \n\n\n::: {.cell mhidden='true'}\n\n```{.r .cell-code}\ndat2a.brm2 |> tidybayes::get_variables()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"     \"b_xmedium\"       \"b_xhigh\"         \"sigma\"          \n [5] \"Intercept\"       \"prior_Intercept\" \"prior_b\"         \"prior_sigma\"    \n [9] \"lprior\"          \"lp__\"            \"accept_stat__\"   \"stepsize__\"     \n[13] \"treedepth__\"     \"n_leapfrog__\"    \"divergent__\"     \"energy__\"       \n```\n\n\n:::\n\n```{.r .cell-code}\ndat2a.brm2 |> SUYR_prior_and_posterior() \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/fitModel2a-1.png){width=768}\n:::\n:::\n\n\n\n**Conclusions:**\n\n- each of the priors are substantially wider than the posteriors\n- the posteriors are definitely distinct from their respective priors\n  and thus we can conclude that the priors and not driving the\n  posteriors (e.g. they are not influencing the outcomes)\n- the priors are simply regularising the parameters such that they are\n  only sampled from plausible regions\n\n:::\n\n:::::\n\n#### Means parameterisation\n\n::::: {.panel-tabset}\n\n##### Define priors\n\nLets try the following priors:\n\n\n\n::: {.cell}\n\n:::\n\n\n\n- $\\beta$: _t_ distribution (3 degrees of freedom) prior centered at\n  18.14 with a variance of 6.26\n  - mean: since each groups mean is being estimated separately, they\n    could either all have different priors, or more commonly, the same\n    priors.\n  - variance: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dat2 |>\n        summarise(across(y, list(med = median, sd = sd, mad = mad)))\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n         y_med     y_sd    y_mad\n    1 18.13762 6.003828 6.255954\n    ```\n    \n    \n    :::\n    :::\n\n\n- $\\sigma$: (half) _t_ distribution (3 degrees of freedom) centred at\n  0 with a variance of 4.7\n  - variance: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dat2 |>\n    group_by(x) |>\n    summarise(across(y, mad)) |>\n    pull(y) |>\n    max()\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 4.702147\n    ```\n    \n    \n    :::\n    :::\n\n::: {.cell}\n\n```{.r .cell-code}\npriors <- prior(student_t(3, 16.18, 6.56), class = \"b\") +\n    prior(student_t(3, 0, 4.7), class = \"sigma\")\n```\n:::\n\n\n\n##### Fit the model\n\n$$\n\\begin{align}\ny_i \\sim{}& N(\\mu_i, \\sigma^2)\\\\\n\\mu_i =& \\beta_0 + \\sum{\\beta_j x_{ij}}\\\\\n\\end{align}\n$$\n\n1. start by fitting the model and sampling from the priors only\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2b.brm <- brm(bf(y ~ -1 + x),\n                data=dat2,\n                prior=priors,\n                sample_prior = 'only', \n                iter = 5000,\n                warmup = 1000,\n                chains = 3, cores = 3,\n                thin = 5,\n                backend = \"rstan\",\n                refresh = 0) \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n\n\n:::\n\n2. explore the range of posterior predictions resulting from the\n   priors alone\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2b.brm |>\n    conditional_effects() |>\n    plot(points = TRUE) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects_2b-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions:** \n\n- the grey ribbon above represents the credible range of the posterior\n  predictions\n- this ribbon is substantially wider than the observed data (black\n  points), suggesting that the priors are not overly narrow\n- the range of posterior predictions are not widely unreasonable\n  (although we could argue that negative predictions might be\n  illogical) since the maximums are not multiple orders of magnitude\n  above the observed data.\n\n:::\n\n3. now refit the model such that it samples from both the priors and\n   likelihood (that is allow the data to have an impact on the\n   estimates)\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2b.brm2 <- update(dat2b.brm,\n    sample_prior = \"yes\"\n) \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe desired updates require recompiling the model\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n\n\n:::\n\n4. re-explore the range of posterior predictions resulting from the\n   fitted model\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2b.brm2 |>\n    conditional_effects() |>\n    plot(points = TRUE) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects2_2b-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions:** \n\n- the range of the posteriors are now substantially reduced (now that\n  the model includes both data and priors)\n- this suggests that the patterns are being driven predominantly by the data\n\n:::\n\n5. compare the priors and posteriors to further confirm that the\n   priors are not overly influential\n   \n::: {.indented}\n\nWhen we have indicated that the posterior should be informed by both\nthe prior and the posterior, both prior (governed by priors alone) and\nposterior (governed by both priors and data/likelihood) draws are\nreturned.  These can be compared by exploring the probabilities\nassociated with specific hypotheses - the most obvious of which is\nthat of no effect (that the parameter = 0).\n\nWhen doing so, ideally the posterior should be distinct from the\nprior. By distinct, I mean that the posteriors should be clearly\ndistinguishable from the priors. Ideally, the priors should be less\nvariable than the priors.  If this is not the case, it might suggest\nthat the posteriors are being too strongly driven by the priors.\n\n\n\n::: {.cell labels='brms_prior_posterior_2b'}\n\n```{.r .cell-code}\ndat2b.brm2 |> tidybayes::get_variables()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_xcontrol\"    \"b_xmedium\"     \"b_xhigh\"       \"sigma\"        \n [5] \"prior_b\"       \"prior_sigma\"   \"lprior\"        \"lp__\"         \n [9] \"accept_stat__\" \"stepsize__\"    \"treedepth__\"   \"n_leapfrog__\" \n[13] \"divergent__\"   \"energy__\"     \n```\n\n\n:::\n\n```{.r .cell-code}\ndat2b.brm2 |> hypothesis(\"xcontrol = 0\") |> plot()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/unnamed-chunk-105-1.png){width=384}\n:::\n\n```{.r .cell-code}\ndat2b.brm2 |> hypothesis(\"xmedium = 0\") |> plot()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/unnamed-chunk-105-2.png){width=384}\n:::\n\n```{.r .cell-code}\ndat2b.brm2 |> hypothesis(\"sigma = 0\", class = \"\") |> plot() \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/unnamed-chunk-105-3.png){width=384}\n:::\n:::\n\n\n\nUnfortunately, it is not possible to do this comparison sensibly for\nthe intercept.  The reason for this is that the prior for intercept\nwas applied to an intercept that is associated with centred\ncontinuous predictors (predictors are centred behind the scenes).\nSince we did not centre the predictor, the intercept returned is as if\nuncentred.  Hence, the prior and posterior are on different scales.\n \n\n\n::: {.cell mhidden='true'}\n\n```{.r .cell-code}\ndat2b.brm2 |> tidybayes::get_variables()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_xcontrol\"    \"b_xmedium\"     \"b_xhigh\"       \"sigma\"        \n [5] \"prior_b\"       \"prior_sigma\"   \"lprior\"        \"lp__\"         \n [9] \"accept_stat__\" \"stepsize__\"    \"treedepth__\"   \"n_leapfrog__\" \n[13] \"divergent__\"   \"energy__\"     \n```\n\n\n:::\n\n```{.r .cell-code}\n#dat2b.brm2 |> SUYR_prior_and_posterior() \n```\n:::\n\n\n\n**Conclusions:**\n\n- each of the priors are substantially wider than the posteriors\n- the posteriors are definitely distinct from their respective priors\n  and thus we can conclude that the priors and not driving the\n  posteriors (e.g. they are not influencing the outcomes)\n- the priors are simply regularising the parameters such that they are\n  only sampled from plausible regions\n\n:::\n\n:::::\n:::::::\n\n### Example 3 (Poisson data)\n\n::::::: {.panel-tabset}\n#### Raw predictor\n\n$$\n\\begin{align}\ny_i \\sim{}& Pois(\\lambda_i)\\\\\nlog(\\lambda_i) =& \\beta_0 + \\beta_1 x_i\\\\\n\\end{align}\n$$\n\n::::: {.panel-tabset}\n\n##### Define priors\n\nWhen considering priors, it is important to remember that they apply\nto parameters on the link scale. For Poisson models, the link scale is\nlog. So the following priors might be appropriate:\n\n- $\\beta_0$: Normal prior centred at 1.99 with a\n  variance of 1.33\n  - mean: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dat3$y |>\n        log() |> \n        median() |>\n        round(2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 1.99\n    ```\n    \n    \n    :::\n    :::\n\n\n  - variance: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dat3$y |>\n        log() |> \n        mad() |>\n        round(2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 1.33\n    ```\n    \n    \n    :::\n    :::\n\n\n- $\\beta_1$: _t_ distribution (3 degrees of freedom) prior centred at\n  0 with a variance of 2.05\n  - mean: since effects are differences and we may not want to\n    pre-determine the direction (polarity) of any trends, it is\n    typical to define effects priors with means of 0\n  - variance: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dat3 |>\n        mutate(across(c(y, x), log)) |>\n        summarise(across(c(y, x), mad)) |>\n        mutate(round(y / x, 2))\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n             y        x round(y/x, 2)\n    1 1.328231 0.648985          2.05\n    ```\n    \n    \n    :::\n    :::\n\n::: {.cell}\n\n```{.r .cell-code}\npriors <- prior(normal(2.00, 1.33), class = \"Intercept\") +\n    prior(student_t(3, 0, 2.00), class = \"b\")\n```\n:::\n\n\n\n##### Fit the model\n\n1. start by fitting the model and sampling from the priors only\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3a.form <- bf(y ~ x, family = poisson(link = \"log\"))\ndat3a.brm <- brm(dat3a.form,\n                data=dat3,\n                prior=priors,\n                sample_prior = 'only', \n                iter = 5000,\n                warmup = 1000,\n                chains = 3, cores = 3,\n                thin = 5,\n                backend = \"rstan\",\n                refresh = 0)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n\n:::\n\n2. explore the range of posterior predictions resulting from the\n   priors alone\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3a.brm |>\n  conditional_effects() |>\n  plot(points = TRUE) |> _[[1]] +\n  scale_y_log10()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects_3a-1.png){width=384}\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects_3a-2.png){width=384}\n:::\n:::\n\n\n\n**Conclusions:** \n\n- the grey ribbon above represents the credible range of the posterior\n  predictions\n- this ribbon is substantially wider than the observed data (black\n  points), suggesting that the priors are not overly narrow\n- the range of posterior predictions are not widely unreasonable\n  (although we could argue that negative predictions might be\n  illogical) since the maximums are not multiple orders of magnitude\n  above the observed data.\n\n\n:::\n\n3. now refit the model such that it samples from both the priors and\n   likelihood (that is allow the data to have an impact on the\n   estimates)\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3a.brm2 <- update(dat3a.brm,\n    sample_prior = \"yes\"\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe desired updates require recompiling the model\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n\n\n:::\n\n4. re-explore the range of posterior predictions resulting from the\n   fitted model\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3a.brm2 |>\n    conditional_effects() |>\n    plot(points = TRUE) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects2_3a-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions:** \n\n- the range of the posteriors are now substantially reduced (now that\n  the model includes both data and priors)\n- this suggests that the patterns are being driven predominantly by the data\n\n:::\n\n5. compare the priors and posteriors to further confirm that the\n   priors are not overly influential\n   \n::: {.indented}\n\nWhen we have indicated that the posterior should be informed by both\nthe prior and the posterior, both prior (governed by priors alone) and\nposterior (governed by both priors and data/likelihood) draws are\nreturned.  These can be compared by exploring the probabilities\nassociated with specific hypotheses - the most obvious of which is\nthat of no effect (that the parameter = 0).\n\nWhen doing so, ideally the posterior should be distinct from the\nprior. By distinct, I mean that the posteriors should be clearly\ndistinguishable from the priors. Ideally, the priors should be less\nvariable than the priors.  If this is not the case, it might suggest\nthat the posteriors are being too strongly driven by the priors.\n\n\n\n::: {.cell labels='brms_prior_posterior_3a'}\n\n```{.r .cell-code}\ndat3a.brm2 |> tidybayes::get_variables() \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"     \"b_x\"             \"Intercept\"       \"prior_Intercept\"\n [5] \"prior_b\"         \"lprior\"          \"lp__\"            \"accept_stat__\"  \n [9] \"stepsize__\"      \"treedepth__\"     \"n_leapfrog__\"    \"divergent__\"    \n[13] \"energy__\"       \n```\n\n\n:::\n\n```{.r .cell-code}\ndat3a.brm2 |> hypothesis(\"x = 0\") |> plot()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/unnamed-chunk-114-1.png){width=384}\n:::\n:::\n\n\n\nUnfortunately, it is not possible to do this comparison sensibly for\nthe intercept.  The reason for this is that the prior for intercept\nwas applied to an intercept that is associated with centred\ncontinuous predictors (predictors are centred behind the scenes).\nSince we did not centre the predictor, the intercept returned is as if\nuncentred.  Hence, the prior and posterior are on different scales.\n \n\n\n::: {.cell mhidden='true'}\n\n```{.r .cell-code}\ndat3a.brm2 |> tidybayes::get_variables() \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"     \"b_x\"             \"Intercept\"       \"prior_Intercept\"\n [5] \"prior_b\"         \"lprior\"          \"lp__\"            \"accept_stat__\"  \n [9] \"stepsize__\"      \"treedepth__\"     \"n_leapfrog__\"    \"divergent__\"    \n[13] \"energy__\"       \n```\n\n\n:::\n\n```{.r .cell-code}\ndat3a.brm2 |> SUYR_prior_and_posterior()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/fitModel3a-1.png){width=768}\n:::\n:::\n\n\n\n**Conclusions:**\n\n- each of the priors are substantially wider than the posteriors\n- the posteriors are definitely distinct from their respective priors\n  and thus we can conclude that the priors and not driving the\n  posteriors (e.g. they are not influencing the outcomes)\n- the priors are simply regularising the parameters such that they are\n  only sampled from plausible regions\n\n:::\n\n:::::\n\n\n#### Centered predictor\n\n$$\n\\begin{align}\ny_i \\sim{}& Pois(\\mu_i)\\\\\nlog(\\mu_i) =& \\beta_0 + (\\beta_1 x_i - \\bar{x})\\\\\n\\end{align}\n$$\n\n::::: {.panel-tabset}\n\n##### Define priors\n\nWhen considering priors, it is important to remember that they apply\nto parameters on the link scale. For Poisson models, the link scale is\nlog. So the following priors might be appropriate:\n\n- $\\beta_0$: Normal prior centred at 1.99 with a\n  variance of 1.33\n  - mean: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dat3$y |>\n        log() |> \n        median() |>\n        round(2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 1.99\n    ```\n    \n    \n    :::\n    :::\n\n\n  - variance: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dat3$y |>\n        log() |> \n        mad() |>\n        round(2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 1.33\n    ```\n    \n    \n    :::\n    :::\n\n\n- $\\beta_1$: _t_ distribution (3 degrees of freedom) prior centred at\n  0 with a variance of 2.05\n  - mean: since effects are differences and we may not want to\n    pre-determine the direction (polarity) of any trends, it is\n    typical to define effects priors with means of 0\n  - variance: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dat3 |>\n        mutate(across(c(y, x), log)) |>\n        summarise(across(c(y, x), mad)) |>\n        mutate(round(y / x, 2))\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n             y        x round(y/x, 2)\n    1 1.328231 0.648985          2.05\n    ```\n    \n    \n    :::\n    :::\n\n::: {.cell}\n\n```{.r .cell-code}\npriors <- prior(normal(2.00, 1.33), class = \"Intercept\") +\n    prior(student_t(3, 0, 2.00), class = \"b\")\n```\n:::\n\n\n\n##### Fit the model\n\n1. start by fitting the model and sampling from the priors only\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3b.form <- bf(y ~ scale(x, scale = FALSE), family = poisson(link = \"log\"))\ndat3b.brm <- brm(dat3b.form,\n                data=dat3,\n                prior=priors,\n                sample_prior = 'only', \n                iter = 5000,\n                warmup = 1000,\n                chains = 3, cores = 3,\n                thin = 5,\n                backend = \"rstan\",\n                refresh = 0)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n\n:::\n\n2. explore the range of posterior predictions resulting from the\n   priors alone\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3b.brm |>\n  conditional_effects() |>\n  plot(points = TRUE) |> _[[1]] +\n  scale_y_log10()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects_3b-1.png){width=384}\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects_3b-2.png){width=384}\n:::\n:::\n\n\n\n**Conclusions:** \n\n- the grey ribbon above represents the credible range of the posterior\n  predictions\n- this ribbon is substantially wider than the observed data (black\n  points), suggesting that the priors are not overly narrow\n- the range of posterior predictions are not widely unreasonable\n  (although we could argue that negative predictions might be\n  illogical) since the maximums are not multiple orders of magnitude\n  above the observed data.\n\n\n:::\n\n3. now refit the model such that it samples from both the priors and\n   likelihood (that is allow the data to have an impact on the\n   estimates)\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3b.brm2 <- update(dat3b.brm,\n    sample_prior = \"yes\"\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe desired updates require recompiling the model\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n\n\n:::\n\n4. re-explore the range of posterior predictions resulting from the\n   fitted model\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3b.brm2 |>\n    conditional_effects() |>\n    plot(points = TRUE) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects2_3b-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions:** \n\n- the range of the posteriors are now substantially reduced (now that\n  the model includes both data and priors)\n- this suggests that the patterns are being driven predominantly by the data\n\n:::\n\n5. compare the priors and posteriors to further confirm that the\n   priors are not overly influential\n   \n::: {.indented}\n\nWhen we have indicated that the posterior should be informed by both\nthe prior and the posterior, both prior (governed by priors alone) and\nposterior (governed by both priors and data/likelihood) draws are\nreturned.  These can be compared by exploring the probabilities\nassociated with specific hypotheses - the most obvious of which is\nthat of no effect (that the parameter = 0).\n\nWhen doing so, ideally the posterior should be distinct from the\nprior. By distinct, I mean that the posteriors should be clearly\ndistinguishable from the priors. Ideally, the priors should be less\nvariable than the priors.  If this is not the case, it might suggest\nthat the posteriors are being too strongly driven by the priors.\n\n\n\n::: {.cell labels='brms_prior_posterior_3b'}\n\n```{.r .cell-code}\ndat3b.brm2 |> tidybayes::get_variables() \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"          \"b_scalexscaleEQFALSE\" \"Intercept\"           \n [4] \"prior_Intercept\"      \"prior_b\"              \"lprior\"              \n [7] \"lp__\"                 \"accept_stat__\"        \"stepsize__\"          \n[10] \"treedepth__\"          \"n_leapfrog__\"         \"divergent__\"         \n[13] \"energy__\"            \n```\n\n\n:::\n\n```{.r .cell-code}\ndat3b.brm2 |> hypothesis(\"scalexscaleEQFALSE = 0\") |> plot()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/unnamed-chunk-123-1.png){width=384}\n:::\n:::\n\n\n\nUnfortunately, it is not possible to do this comparison sensibly for\nthe intercept.  The reason for this is that the prior for intercept\nwas applied to an intercept that is associated with centred\ncontinuous predictors (predictors are centred behind the scenes).\nSince we did not centre the predictor, the intercept returned is as if\nuncentred.  Hence, the prior and posterior are on different scales.\n \n\n\n::: {.cell mhidden='true'}\n\n```{.r .cell-code}\ndat3b.brm2 |> tidybayes::get_variables() \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"          \"b_scalexscaleEQFALSE\" \"Intercept\"           \n [4] \"prior_Intercept\"      \"prior_b\"              \"lprior\"              \n [7] \"lp__\"                 \"accept_stat__\"        \"stepsize__\"          \n[10] \"treedepth__\"          \"n_leapfrog__\"         \"divergent__\"         \n[13] \"energy__\"            \n```\n\n\n:::\n\n```{.r .cell-code}\ndat3b.brm2 |> SUYR_prior_and_posterior()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/fitModel3b-1.png){width=768}\n:::\n:::\n\n\n\n**Conclusions:**\n\n- each of the priors are substantially wider than the posteriors\n- the posteriors are definitely distinct from their respective priors\n  and thus we can conclude that the priors and not driving the\n  posteriors (e.g. they are not influencing the outcomes)\n- the priors are simply regularising the parameters such that they are\n  only sampled from plausible regions\n\n:::\n\n:::::\n\n\n#### Standardised predictor\n\n$$\n\\begin{align}\ny_i \\sim{}& Pois(\\lambda_i)\\\\\nlog(\\lambda_i) =& \\beta_0 + (\\beta_1 x_i - \\bar{x})/\\sigma_x\\\\\n\\end{align}\n$$\n\n::::: {.panel-tabset}\n\n##### Define priors\n\nWhen considering priors, it is important to remember that they apply\nto parameters on the link scale. For Poisson models, the link scale is\nlog. So the following priors might be appropriate:\n\n- $\\beta_0$: Normal prior centred at 1.99 with a\n  variance of 1.33\n  - mean: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dat3$y |>\n        log() |> \n        median() |>\n        round(2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 1.99\n    ```\n    \n    \n    :::\n    :::\n\n\n  - variance: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dat3$y |>\n        log() |> \n        mad() |>\n        round(2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 1.33\n    ```\n    \n    \n    :::\n    :::\n\n\n- $\\beta_1$: _t_ distribution (3 degrees of freedom) prior centred at\n  0 with a variance of 1.33\n  - mean: since effects are differences and we may not want to\n    pre-determine the direction (polarity) of any trends, it is\n    typical to define effects priors with means of 0\n  - variance: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dat3$y |> log() |> mad() |> round(2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 1.33\n    ```\n    \n    \n    :::\n    :::\n\n::: {.cell}\n\n```{.r .cell-code}\npriors <- prior(normal(2.00, 1.33), class = \"Intercept\") +\n    prior(student_t(3, 0, 1.33), class = \"b\")\n```\n:::\n\n\n\n##### Fit the model\n\n1. start by fitting the model and sampling from the priors only\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3c.form <- bf(y ~ scale(x), family = poisson(link = \"log\"))\ndat3c.brm <- brm(dat3c.form,\n                data=dat3,\n                prior=priors,\n                sample_prior = 'only', \n                iter = 5000,\n                warmup = 1000,\n                chains = 3, cores = 3,\n                thin = 5,\n                backend = \"rstan\",\n                refresh = 0)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n\n:::\n\n2. explore the range of posterior predictions resulting from the\n   priors alone\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3c.brm |>\n  conditional_effects() |>\n  plot(points = TRUE) |> _[[1]] +\n  scale_y_log10()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects_3c-1.png){width=384}\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects_3c-2.png){width=384}\n:::\n:::\n\n\n\n**Conclusions:** \n\n- the grey ribbon above represents the credible range of the posterior\n  predictions\n- this ribbon is substantially wider than the observed data (black\n  points), suggesting that the priors are not overly narrow\n- the range of posterior predictions are not widely unreasonable\n  (although we could argue that negative predictions might be\n  illogical) since the maximums are not multiple orders of magnitude\n  above the observed data.\n\n\n:::\n\n3. now refit the model such that it samples from both the priors and\n   likelihood (that is allow the data to have an impact on the\n   estimates)\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3c.brm2 <- update(dat3c.brm,\n    sample_prior = \"yes\"\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe desired updates require recompiling the model\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n\n\n:::\n\n4. re-explore the range of posterior predictions resulting from the\n   fitted model\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3c.brm2 |>\n    conditional_effects() |>\n    plot(points = TRUE) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects2_3c-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions:** \n\n- the range of the posteriors are now substantially reduced (now that\n  the model includes both data and priors)\n- this suggests that the patterns are being driven predominantly by the data\n\n:::\n\n5. compare the priors and posteriors to further confirm that the\n   priors are not overly influential\n   \n::: {.indented}\n\nWhen we have indicated that the posterior should be informed by both\nthe prior and the posterior, both prior (governed by priors alone) and\nposterior (governed by both priors and data/likelihood) draws are\nreturned.  These can be compared by exploring the probabilities\nassociated with specific hypotheses - the most obvious of which is\nthat of no effect (that the parameter = 0).\n\nWhen doing so, ideally the posterior should be distinct from the\nprior. By distinct, I mean that the posteriors should be clearly\ndistinguishable from the priors. Ideally, the priors should be less\nvariable than the priors.  If this is not the case, it might suggest\nthat the posteriors are being too strongly driven by the priors.\n\n\n\n::: {.cell labels='brms_prior_posterior_3c'}\n\n```{.r .cell-code}\ndat3c.brm2 |> tidybayes::get_variables() \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"     \"b_scalex\"        \"Intercept\"       \"prior_Intercept\"\n [5] \"prior_b\"         \"lprior\"          \"lp__\"            \"accept_stat__\"  \n [9] \"stepsize__\"      \"treedepth__\"     \"n_leapfrog__\"    \"divergent__\"    \n[13] \"energy__\"       \n```\n\n\n:::\n\n```{.r .cell-code}\ndat3c.brm2 |> hypothesis(\"scalex = 0\") |> plot()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/unnamed-chunk-132-1.png){width=384}\n:::\n:::\n\n\n\nUnfortunately, it is not possible to do this comparison sensibly for\nthe intercept.  The reason for this is that the prior for intercept\nwas applied to an intercept that is associated with centred\ncontinuous predictors (predictors are centred behind the scenes).\nSince we did not centre the predictor, the intercept returned is as if\nuncentred.  Hence, the prior and posterior are on different scales.\n \n\n\n::: {.cell mhidden='true'}\n\n```{.r .cell-code}\ndat3c.brm2 |> tidybayes::get_variables() \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"     \"b_scalex\"        \"Intercept\"       \"prior_Intercept\"\n [5] \"prior_b\"         \"lprior\"          \"lp__\"            \"accept_stat__\"  \n [9] \"stepsize__\"      \"treedepth__\"     \"n_leapfrog__\"    \"divergent__\"    \n[13] \"energy__\"       \n```\n\n\n:::\n\n```{.r .cell-code}\ndat3c.brm2 |> SUYR_prior_and_posterior()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/fitModel3c-1.png){width=768}\n:::\n:::\n\n\n\n**Conclusions:**\n\n- each of the priors are substantially wider than the posteriors\n- the posteriors are definitely distinct from their respective priors\n  and thus we can conclude that the priors and not driving the\n  posteriors (e.g. they are not influencing the outcomes)\n- the priors are simply regularising the parameters such that they are\n  only sampled from plausible regions\n\n:::\n\n:::::\n\n:::::::\n\n### Example 4 (NB)\n\n::::::: {.panel-tabset}\n#### Raw predictor\n\n$$\n\\begin{align}\ny_i \\sim{}& NB(\\lambda_i, \\phi)\\\\\nlog(\\lambda_i) =& \\beta_0 + \\beta_1 x_i\\\\\n\\end{align}\n$$\n\n::::: {.panel-tabset}\n\n##### Define priors\n\nWhen considering priors, it is important to remember that they apply\nto parameters on the link scale. For Negative Binomial models, the link scale is\nlog. So the following priors might be appropriate:\n\n- $\\beta_0$: Normal prior centred at 2.07 with a\n  variance of 0.93\n  - mean: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dat4$y |>\n        log() |> \n        median() |>\n        round(2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 2.07\n    ```\n    \n    \n    :::\n    :::\n\n\n  - variance: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dat4$y |>\n        log() |> \n        mad() |>\n        round(2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 0.93\n    ```\n    \n    \n    :::\n    :::\n\n\n- $\\beta_1$: _t_ distribution (3 degrees of freedom) prior centred at\n  0 with a variance of 1.43\n  - mean: since effects are differences and we may not want to\n    pre-determine the direction (polarity) of any trends, it is\n    typical to define effects priors with means of 0\n  - variance: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dat4 |>\n        mutate(across(c(y, x), log)) |>\n        summarise(across(c(y, x), mad)) |>\n        mutate(round(y / x, 2))\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n              y        x round(y/x, 2)\n    1 0.9303522 0.648985          1.43\n    ```\n    \n    \n    :::\n    :::\n\n::: {.cell}\n\n```{.r .cell-code}\npriors <- prior(normal(2.00, 1.00), class = \"Intercept\") +\n    prior(student_t(3, 0, 1.5), class = \"b\") +\n    prior(gamma(0.01, 0.01), class = \"shape\")\n```\n:::\n\n\n\n##### Fit the model\n\n1. start by fitting the model and sampling from the priors only\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4a.form <- bf(y ~ x, family = negbinomial(link = \"log\"))\ndat4a.brm <- brm(dat4a.form,\n                data=dat4,\n                prior=priors,\n                sample_prior = 'only', \n                iter = 5000,\n                warmup = 1000,\n                chains = 3, cores = 3,\n                thin = 5,\n                backend = \"rstan\",\n                refresh = 0) \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: There were 1098 divergent transitions after warmup. See\nhttps://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\nto find out why this is a problem and how to eliminate them.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Examine the pairs() plot to diagnose sampling problems\n```\n\n\n:::\n:::\n\n\n:::\n\n2. explore the range of posterior predictions resulting from the\n   priors alone\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4a.brm |>\n  conditional_effects() |>\n  plot(points = TRUE) |> _[[1]] +\n  scale_y_log10()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects_4a-1.png){width=384}\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in scale_y_log10(): log-10 transformation introduced infinite values.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects_4a-2.png){width=384}\n:::\n:::\n\n\n\n**Conclusions:** \n\n- the grey ribbon above represents the credible range of the posterior\n  predictions\n- this ribbon is substantially wider than the observed data (black\n  points), suggesting that the priors are not overly narrow\n- the range of posterior predictions are not widely unreasonable\n  (although we could argue that negative predictions might be\n  illogical) since the maximums are not multiple orders of magnitude\n  above the observed data.\n\n\n:::\n\n3. now refit the model such that it samples from both the priors and\n   likelihood (that is allow the data to have an impact on the\n   estimates)\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4a.brm2 <- update(dat4a.brm,\n    sample_prior = \"yes\"\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe desired updates require recompiling the model\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n\n\n:::\n\n4. re-explore the range of posterior predictions resulting from the\n   fitted model\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4a.brm2 |>\n    conditional_effects() |>\n    plot(points = TRUE) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects2_4a-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions:** \n\n- the range of the posteriors are now substantially reduced (now that\n  the model includes both data and priors)\n- this suggests that the patterns are being driven predominantly by the data\n\n:::\n\n5. compare the priors and posteriors to further confirm that the\n   priors are not overly influential\n   \n::: {.indented}\n\nWhen we have indicated that the posterior should be informed by both\nthe prior and the posterior, both prior (governed by priors alone) and\nposterior (governed by both priors and data/likelihood) draws are\nreturned.  These can be compared by exploring the probabilities\nassociated with specific hypotheses - the most obvious of which is\nthat of no effect (that the parameter = 0).\n\nWhen doing so, ideally the posterior should be distinct from the\nprior. By distinct, I mean that the posteriors should be clearly\ndistinguishable from the priors. Ideally, the priors should be less\nvariable than the priors.  If this is not the case, it might suggest\nthat the posteriors are being too strongly driven by the priors.\n\n\n\n::: {.cell labels='brms_prior_posterior_4a'}\n\n```{.r .cell-code}\ndat4a.brm2 |> tidybayes::get_variables() \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"     \"b_x\"             \"shape\"           \"Intercept\"      \n [5] \"prior_Intercept\" \"prior_b\"         \"prior_shape\"     \"lprior\"         \n [9] \"lp__\"            \"accept_stat__\"   \"stepsize__\"      \"treedepth__\"    \n[13] \"n_leapfrog__\"    \"divergent__\"     \"energy__\"       \n```\n\n\n:::\n\n```{.r .cell-code}\ndat4a.brm2 |> hypothesis(\"x = 0\") |> plot()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/unnamed-chunk-141-1.png){width=384}\n:::\n:::\n\n\n\nUnfortunately, it is not possible to do this comparison sensibly for\nthe intercept.  The reason for this is that the prior for intercept\nwas applied to an intercept that is associated with centred\ncontinuous predictors (predictors are centred behind the scenes).\nSince we did not centre the predictor, the intercept returned is as if\nuncentred.  Hence, the prior and posterior are on different scales.\n \n\n\n::: {.cell mhidden='true'}\n\n```{.r .cell-code}\ndat4a.brm2 |> tidybayes::get_variables() \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"     \"b_x\"             \"shape\"           \"Intercept\"      \n [5] \"prior_Intercept\" \"prior_b\"         \"prior_shape\"     \"lprior\"         \n [9] \"lp__\"            \"accept_stat__\"   \"stepsize__\"      \"treedepth__\"    \n[13] \"n_leapfrog__\"    \"divergent__\"     \"energy__\"       \n```\n\n\n:::\n\n```{.r .cell-code}\ndat4a.brm2 |> SUYR_prior_and_posterior()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/fitModel4a-1.png){width=768}\n:::\n:::\n\n\n\n**Conclusions:**\n\n- each of the priors are substantially wider than the posteriors\n- the posteriors are definitely distinct from their respective priors\n  and thus we can conclude that the priors and not driving the\n  posteriors (e.g. they are not influencing the outcomes)\n- the priors are simply regularising the parameters such that they are\n  only sampled from plausible regions\n\n:::\n\n:::::\n\n\n#### Centered predictor\n\n$$\n\\begin{align}\ny_i \\sim{}& NB(\\mu_i, \\phi)\\\\\nlog(\\mu_i) =& \\beta_0 + (\\beta_1 x_i - \\bar{x})\\\\\n\\end{align}\n$$\n\n::::: {.panel-tabset}\n\n##### Define priors\n\nWhen considering priors, it is important to remember that they apply\nto parameters on the link scale. For Poisson models, the link scale is\nlog. So the following priors might be appropriate:\n\n- $\\beta_0$: Normal prior centred at 2.07 with a\n  variance of 0.93\n  - mean: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dat4$y |>\n        log() |> \n        median() |>\n        round(2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 2.07\n    ```\n    \n    \n    :::\n    :::\n\n\n  - variance: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dat4$y |>\n        log() |> \n        mad() |>\n        round(2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 0.93\n    ```\n    \n    \n    :::\n    :::\n\n\n- $\\beta_1$: _t_ distribution (3 degrees of freedom) prior centred at\n  0 with a variance of 1.43\n  - mean: since effects are differences and we may not want to\n    pre-determine the direction (polarity) of any trends, it is\n    typical to define effects priors with means of 0\n  - variance: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dat4 |>\n        mutate(across(c(y, x), log)) |>\n        summarise(across(c(y, x), mad)) |>\n        mutate(round(y / x, 2))\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n              y        x round(y/x, 2)\n    1 0.9303522 0.648985          1.43\n    ```\n    \n    \n    :::\n    :::\n\n::: {.cell}\n\n```{.r .cell-code}\npriors <- prior(normal(2.07, 0.93), class = \"Intercept\") +\n    prior(student_t(3, 0, 1.43), class = \"b\") +\n    prior(gamma(0.01, 0.01), class = \"shape\")\n```\n:::\n\n\n\n##### Fit the model\n\n1. start by fitting the model and sampling from the priors only\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4b.form <- bf(y ~ scale(x, scale = FALSE), family = negbinomial(link = \"log\"))\ndat4b.brm <- brm(dat4b.form,\n                data=dat4,\n                prior=priors,\n                sample_prior = 'only', \n                iter = 5000,\n                warmup = 1000,\n                chains = 3, cores = 3,\n                thin = 5,\n                backend = \"rstan\",\n                refresh = 0)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: There were 1636 divergent transitions after warmup. See\nhttps://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\nto find out why this is a problem and how to eliminate them.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Examine the pairs() plot to diagnose sampling problems\n```\n\n\n:::\n:::\n\n\n:::\n\n2. explore the range of posterior predictions resulting from the\n   priors alone\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4b.brm |>\n  conditional_effects() |>\n  plot(points = TRUE) |> _[[1]] +\n  scale_y_log10()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects_4b-1.png){width=384}\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in scale_y_log10(): log-10 transformation introduced infinite values.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects_4b-2.png){width=384}\n:::\n:::\n\n\n\n**Conclusions:** \n\n- the grey ribbon above represents the credible range of the posterior\n  predictions\n- this ribbon is substantially wider than the observed data (black\n  points), suggesting that the priors are not overly narrow\n- the range of posterior predictions are not widely unreasonable\n  (although we could argue that negative predictions might be\n  illogical) since the maximums are not multiple orders of magnitude\n  above the observed data.\n\n\n:::\n\n3. now refit the model such that it samples from both the priors and\n   likelihood (that is allow the data to have an impact on the\n   estimates)\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4b.brm2 <- update(dat4b.brm,\n    sample_prior = \"yes\"\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe desired updates require recompiling the model\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n\n\n:::\n\n4. re-explore the range of posterior predictions resulting from the\n   fitted model\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4b.brm2 |>\n    conditional_effects() |>\n    plot(points = TRUE) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects2_4b-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions:** \n\n- the range of the posteriors are now substantially reduced (now that\n  the model includes both data and priors)\n- this suggests that the patterns are being driven predominantly by the data\n\n:::\n\n5. compare the priors and posteriors to further confirm that the\n   priors are not overly influential\n   \n::: {.indented}\n\nWhen we have indicated that the posterior should be informed by both\nthe prior and the posterior, both prior (governed by priors alone) and\nposterior (governed by both priors and data/likelihood) draws are\nreturned.  These can be compared by exploring the probabilities\nassociated with specific hypotheses - the most obvious of which is\nthat of no effect (that the parameter = 0).\n\nWhen doing so, ideally the posterior should be distinct from the\nprior. By distinct, I mean that the posteriors should be clearly\ndistinguishable from the priors. Ideally, the priors should be less\nvariable than the priors.  If this is not the case, it might suggest\nthat the posteriors are being too strongly driven by the priors.\n\n\n\n::: {.cell labels='brms_prior_posterior_4b'}\n\n```{.r .cell-code}\ndat4b.brm2 |> tidybayes::get_variables() \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"          \"b_scalexscaleEQFALSE\" \"shape\"               \n [4] \"Intercept\"            \"prior_Intercept\"      \"prior_b\"             \n [7] \"prior_shape\"          \"lprior\"               \"lp__\"                \n[10] \"accept_stat__\"        \"stepsize__\"           \"treedepth__\"         \n[13] \"n_leapfrog__\"         \"divergent__\"          \"energy__\"            \n```\n\n\n:::\n\n```{.r .cell-code}\ndat4b.brm2 |> hypothesis(\"scalexscaleEQFALSE = 0\") |> plot()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/unnamed-chunk-150-1.png){width=384}\n:::\n:::\n\n\n\nUnfortunately, it is not possible to do this comparison sensibly for\nthe intercept.  The reason for this is that the prior for intercept\nwas applied to an intercept that is associated with centred\ncontinuous predictors (predictors are centred behind the scenes).\nSince we did not centre the predictor, the intercept returned is as if\nuncentred.  Hence, the prior and posterior are on different scales.\n \n\n\n::: {.cell mhidden='true'}\n\n```{.r .cell-code}\ndat4b.brm2 |> tidybayes::get_variables() \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"          \"b_scalexscaleEQFALSE\" \"shape\"               \n [4] \"Intercept\"            \"prior_Intercept\"      \"prior_b\"             \n [7] \"prior_shape\"          \"lprior\"               \"lp__\"                \n[10] \"accept_stat__\"        \"stepsize__\"           \"treedepth__\"         \n[13] \"n_leapfrog__\"         \"divergent__\"          \"energy__\"            \n```\n\n\n:::\n\n```{.r .cell-code}\ndat4b.brm2 |> SUYR_prior_and_posterior()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/fitModel4b-1.png){width=768}\n:::\n:::\n\n\n\n**Conclusions:**\n\n- each of the priors are substantially wider than the posteriors\n- the posteriors are definitely distinct from their respective priors\n  and thus we can conclude that the priors and not driving the\n  posteriors (e.g. they are not influencing the outcomes)\n- the priors are simply regularising the parameters such that they are\n  only sampled from plausible regions\n\n:::\n\n:::::\n\n\n#### Standardised predictor\n\n$$\n\\begin{align}\ny_i \\sim{}& NB(\\lambda_i, \\phi)\\\\\nlog(\\lambda_i) =& \\beta_0 + (\\beta_1 x_i - \\bar{x})/\\sigma_x\\\\\n\\end{align}\n$$\n\n::::: {.panel-tabset}\n\n##### Define priors\n\nWhen considering priors, it is important to remember that they apply\nto parameters on the link scale. For Poisson models, the link scale is\nlog. So the following priors might be appropriate:\n\n- $\\beta_0$: Normal prior centred at 2.07 with a\n  variance of 0.93\n  - mean: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dat4$y |>\n        log() |> \n        median() |>\n        round(2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 2.07\n    ```\n    \n    \n    :::\n    :::\n\n\n  - variance: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dat4$y |>\n        log() |> \n        mad() |>\n        round(2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 0.93\n    ```\n    \n    \n    :::\n    :::\n\n\n- $\\beta_1$: _t_ distribution (3 degrees of freedom) prior centred at\n  0 with a variance of 0.93\n  - mean: since effects are differences and we may not want to\n    pre-determine the direction (polarity) of any trends, it is\n    typical to define effects priors with means of 0\n  - variance: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dat4$y |> log() |> mad() |> round(2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 0.93\n    ```\n    \n    \n    :::\n    :::\n\n::: {.cell}\n\n```{.r .cell-code}\npriors <- prior(normal(2.07,0.93), class = \"Intercept\") +\n    prior(student_t(3, 0, 1.43), class = \"b\") +\n    prior(gamma(0.01, 0.01), class = \"shape\")\n```\n:::\n\n\n\n##### Fit the model\n\n1. start by fitting the model and sampling from the priors only\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4c.form <- bf(y ~ scale(x), family = negbinomial(link = \"log\"))\ndat4c.brm <- brm(dat4c.form,\n                data=dat4,\n                prior=priors,\n                sample_prior = 'only', \n                iter = 5000,\n                warmup = 1000,\n                chains = 3, cores = 3,\n                thin = 5,\n                backend = \"rstan\",\n                refresh = 0)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: There were 1348 divergent transitions after warmup. See\nhttps://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\nto find out why this is a problem and how to eliminate them.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Examine the pairs() plot to diagnose sampling problems\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#tail-ess\n```\n\n\n:::\n:::\n\n\n:::\n\n2. explore the range of posterior predictions resulting from the\n   priors alone\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4c.brm |>\n  conditional_effects() |>\n  plot(points = TRUE) |> _[[1]] +\n  scale_y_log10()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects_4c-1.png){width=384}\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in scale_y_log10(): log-10 transformation introduced infinite values.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects_4c-2.png){width=384}\n:::\n:::\n\n\n\n**Conclusions:** \n\n- the grey ribbon above represents the credible range of the posterior\n  predictions\n- this ribbon is substantially wider than the observed data (black\n  points), suggesting that the priors are not overly narrow\n- the range of posterior predictions are not widely unreasonable\n  (although we could argue that negative predictions might be\n  illogical) since the maximums are not multiple orders of magnitude\n  above the observed data.\n\n\n:::\n\n3. now refit the model such that it samples from both the priors and\n   likelihood (that is allow the data to have an impact on the\n   estimates)\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4c.brm2 <- update(dat4c.brm,\n    sample_prior = \"yes\"\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe desired updates require recompiling the model\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n\n\n:::\n\n4. re-explore the range of posterior predictions resulting from the\n   fitted model\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4c.brm2 |>\n    conditional_effects() |>\n    plot(points = TRUE) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects2_4c-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions:** \n\n- the range of the posteriors are now substantially reduced (now that\n  the model includes both data and priors)\n- this suggests that the patterns are being driven predominantly by the data\n\n:::\n\n5. compare the priors and posteriors to further confirm that the\n   priors are not overly influential\n   \n::: {.indented}\n\nWhen we have indicated that the posterior should be informed by both\nthe prior and the posterior, both prior (governed by priors alone) and\nposterior (governed by both priors and data/likelihood) draws are\nreturned.  These can be compared by exploring the probabilities\nassociated with specific hypotheses - the most obvious of which is\nthat of no effect (that the parameter = 0).\n\nWhen doing so, ideally the posterior should be distinct from the\nprior. By distinct, I mean that the posteriors should be clearly\ndistinguishable from the priors. Ideally, the priors should be less\nvariable than the priors.  If this is not the case, it might suggest\nthat the posteriors are being too strongly driven by the priors.\n\n\n\n::: {.cell labels='brms_prior_posterior_4c'}\n\n```{.r .cell-code}\ndat4c.brm2 |> tidybayes::get_variables() \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"     \"b_scalex\"        \"shape\"           \"Intercept\"      \n [5] \"prior_Intercept\" \"prior_b\"         \"prior_shape\"     \"lprior\"         \n [9] \"lp__\"            \"accept_stat__\"   \"stepsize__\"      \"treedepth__\"    \n[13] \"n_leapfrog__\"    \"divergent__\"     \"energy__\"       \n```\n\n\n:::\n\n```{.r .cell-code}\ndat3c.brm2 |> hypothesis(\"scalex = 0\") |> plot()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/unnamed-chunk-159-1.png){width=384}\n:::\n:::\n\n\n\nUnfortunately, it is not possible to do this comparison sensibly for\nthe intercept.  The reason for this is that the prior for intercept\nwas applied to an intercept that is associated with centred\ncontinuous predictors (predictors are centred behind the scenes).\nSince we did not centre the predictor, the intercept returned is as if\nuncentred.  Hence, the prior and posterior are on different scales.\n \n\n\n::: {.cell mhidden='true'}\n\n```{.r .cell-code}\ndat4c.brm2 |> tidybayes::get_variables() \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"     \"b_scalex\"        \"shape\"           \"Intercept\"      \n [5] \"prior_Intercept\" \"prior_b\"         \"prior_shape\"     \"lprior\"         \n [9] \"lp__\"            \"accept_stat__\"   \"stepsize__\"      \"treedepth__\"    \n[13] \"n_leapfrog__\"    \"divergent__\"     \"energy__\"       \n```\n\n\n:::\n\n```{.r .cell-code}\ndat4c.brm2 |> SUYR_prior_and_posterior()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/fitModel4c-1.png){width=768}\n:::\n:::\n\n\n\n**Conclusions:**\n\n- each of the priors are substantially wider than the posteriors\n- the posteriors are definitely distinct from their respective priors\n  and thus we can conclude that the priors and not driving the\n  posteriors (e.g. they are not influencing the outcomes)\n- the priors are simply regularising the parameters such that they are\n  only sampled from plausible regions\n\n:::\n\n:::::\n\n:::::::\n\n### Example 5 (Binary data)\n\n::::::: {.panel-tabset}\n#### Raw predictor\n\n$$\n\\begin{align}\ny_i \\sim{}& Bin(\\pi_i, 1)\\\\\nlog(\\frac{\\pi_i}{1-\\pi_i}) =& \\beta_0 + \\beta_1 x_i\\\\\n\\end{align}\n$$\n\n::::: {.panel-tabset}\n\n##### Define priors\n\nWhen considering priors, it is important to remember that they apply\nto parameters on the link scale. For Binary models, the link scale is\nlogit. Binomial data is notoriously difficult to define priors for.  Nevertheless the following considerations are useful:\n\n- the observed response values are only ever either 0 or 1\n- a linear model is exploring whether the probability of a 1 changes from high to low or low to high according to the linear predictor\n- the switch in probability is likely to be somewhere near the middle of the $x$ range\n- with a centered predictor, the mean response is expected to be approximately 0.5\n- on a logit (log odds) scale, this corresponds to a value of 0.\n- on a logit (log odds) scale, values of -3 and 3 are considered very wide\n- on a logit scale, values between -1 and 1 are reasonable.\n\nSo the following priors might be appropriate:\n\n- $\\beta_0$: Normal prior centred at 0 with a\nvariance of 1\n- $\\beta_1$: _t_ distribution (3 degrees of freedom) prior centred at\n  0 with a variance of 1\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npriors <- prior(normal(0, 1), class = \"Intercept\") +\n    prior(student_t(3, 0, 1), class = \"b\")\n```\n:::\n\n\n\n##### Fit the model\n\n1. start by fitting the model and sampling from the priors only\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5a.form <- bf(y | trials(1) ~ x, family = binomial(link = \"logit\"))\ndat5a.brm <- brm(dat5a.form,\n                data=dat5,\n                prior=priors,\n                sample_prior = 'only', \n                iter = 5000,\n                warmup = 1000,\n                chains = 3, cores = 3,\n                thin = 5,\n                backend = \"rstan\",\n                refresh = 0)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nOnly 2 levels detected so that family 'bernoulli' might be a more efficient choice.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n\n:::\n\n2. explore the range of posterior predictions resulting from the\n   priors alone\n\n::: {.indented}\n\nFor Binary data, it is often more useful to explore the predictions on\nthe link scale. Ribbons that extend much beyond -3 and 3 would\ndefinitely be considered wide enough.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5a.brm |>\n  conditional_effects(method = \"posterior_linpred\") \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects_5a-1.png){width=384}\n:::\n\n```{.r .cell-code}\ndat5a.brm |>\n  conditional_effects() |>\n  plot(points = TRUE) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects_5a-2.png){width=384}\n:::\n:::\n\n\n\n**Conclusions:** \n\n- the grey ribbon above represents the credible range of the posterior\n  predictions\n- this ribbon is substantially wider than the observed data (black\n  points), suggesting that the priors are not overly narrow\n- the range of posterior predictions are not widely unreasonable\n  (although we could argue that negative predictions might be\n  illogical) since the maximums are not multiple orders of magnitude\n  above the observed data.\n\n\n:::\n\n3. now refit the model such that it samples from both the priors and\n   likelihood (that is allow the data to have an impact on the\n   estimates)\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5a.brm2 <- update(dat5a.brm,\n    sample_prior = \"yes\"\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe desired updates require recompiling the model\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nOnly 2 levels detected so that family 'bernoulli' might be a more efficient choice.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n\n\n:::\n\n4. re-explore the range of posterior predictions resulting from the\n   fitted model\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5a.brm2 |>\n    conditional_effects() |>\n    plot(points = TRUE) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects2_5a-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions:** \n\n- the range of the posteriors are now substantially reduced (now that\n  the model includes both data and priors)\n- this suggests that the patterns are being driven predominantly by the data\n\n:::\n\n5. compare the priors and posteriors to further confirm that the\n   priors are not overly influential\n   \n::: {.indented}\n\nWhen we have indicated that the posterior should be informed by both\nthe prior and the posterior, both prior (governed by priors alone) and\nposterior (governed by both priors and data/likelihood) draws are\nreturned.  These can be compared by exploring the probabilities\nassociated with specific hypotheses - the most obvious of which is\nthat of no effect (that the parameter = 0).\n\nWhen doing so, ideally the posterior should be distinct from the\nprior. By distinct, I mean that the posteriors should be clearly\ndistinguishable from the priors. Ideally, the priors should be less\nvariable than the priors.  If this is not the case, it might suggest\nthat the posteriors are being too strongly driven by the priors.\n\n\n\n::: {.cell labels='brms_prior_posterior_5a'}\n\n```{.r .cell-code}\ndat5a.brm2 |> tidybayes::get_variables() \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"     \"b_x\"             \"Intercept\"       \"prior_Intercept\"\n [5] \"prior_b\"         \"lprior\"          \"lp__\"            \"accept_stat__\"  \n [9] \"stepsize__\"      \"treedepth__\"     \"n_leapfrog__\"    \"divergent__\"    \n[13] \"energy__\"       \n```\n\n\n:::\n\n```{.r .cell-code}\ndat5a.brm2 |> hypothesis(\"x = 0\") |> plot()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/unnamed-chunk-165-1.png){width=384}\n:::\n:::\n\n\n\nUnfortunately, it is not possible to do this comparison sensibly for\nthe intercept.  The reason for this is that the prior for intercept\nwas applied to an intercept that is associated with centred\ncontinuous predictors (predictors are centred behind the scenes).\nSince we did not centre the predictor, the intercept returned is as if\nuncentred.  Hence, the prior and posterior are on different scales.\n \n\n\n::: {.cell mhidden='true'}\n\n```{.r .cell-code}\ndat5a.brm2 |> tidybayes::get_variables() \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"     \"b_x\"             \"Intercept\"       \"prior_Intercept\"\n [5] \"prior_b\"         \"lprior\"          \"lp__\"            \"accept_stat__\"  \n [9] \"stepsize__\"      \"treedepth__\"     \"n_leapfrog__\"    \"divergent__\"    \n[13] \"energy__\"       \n```\n\n\n:::\n\n```{.r .cell-code}\ndat5a.brm2 |> SUYR_prior_and_posterior()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/fitModel5a-1.png){width=768}\n:::\n:::\n\n\n\n**Conclusions:**\n\n- each of the priors are substantially wider than the posteriors\n- the posteriors are definitely distinct from their respective priors\n  and thus we can conclude that the priors and not driving the\n  posteriors (e.g. they are not influencing the outcomes)\n- the priors are simply regularising the parameters such that they are\n  only sampled from plausible regions\n\n:::\n\n:::::\n\n#### Centered predictor\n\n$$\n\\begin{align}\ny_i \\sim{}& Bin(\\pi_i, 1)\\\\\nlog(\\frac{\\pi_i}{1 -\\pi_i}) =& \\beta_0 + (\\beta_1 x_i - \\bar{x})\\\\\n\\end{align}\n$$\n\n::::: {.panel-tabset}\n\n##### Define priors\n\n- $\\beta_0$: Normal prior centred at 0 with a\nvariance of 1\n- $\\beta_1$: _t_ distribution (3 degrees of freedom) prior centred at\n  0 with a variance of 1\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npriors <- prior(normal(0, 1), class = \"Intercept\") +\n    prior(student_t(3, 0, 1), class = \"b\")\n```\n:::\n\n\n\n##### Fit the model\n\n1. start by fitting the model and sampling from the priors only\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5b.form <- bf(y | trials(1) ~ scale(x, scale = FALSE), family = binomial(link = \"logit\"))\ndat5b.brm <- brm(dat5b.form,\n                data=dat5,\n                prior=priors,\n                sample_prior = 'only', \n                iter = 5000,\n                warmup = 1000,\n                chains = 3, cores = 3,\n                thin = 5,\n                backend = \"rstan\",\n                refresh = 0)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nOnly 2 levels detected so that family 'bernoulli' might be a more efficient choice.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n\n:::\n\n2. explore the range of posterior predictions resulting from the\n   priors alone\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5b.brm |>\n  conditional_effects(method = \"posterior_linpred\") \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects_5b-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions:** \n\n- the grey ribbon above represents the credible range of the posterior\n  predictions\n- this ribbon is substantially wider than the observed data (black\n  points), suggesting that the priors are not overly narrow\n- the range of posterior predictions are not widely unreasonable\n  (although we could argue that negative predictions might be\n  illogical) since the maximums are not multiple orders of magnitude\n  above the observed data.\n\n\n:::\n\n3. now refit the model such that it samples from both the priors and\n   likelihood (that is allow the data to have an impact on the\n   estimates)\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5b.brm2 <- update(dat5b.brm,\n    sample_prior = \"yes\"\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe desired updates require recompiling the model\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nOnly 2 levels detected so that family 'bernoulli' might be a more efficient choice.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n\n\n:::\n\n4. re-explore the range of posterior predictions resulting from the\n   fitted model\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5b.brm2 |>\n    conditional_effects() |>\n    plot(points = TRUE) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects2_5b-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions:** \n\n- the range of the posteriors are now substantially reduced (now that\n  the model includes both data and priors)\n- this suggests that the patterns are being driven predominantly by the data\n\n:::\n\n5. compare the priors and posteriors to further confirm that the\n   priors are not overly influential\n   \n::: {.indented}\n\nWhen we have indicated that the posterior should be informed by both\nthe prior and the posterior, both prior (governed by priors alone) and\nposterior (governed by both priors and data/likelihood) draws are\nreturned.  These can be compared by exploring the probabilities\nassociated with specific hypotheses - the most obvious of which is\nthat of no effect (that the parameter = 0).\n\nWhen doing so, ideally the posterior should be distinct from the\nprior. By distinct, I mean that the posteriors should be clearly\ndistinguishable from the priors. Ideally, the priors should be less\nvariable than the priors.  If this is not the case, it might suggest\nthat the posteriors are being too strongly driven by the priors.\n\n\n\n::: {.cell labels='brms_prior_posterior_5b'}\n\n```{.r .cell-code}\ndat5b.brm2 |> tidybayes::get_variables() \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"          \"b_scalexscaleEQFALSE\" \"Intercept\"           \n [4] \"prior_Intercept\"      \"prior_b\"              \"lprior\"              \n [7] \"lp__\"                 \"accept_stat__\"        \"stepsize__\"          \n[10] \"treedepth__\"          \"n_leapfrog__\"         \"divergent__\"         \n[13] \"energy__\"            \n```\n\n\n:::\n\n```{.r .cell-code}\ndat5b.brm2 |> hypothesis(\"scalexscaleEQFALSE = 0\") |> plot()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/unnamed-chunk-171-1.png){width=384}\n:::\n:::\n\n\n\nUnfortunately, it is not possible to do this comparison sensibly for\nthe intercept.  The reason for this is that the prior for intercept\nwas applied to an intercept that is associated with centred\ncontinuous predictors (predictors are centred behind the scenes).\nSince we did not centre the predictor, the intercept returned is as if\nuncentred.  Hence, the prior and posterior are on different scales.\n \n\n\n::: {.cell mhidden='true'}\n\n```{.r .cell-code}\ndat5b.brm2 |> tidybayes::get_variables() \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"          \"b_scalexscaleEQFALSE\" \"Intercept\"           \n [4] \"prior_Intercept\"      \"prior_b\"              \"lprior\"              \n [7] \"lp__\"                 \"accept_stat__\"        \"stepsize__\"          \n[10] \"treedepth__\"          \"n_leapfrog__\"         \"divergent__\"         \n[13] \"energy__\"            \n```\n\n\n:::\n\n```{.r .cell-code}\ndat5b.brm2 |> SUYR_prior_and_posterior()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/fitModel5b-1.png){width=768}\n:::\n:::\n\n\n\n**Conclusions:**\n\n- each of the priors are substantially wider than the posteriors\n- the posteriors are definitely distinct from their respective priors\n  and thus we can conclude that the priors and not driving the\n  posteriors (e.g. they are not influencing the outcomes)\n- the priors are simply regularising the parameters such that they are\n  only sampled from plausible regions\n\n:::\n\n:::::\n\n#### Standardised predictor\n\n$$\n\\begin{align}\ny_i \\sim{}& Bin(\\pi_i, 1)\\\\\nlog(\\frac{\\pi_i}{1 -\\pi_i}) =& \\beta_0 + (\\beta_1 x_i - \\bar{x})/\\sigma_x\\\\\n\\end{align}\n$$\n\n::::: {.panel-tabset}\n\n##### Define priors\n\n- $\\beta_0$: Normal prior centred at 0 with a\nvariance of 1\n- $\\beta_1$: _t_ distribution (3 degrees of freedom) prior centred at\n  0 with a variance of 1\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npriors <- prior(normal(0, 1), class = \"Intercept\") +\n    prior(student_t(3, 0, 1), class = \"b\")\n```\n:::\n\n\n\n##### Fit the model\n\n1. start by fitting the model and sampling from the priors only\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5c.form <- bf(y | trials(1) ~ scale(x), family = binomial(link = \"logit\"))\ndat5c.brm <- brm(dat5c.form,\n                data=dat5,\n                prior=priors,\n                sample_prior = 'only', \n                iter = 5000,\n                warmup = 1000,\n                chains = 3, cores = 3,\n                thin = 5,\n                backend = \"rstan\",\n                refresh = 0)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nOnly 2 levels detected so that family 'bernoulli' might be a more efficient choice.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n\n:::\n\n2. explore the range of posterior predictions resulting from the\n   priors alone\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5c.brm |>\n  conditional_effects(method = \"posterior_linpred\") \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects_5c-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions:** \n\n- the grey ribbon above represents the credible range of the posterior\n  predictions\n- this ribbon is substantially wider than the observed data (black\n  points), suggesting that the priors are not overly narrow\n- the range of posterior predictions are not widely unreasonable\n  (although we could argue that negative predictions might be\n  illogical) since the maximums are not multiple orders of magnitude\n  above the observed data.\n\n\n:::\n\n3. now refit the model such that it samples from both the priors and\n   likelihood (that is allow the data to have an impact on the\n   estimates)\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5c.brm2 <- update(dat5c.brm,\n    sample_prior = \"yes\"\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe desired updates require recompiling the model\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nOnly 2 levels detected so that family 'bernoulli' might be a more efficient choice.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n\n\n:::\n\n4. re-explore the range of posterior predictions resulting from the\n   fitted model\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5c.brm2 |>\n    conditional_effects() |>\n    plot(points = TRUE) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects2_5c-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions:** \n\n- the range of the posteriors are now substantially reduced (now that\n  the model includes both data and priors)\n- this suggests that the patterns are being driven predominantly by the data\n\n:::\n\n5. compare the priors and posteriors to further confirm that the\n   priors are not overly influential\n   \n::: {.indented}\n\nWhen we have indicated that the posterior should be informed by both\nthe prior and the posterior, both prior (governed by priors alone) and\nposterior (governed by both priors and data/likelihood) draws are\nreturned.  These can be compared by exploring the probabilities\nassociated with specific hypotheses - the most obvious of which is\nthat of no effect (that the parameter = 0).\n\nWhen doing so, ideally the posterior should be distinct from the\nprior. By distinct, I mean that the posteriors should be clearly\ndistinguishable from the priors. Ideally, the priors should be less\nvariable than the priors.  If this is not the case, it might suggest\nthat the posteriors are being too strongly driven by the priors.\n\n\n\n::: {.cell labels='brms_prior_posterior_5c'}\n\n```{.r .cell-code}\ndat5c.brm2 |> tidybayes::get_variables() \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"     \"b_scalex\"        \"Intercept\"       \"prior_Intercept\"\n [5] \"prior_b\"         \"lprior\"          \"lp__\"            \"accept_stat__\"  \n [9] \"stepsize__\"      \"treedepth__\"     \"n_leapfrog__\"    \"divergent__\"    \n[13] \"energy__\"       \n```\n\n\n:::\n\n```{.r .cell-code}\ndat5c.brm2 |> hypothesis(\"scalex = 0\") |> plot()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/unnamed-chunk-177-1.png){width=384}\n:::\n:::\n\n\n\nUnfortunately, it is not possible to do this comparison sensibly for\nthe intercept.  The reason for this is that the prior for intercept\nwas applied to an intercept that is associated with centred\ncontinuous predictors (predictors are centred behind the scenes).\nSince we did not centre the predictor, the intercept returned is as if\nuncentred.  Hence, the prior and posterior are on different scales.\n \n\n\n::: {.cell mhidden='true'}\n\n```{.r .cell-code}\ndat5c.brm2 |> tidybayes::get_variables() \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"     \"b_scalex\"        \"Intercept\"       \"prior_Intercept\"\n [5] \"prior_b\"         \"lprior\"          \"lp__\"            \"accept_stat__\"  \n [9] \"stepsize__\"      \"treedepth__\"     \"n_leapfrog__\"    \"divergent__\"    \n[13] \"energy__\"       \n```\n\n\n:::\n\n```{.r .cell-code}\ndat5c.brm2 |> SUYR_prior_and_posterior()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/fitModel5c-1.png){width=768}\n:::\n:::\n\n\n\n**Conclusions:**\n\n- each of the priors are substantially wider than the posteriors\n- the posteriors are definitely distinct from their respective priors\n  and thus we can conclude that the priors and not driving the\n  posteriors (e.g. they are not influencing the outcomes)\n- the priors are simply regularising the parameters such that they are\n  only sampled from plausible regions\n\n:::\n\n:::::\n\n:::::::\n\n### Example 6 (Binomial data)\n\n::::::: {.panel-tabset}\n#### Raw predictor\n\n$$\n\\begin{align}\ny_i \\sim{}& Bin(\\pi_i, n_i)\\\\\nlog(\\frac{\\pi_i}{1-\\pi_i}) =& \\beta_0 + \\beta_1 x_i\\\\\n\\end{align}\n$$\n\n::::: {.panel-tabset}\n\n##### Define priors\n\nWhen considering priors, it is important to remember that they apply\nto parameters on the link scale. For Binomial models, the link scale is\nlogit. Binomial data is notoriously difficult to define priors for.  Nevertheless the following considerations are useful:\n\n- the expected $\\pi$ values are only ever between 0 or 1\n- on a logit (log odds) scale, values of -3 and 3 are considered very wide\n- on a logit scale, values between -1 and 1 are reasonable.\n\nSo the following priors might be appropriate:\n\n- $\\beta_0$: Normal prior centred at  0 with a\nvariance of 0 \n  - mean: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    (dat6$count/dat6$total) |>\n        log() |> \n        median() |>\n        round(2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] -0.22\n    ```\n    \n    \n    :::\n    :::\n\n\n  - variance:\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    (dat6$count/dat6$total) |>\n        log() |> \n        mad() |>\n        round(2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 0.33\n    ```\n    \n    \n    :::\n    :::\n\n\n\n- $\\beta_1$: _t_ distribution (3 degrees of freedom) prior centred at\n  0 with a variance of 0.51\n  - mean: since effects are differences and we may not want to\n    pre-determine the direction (polarity) of any trends, it is\n    typical to define effects priors with means of 0\n  - variance: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dat6 |>\n        mutate(across(c(y, x), log)) |>\n        summarise(across(c(y, x), mad)) |>\n        mutate(round(y / x, 2))\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n              y        x round(y/x, 2)\n    1 0.3308326 0.648985          0.51\n    ```\n    \n    \n    :::\n    :::\n\n::: {.cell}\n\n```{.r .cell-code}\npriors <- prior(normal(-0.22, 0.33), class = \"Intercept\") +\n    prior(student_t(3, 0, 0.51), class = \"b\") \n```\n:::\n\n\n\n##### Fit the model\n\n1. start by fitting the model and sampling from the priors only\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6a.form <- bf(count | trials(total) ~ x, family = binomial(link = \"logit\"))\ndat6a.brm <- brm(dat6a.form,\n                data=dat6,\n                prior=priors,\n                sample_prior = 'only', \n                iter = 5000,\n                warmup = 1000,\n                chains = 3, cores = 3,\n                thin = 5,\n                backend = \"rstan\",\n                refresh = 0)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n\n:::\n\n2. explore the range of posterior predictions resulting from the\n   priors alone\n\n::: {.indented}\n\nFor Binary data, it is often more useful to explore the predictions on\nthe link scale. Ribbons that extend much beyond -3 and 3 would\ndefinitely be considered wide enough.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6a.brm |>\n  conditional_effects(method = \"posterior_linpred\") \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSetting all 'trials' variables to 1 by default if not specified otherwise.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects_6a-1.png){width=384}\n:::\n\n```{.r .cell-code}\ndat6a.brm |>\n  conditional_effects(conditions = data.frame(total = dat6$total)) |>\n  plot(points = TRUE) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects_6a-2.png){width=384}\n:::\n:::\n\n\n\n**Conclusions:** \n\n- the grey ribbon above represents the credible range of the posterior\n  predictions\n- this ribbon is substantially wider than the observed data (black\n  points), suggesting that the priors are not overly narrow\n- the range of posterior predictions are not widely unreasonable\n  (although we could argue that negative predictions might be\n  illogical) since the maximums are not multiple orders of magnitude\n  above the observed data.\n\n\n:::\n\n3. now refit the model such that it samples from both the priors and\n   likelihood (that is allow the data to have an impact on the\n   estimates)\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6a.brm2 <- update(dat6a.brm,\n    sample_prior = \"yes\"\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe desired updates require recompiling the model\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n\n\n:::\n\n4. re-explore the range of posterior predictions resulting from the\n   fitted model\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6a.brm2 |>\n    conditional_effects(conditions = data.frame(total = dat6$total)) |>\n    plot(points = TRUE) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects2_6a-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions:** \n\n- the range of the posteriors are now substantially reduced (now that\n  the model includes both data and priors)\n- this suggests that the patterns are being driven predominantly by the data\n\n:::\n\n5. compare the priors and posteriors to further confirm that the\n   priors are not overly influential\n   \n::: {.indented}\n\nWhen we have indicated that the posterior should be informed by both\nthe prior and the posterior, both prior (governed by priors alone) and\nposterior (governed by both priors and data/likelihood) draws are\nreturned.  These can be compared by exploring the probabilities\nassociated with specific hypotheses - the most obvious of which is\nthat of no effect (that the parameter = 0).\n\nWhen doing so, ideally the posterior should be distinct from the\nprior. By distinct, I mean that the posteriors should be clearly\ndistinguishable from the priors. Ideally, the priors should be less\nvariable than the priors.  If this is not the case, it might suggest\nthat the posteriors are being too strongly driven by the priors.\n\n\n\n::: {.cell labels='brms_prior_posterior_6a'}\n\n```{.r .cell-code}\ndat6a.brm2 |> tidybayes::get_variables() \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"     \"b_x\"             \"Intercept\"       \"prior_Intercept\"\n [5] \"prior_b\"         \"lprior\"          \"lp__\"            \"accept_stat__\"  \n [9] \"stepsize__\"      \"treedepth__\"     \"n_leapfrog__\"    \"divergent__\"    \n[13] \"energy__\"       \n```\n\n\n:::\n\n```{.r .cell-code}\ndat6a.brm2 |> hypothesis(\"x = 0\") |> plot()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/unnamed-chunk-186-1.png){width=384}\n:::\n:::\n\n\n\nUnfortunately, it is not possible to do this comparison sensibly for\nthe intercept.  The reason for this is that the prior for intercept\nwas applied to an intercept that is associated with centred\ncontinuous predictors (predictors are centred behind the scenes).\nSince we did not centre the predictor, the intercept returned is as if\nuncentred.  Hence, the prior and posterior are on different scales.\n \n\n\n::: {.cell mhidden='true'}\n\n```{.r .cell-code}\ndat6a.brm2 |> tidybayes::get_variables() \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"     \"b_x\"             \"Intercept\"       \"prior_Intercept\"\n [5] \"prior_b\"         \"lprior\"          \"lp__\"            \"accept_stat__\"  \n [9] \"stepsize__\"      \"treedepth__\"     \"n_leapfrog__\"    \"divergent__\"    \n[13] \"energy__\"       \n```\n\n\n:::\n\n```{.r .cell-code}\ndat6a.brm2 |> SUYR_prior_and_posterior()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/fitModel6a-1.png){width=768}\n:::\n:::\n\n\n\n**Conclusions:**\n\n- each of the priors are substantially wider than the posteriors\n- the posteriors are definitely distinct from their respective priors\n  and thus we can conclude that the priors and not driving the\n  posteriors (e.g. they are not influencing the outcomes)\n- the priors are simply regularising the parameters such that they are\n  only sampled from plausible regions\n\n:::\n\n:::::\n\n#### Centered predictor\n\n$$\n\\begin{align}\ny_i \\sim{}& Bin(\\pi_i, n_i)\\\\\nlog(\\frac{\\pi_i}{1-\\pi_i}) =& \\beta_0 + (\\beta_1 x_i - \\bar{x})\\\\\n\\end{align}\n$$\n\n::::: {.panel-tabset}\n\n##### Define priors\n\nWhen considering priors, it is important to remember that they apply\nto parameters on the link scale. For Binomial models, the link scale is\nlogit. Binomial data is notoriously difficult to define priors for.  Nevertheless the following considerations are useful:\n\n- the expected $\\pi$ values are only ever between 0 or 1\n- on a logit (log odds) scale, values of -3 and 3 are considered very wide\n- on a logit scale, values between -1 and 1 are reasonable.\n\nSo the following priors might be appropriate:\n\n- $\\beta_0$: Normal prior centred at  0 with a\nvariance of 0 \n  - mean: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    (dat6$count/dat6$total) |>\n        log() |> \n        median() |>\n        round(2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] -0.22\n    ```\n    \n    \n    :::\n    :::\n\n\n  - variance:\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    (dat6$count/dat6$total) |>\n        log() |> \n        mad() |>\n        round(2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 0.33\n    ```\n    \n    \n    :::\n    :::\n\n\n\n- $\\beta_1$: _t_ distribution (3 degrees of freedom) prior centred at\n  0 with a variance of 0.51\n  - mean: since effects are differences and we may not want to\n    pre-determine the direction (polarity) of any trends, it is\n    typical to define effects priors with means of 0\n  - variance: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dat6 |>\n        mutate(across(c(y, x), log)) |>\n        summarise(across(c(y, x), mad)) |>\n        mutate(round(y / x, 2))\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n              y        x round(y/x, 2)\n    1 0.3308326 0.648985          0.51\n    ```\n    \n    \n    :::\n    :::\n\n::: {.cell}\n\n```{.r .cell-code}\npriors <- prior(normal(-0.22, 0.33), class = \"Intercept\") +\n    prior(student_t(3, 0, 0.51), class = \"b\") \n```\n:::\n\n\n\n##### Fit the model\n\n1. start by fitting the model and sampling from the priors only\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6b.form <- bf(count | trials(total) ~ scale(x, scale = FALSE), \n  family = binomial(link = \"logit\"))\ndat6b.brm <- brm(dat6b.form,\n                data=dat6,\n                prior=priors,\n                sample_prior = 'only', \n                iter = 5000,\n                warmup = 1000,\n                chains = 3, cores = 3,\n                thin = 5,\n                backend = \"rstan\",\n                refresh = 0)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n\n:::\n\n2. explore the range of posterior predictions resulting from the\n   priors alone\n\n::: {.indented}\n\nFor Binomial data, it is often more useful to explore the predictions on\nthe link scale. Ribbons that extend much beyond -3 and 3 would\ndefinitely be considered wide enough.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6b.brm |>\n  conditional_effects(method = \"posterior_linpred\") \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSetting all 'trials' variables to 1 by default if not specified otherwise.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects_6b-1.png){width=384}\n:::\n\n```{.r .cell-code}\ndat6b.brm |>\n  conditional_effects(conditions = data.frame(total = dat6$total)) |>\n  plot(points = TRUE) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects_6b-2.png){width=384}\n:::\n:::\n\n\n\n**Conclusions:** \n\n- the grey ribbon above represents the credible range of the posterior\n  predictions\n- this ribbon is substantially wider than the observed data (black\n  points), suggesting that the priors are not overly narrow\n- the range of posterior predictions are not widely unreasonable\n  (although we could argue that negative predictions might be\n  illogical) since the maximums are not multiple orders of magnitude\n  above the observed data.\n\n\n:::\n\n3. now refit the model such that it samples from both the priors and\n   likelihood (that is allow the data to have an impact on the\n   estimates)\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6b.brm2 <- update(dat6b.brm,\n    sample_prior = \"yes\"\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe desired updates require recompiling the model\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n\n\n:::\n\n4. re-explore the range of posterior predictions resulting from the\n   fitted model\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6b.brm2 |>\n    conditional_effects(conditions = data.frame(total = dat6$total)) |>\n    plot(points = TRUE) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects2_6b-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions:** \n\n- the range of the posteriors are now substantially reduced (now that\n  the model includes both data and priors)\n- this suggests that the patterns are being driven predominantly by the data\n\n:::\n\n5. compare the priors and posteriors to further confirm that the\n   priors are not overly influential\n   \n::: {.indented}\n\nWhen we have indicated that the posterior should be informed by both\nthe prior and the posterior, both prior (governed by priors alone) and\nposterior (governed by both priors and data/likelihood) draws are\nreturned.  These can be compared by exploring the probabilities\nassociated with specific hypotheses - the most obvious of which is\nthat of no effect (that the parameter = 0).\n\nWhen doing so, ideally the posterior should be distinct from the\nprior. By distinct, I mean that the posteriors should be clearly\ndistinguishable from the priors. Ideally, the priors should be less\nvariable than the priors.  If this is not the case, it might suggest\nthat the posteriors are being too strongly driven by the priors.\n\n\n\n::: {.cell labels='brms_prior_posterior_6b'}\n\n```{.r .cell-code}\ndat6b.brm2 |> tidybayes::get_variables() \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"          \"b_scalexscaleEQFALSE\" \"Intercept\"           \n [4] \"prior_Intercept\"      \"prior_b\"              \"lprior\"              \n [7] \"lp__\"                 \"accept_stat__\"        \"stepsize__\"          \n[10] \"treedepth__\"          \"n_leapfrog__\"         \"divergent__\"         \n[13] \"energy__\"            \n```\n\n\n:::\n\n```{.r .cell-code}\ndat6b.brm2 |> hypothesis(\"scalexscaleEQFALSE = 0\") |> plot()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/unnamed-chunk-195-1.png){width=384}\n:::\n:::\n\n\n\nUnfortunately, it is not possible to do this comparison sensibly for\nthe intercept.  The reason for this is that the prior for intercept\nwas applied to an intercept that is associated with centred\ncontinuous predictors (predictors are centred behind the scenes).\nSince we did not centre the predictor, the intercept returned is as if\nuncentred.  Hence, the prior and posterior are on different scales.\n \n\n\n::: {.cell mhidden='true'}\n\n```{.r .cell-code}\ndat6b.brm2 |> tidybayes::get_variables() \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"          \"b_scalexscaleEQFALSE\" \"Intercept\"           \n [4] \"prior_Intercept\"      \"prior_b\"              \"lprior\"              \n [7] \"lp__\"                 \"accept_stat__\"        \"stepsize__\"          \n[10] \"treedepth__\"          \"n_leapfrog__\"         \"divergent__\"         \n[13] \"energy__\"            \n```\n\n\n:::\n\n```{.r .cell-code}\ndat6b.brm2 |> SUYR_prior_and_posterior()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/fitModel6b-1.png){width=768}\n:::\n:::\n\n\n\n**Conclusions:**\n\n- each of the priors are substantially wider than the posteriors\n- the posteriors are definitely distinct from their respective priors\n  and thus we can conclude that the priors and not driving the\n  posteriors (e.g. they are not influencing the outcomes)\n- the priors are simply regularising the parameters such that they are\n  only sampled from plausible regions\n\n:::\n\n:::::\n\n#### Standardized predictor\n\n$$\n\\begin{align}\ny_i \\sim{}& Bin(\\pi_i, n_i)\\\\\nlog(\\frac{\\pi_i}{1-\\pi_i}) =& \\beta_0 + (\\beta_1 x_i - \\bar{x})\\sigma\\\\\n\\end{align}\n$$\n\n::::: {.panel-tabset}\n\n##### Define priors\n\nWhen considering priors, it is important to remember that they apply\nto parameters on the link scale. For Binomial models, the link scale is\nlogit. Binomial data is notoriously difficult to define priors for.  Nevertheless the following considerations are useful:\n\n- the expected $\\pi$ values are only ever between 0 or 1\n- on a logit (log odds) scale, values of -3 and 3 are considered very wide\n- on a logit scale, values between -1 and 1 are reasonable.\n\nSo the following priors might be appropriate:\n\n- $\\beta_0$: Normal prior centred at  0 with a\nvariance of 0 \n  - mean: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    (dat6$count/dat6$total) |>\n        log() |> \n        median() |>\n        round(2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] -0.22\n    ```\n    \n    \n    :::\n    :::\n\n\n  - variance:\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    (dat6$count/dat6$total) |>\n        log() |> \n        mad() |>\n        round(2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 0.33\n    ```\n    \n    \n    :::\n    :::\n\n\n\n- $\\beta_1$: _t_ distribution (3 degrees of freedom) prior centred at\n  0 with a variance of 0.33\n  - mean: since effects are differences and we may not want to\n    pre-determine the direction (polarity) of any trends, it is\n    typical to define effects priors with means of 0\n  - variance: \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    (dat6$count / dat6$total) |>\n        log() |>\n        mad() |>\n        round(2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 0.33\n    ```\n    \n    \n    :::\n    :::\n\n::: {.cell}\n\n```{.r .cell-code}\npriors <- prior(normal(-0.22, 0.33), class = \"Intercept\") +\n    prior(student_t(3, 0, 0.33), class = \"b\") \n```\n:::\n\n\n\n##### Fit the model\n\n1. start by fitting the model and sampling from the priors only\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6c.form <- bf(count | trials(total) ~ scale(x), \n  family = binomial(link = \"logit\"))\ndat6c.brm <- brm(dat6c.form,\n                data=dat6,\n                prior=priors,\n                sample_prior = 'only', \n                iter = 5000,\n                warmup = 1000,\n                chains = 3, cores = 3,\n                thin = 5,\n                backend = \"rstan\",\n                refresh = 0)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n\n:::\n\n2. explore the range of posterior predictions resulting from the\n   priors alone\n\n::: {.indented}\n\nFor Binomial data, it is often more useful to explore the predictions on\nthe link scale. Ribbons that extend much beyond -3 and 3 would\ndefinitely be considered wide enough.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6c.brm |>\n  conditional_effects(method = \"posterior_linpred\") \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSetting all 'trials' variables to 1 by default if not specified otherwise.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects_6c-1.png){width=384}\n:::\n\n```{.r .cell-code}\ndat6c.brm |>\n  conditional_effects(conditions = data.frame(total = dat6$total)) |>\n  plot(points = TRUE) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects_6c-2.png){width=384}\n:::\n:::\n\n\n\n**Conclusions:** \n\n- the grey ribbon above represents the credible range of the posterior\n  predictions\n- this ribbon is substantially wider than the observed data (black\n  points), suggesting that the priors are not overly narrow\n- the range of posterior predictions are not widely unreasonable\n  (although we could argue that negative predictions might be\n  illogical) since the maximums are not multiple orders of magnitude\n  above the observed data.\n\n\n:::\n\n3. now refit the model such that it samples from both the priors and\n   likelihood (that is allow the data to have an impact on the\n   estimates)\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6c.brm2 <- update(dat6c.brm,\n    sample_prior = \"yes\"\n) \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe desired updates require recompiling the model\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n\n\n:::\n\n4. re-explore the range of posterior predictions resulting from the\n   fitted model\n\n::: {.indented}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6c.brm2 |>\n    conditional_effects(conditions = data.frame(total = dat6$total)) |>\n    plot(points = TRUE)  \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/brms_conditional_effects2_6c-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions:** \n\n- the range of the posteriors are now substantially reduced (now that\n  the model includes both data and priors)\n- this suggests that the patterns are being driven predominantly by the data\n\n:::\n\n5. compare the priors and posteriors to further confirm that the\n   priors are not overly influential\n   \n::: {.indented}\n\nWhen we have indicated that the posterior should be informed by both\nthe prior and the posterior, both prior (governed by priors alone) and\nposterior (governed by both priors and data/likelihood) draws are\nreturned.  These can be compared by exploring the probabilities\nassociated with specific hypotheses - the most obvious of which is\nthat of no effect (that the parameter = 0).\n\nWhen doing so, ideally the posterior should be distinct from the\nprior. By distinct, I mean that the posteriors should be clearly\ndistinguishable from the priors. Ideally, the priors should be less\nvariable than the priors.  If this is not the case, it might suggest\nthat the posteriors are being too strongly driven by the priors.\n\n\n\n::: {.cell labels='brms_prior_posterior_6c'}\n\n```{.r .cell-code}\ndat6c.brm2 |> tidybayes::get_variables() \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"     \"b_scalex\"        \"Intercept\"       \"prior_Intercept\"\n [5] \"prior_b\"         \"lprior\"          \"lp__\"            \"accept_stat__\"  \n [9] \"stepsize__\"      \"treedepth__\"     \"n_leapfrog__\"    \"divergent__\"    \n[13] \"energy__\"       \n```\n\n\n:::\n\n```{.r .cell-code}\ndat6c.brm2 |> hypothesis(\"scalex = 0\") |> plot() \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/unnamed-chunk-204-1.png){width=384}\n:::\n:::\n\n\n\nUnfortunately, it is not possible to do this comparison sensibly for\nthe intercept.  The reason for this is that the prior for intercept\nwas applied to an intercept that is associated with centred\ncontinuous predictors (predictors are centred behind the scenes).\nSince we did not centre the predictor, the intercept returned is as if\nuncentred.  Hence, the prior and posterior are on different scales.\n \n\n\n::: {.cell mhidden='true'}\n\n```{.r .cell-code}\ndat6c.brm2 |> tidybayes::get_variables() \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"     \"b_scalex\"        \"Intercept\"       \"prior_Intercept\"\n [5] \"prior_b\"         \"lprior\"          \"lp__\"            \"accept_stat__\"  \n [9] \"stepsize__\"      \"treedepth__\"     \"n_leapfrog__\"    \"divergent__\"    \n[13] \"energy__\"       \n```\n\n\n:::\n\n```{.r .cell-code}\ndat6c.brm2 |> SUYR_prior_and_posterior() \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/fitModel6c-1.png){width=768}\n:::\n:::\n\n\n\n**Conclusions:**\n\n- each of the priors are substantially wider than the posteriors\n- the posteriors are definitely distinct from their respective priors\n  and thus we can conclude that the priors and not driving the\n  posteriors (e.g. they are not influencing the outcomes)\n- the priors are simply regularising the parameters such that they are\n  only sampled from plausible regions\n\n:::\n\n:::::\n\n\n:::::::\n\n\n::::\n\n\n# MCMC sampling diagnostics\n    \n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n**MCMC sampling behaviour**\n\nSince the purpose of the MCMC sampling is to estimate the posterior of\nan unknown joint likelihood, it is important that we explore a range\nof diagnostics designed to help identify when the resulting likelihood\nmight not be accurate.\n\n- **traceplots** - plots of the individual draws in sequence.  Traces\n  that resemble noise suggest that all likelihood features are likely\n  to have be traversed. Obvious steps or blocks of noise are likely to\n  represent distinct features and could imply that there are yet other\n  features that have not yet been traversed - necessitating additional\n  iterations.  Furthermore, each chain should be indistinguishable\n  from the others\n\n::: {.indented}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/sim7-1.png){width=864}\n:::\n:::\n\n\n  \n:::\n\n- **autocorrelation function** - plots of the degree of correlation\n  between pairs of draws for a range of lags (distance along the\n  chains). High levels of correlation (after a lag of 0, which is\n  correlating each draw with itself) suggests a lack of independence\n  between the draws and that therefore, summaries such as mean and\n  median will be biased estimates. Ideally, all non-zero lag\n  correlations should be less than 0.2. The left hand figure below\n  demonstrates a clear pattern of autocorrelation, whereas the right\n  hand figure shows no autocorrelation.\n\n::: {.indented}\n\n:::: {.columns}\n\n::::: {.column width=\"45%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/sim8-1.png){width=384}\n:::\n:::\n\n\n\n:::::\n\n::::: {.column width=\"45%\"}\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/sim8b2-1.png){width=384}\n:::\n:::\n\n\n:::::\n\n::::\n\n:::\n\n- **convergence diagnostics** - there are a range of diagnostics aimed\n  at exploring whether the multiple chains are likely to have\n  converged upon similar posteriors\n  - **R hat** - this metric compares between and within chain model\n    parameter estimates, with the expectation that if the chains have\n    converged, the between and within rank normalised estimates should\n    be very similar (and Rhat should be close to 1).  The more one\n    chains deviates from the others, the higher the Rhat value.\n    Values less than 1.05 are considered evidence of convergence.\n  - **Bulk ESS** - this is a measure of the effective sample size\n    from the whole (bulk) of the posterior and is a good measure of\n    the sampling efficiency of draws across the entire posterior\n  - **Tail ESS** - this is a measure of the effective sample size from\n    the 5% and 95% quantiles (tails) of the posterior and is a good\n    measure of the sampling efficiency of draws from the tail (areas\n    of the posterior with least support and where samplers can get\n    stuck).\n\nThere are numerous packages in R that support MCMC diagnostics.  Popular packages include:\n\n- `bayesplot`\n- `rstan`\n- `ggmcmcm`\n\nSome of the most useful diagnostics are presented in the following table.\n\n| Package   | Description       | function               | rstanarm                         | brms                               |\n|-----------|-------------------|------------------------|----------------------------------|------------------------------------|\n| bayesplot | Traceplot         | `mcmc_trace`           | `plot(mod, plotfun='trace')`     | `mcmc_plot(mod, type='trace')`     |\n|           | Density plot      | `mcmc_dens`            | `plot(mod, plotfun='dens')`      | `mcmc_plot(mod, type='dens')`      |\n|           | Density & Trace   | `mcmc_combo`           | `plot(mod, plotfun='combo')`     | `mcmc_plot(mod, type='combo')`     |\n|           | ACF               | `mcmc_acf_bar`         | `plot(mod, plotfun='acf_bar')`   | `mcmc_plot(mod, type='acf_bar')`   |\n|           | Rhat hist         | `mcmc_rhat_hist`       | `plot(mod, plotfun='rhat_hist')` | `mcmc_plot(mod, type='rhat_hist')` |\n|           | No. Effective     | `mcmc_neff_hist`       | `plot(mod, plotfun='neff_hist')` | `mcmc_plot(mod, type='neff_hist')` |\n| rstan     | Traceplot         | `stan_trace`           | `stan_trace(mod)`                | `stan_trace(mod)`                  |\n|           | ACF               | `stan_ac`              | `stan_ac(mod)`                   | `stan_ac(mod)`                     |\n|           | Rhat              | `stan_rhat`            | `stan_rhat(mod)`                 | `stan_rhat(mod)`                   |\n|           | No. Effective     | `stan_ess`             | `stan_ess(mod)`                  | `stan_ess(mod)`                    |\n|           | Density plot      | `stan_dens`            | `stan_dens(mod)`                 | `stan_dens(mod)`                   |\n| ggmcmc    | Traceplot         | `ggs_traceplot`        | `ggs_traceplot(ggs(mod))`        | `ggs_traceplot(ggs(mod))`          |\n|           | ACF               | `ggs_autocorrelation`  | `ggs_autocorrelation(ggs(mod))`  | `ggs_autocorrelation(ggs(mod))`    |\n|           | Rhat              | `ggs_Rhat`             | `ggs_Rhat(ggs(mod))`             | `ggs_Rhat(ggs(mod))`               |\n|           | No. Effective     | `ggs_effective`        | `ggs_effective(ggs(mod))`        | `ggs_effective(ggs(mod))`          |\n|           | Cross correlation | `ggs_crosscorrelation` | `ggs_crosscorrelation(ggs(mod))` | `ggs_crosscorrelation(ggs(mod))`   |\n|           | Scale reduction   | `ggs_grb`              | `ggs_grb(ggs(mod))`              | `ggs_grb(ggs(mod))`                |\n|           |                   |                        |                                  |                                    |\n\n: {.primary .bordered .sm .paramsTable tbl-colwidths=\"[10,10, 20, 30, 30]\"}\n\n\nI personally prefer the `rstan` version of plots and thus these are\nthe ones I will showcase.\n\n\n:::: {.panel-tabset}\n\n\n### Example 1 (Gaussian data)\n\n::: {.callout-note}\n\nBayesian samplers involve many calls to randomisation functions. As a\nresult, the estimates will vary slightly each time the routines are\nrun. You should expect that the outputs that you obtain will differ\nslightly from those that I am displaying. Nevertheless, the main\nconclusions should remain robust across subsequent runs.\n\n:::\n\n::::::: {.panel-tabset}\n#### Raw predictor\n::::: {.panel-tabset}\n\n##### Traceplots\n\nPlots of the estimates of each parameter over the post-warmup length\nof each MCMC chain. Each chain is plotted in a different colour, with\neach parameter in its own facet. Ideally, each **trace** should just\nlook like noise without any discernible drift and each of the traces\nfor a specific parameter should look the same (i.e, should not be\ndisplaced above or below any other trace for that parameter).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1a.brm2$fit |> stan_trace()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_traceplots1a-1.png){width=576}\n:::\n\n```{.r .cell-code}\ndat1a.brm2$fit |> stan_trace(inc_warmup = TRUE)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_traceplots1a-2.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n    - the chains appear well mixed and very similar\n   \n##### Autocorrelation plots\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1a.brm2$fit |> stan_ac()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_acf1a-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there is no evidence of autocorrelation in the MCMC samples\n\n##### Rhat\n\nRhat is a **scale reduction factor** measure of convergence between\nthe chains. The closer the values are to 1, the more the chains have\nconverged. Values greater than 1.05 indicate a lack of convergence.\nThere will be an Rhat value for each parameter estimated.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1a.brm2$fit |> stan_rhat()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_rhat1a-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- all Rhat values are below 1.05, suggesting the chains have converged. \n\n##### Effective sample sizes\n\nThe number of effective samples - the ratio of the number of effective\nsamples (those not rejected by the sampler) to the number of samples\nprovides an indication of the effectiveness (and efficiency) of the\nMCMC sampler. Ratios that are less than 0.5 for a parameter suggest\nthat the sampler spent considerable time in difficult areas of the\nsampling domain and rejected more than half of the samples (replacing\nthem with the previous effective sample).\n  \nIf the ratios are low, tightening the priors may help.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1a.brm2$fit |> stan_ess()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ess1a-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- ratios are all very high\n\n:::::\n\n**Conclusions**:\n\n- all the diagnostics appear reasonable\n- we can conclude that the chains are all well mixed and have\n  converged on a stable posterior.\n\n\n#### Centered predictor\n\n::::: {.panel-tabset}\n\n##### Traceplots\n\nPlots of the estimates of each parameter over the post-warmup length\nof each MCMC chain. Each chain is plotted in a different colour, with\neach parameter in its own facet. Ideally, each **trace** should just\nlook like noise without any discernible drift and each of the traces\nfor a specific parameter should look the same (i.e, should not be\ndisplaced above or below any other trace for that parameter).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1b.brm2$fit |> stan_trace()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_traceplots1b-1.png){width=576}\n:::\n\n```{.r .cell-code}\ndat1b.brm2$fit |> stan_trace(inc_warmup = TRUE)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_traceplots1b-2.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n    - the chains appear well mixed and very similar\n   \n##### Autocorrelation plots\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1b.brm2$fit |> stan_ac()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_acf1b-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there is no evidence of autocorrelation in the MCMC samples\n\n##### Rhat\n\nRhat is a **scale reduction factor** measure of convergence between\nthe chains. The closer the values are to 1, the more the chains have\nconverged. Values greater than 1.05 indicate a lack of convergence.\nThere will be an Rhat value for each parameter estimated.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1b.brm2$fit |> stan_rhat()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_rhat1b-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- all Rhat values are below 1.05, suggesting the chains have converged. \n\n##### Effective sample sizes\n\nThe number of effective samples - the ratio of the number of effective\nsamples (those not rejected by the sampler) to the number of samples\nprovides an indication of the effectiveness (and efficiency) of the\nMCMC sampler. Ratios that are less than 0.5 for a parameter suggest\nthat the sampler spent considerable time in difficult areas of the\nsampling domain and rejected more than half of the samples (replacing\nthem with the previous effective sample).\n  \nIf the ratios are low, tightening the priors may help.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1b.brm2$fit |> stan_ess()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ess1b-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- ratios are all very high\n\n:::::\n\n**Conclusions**:\n\n- all the diagnostics appear reasonable\n- we can conclude that the chains are all well mixed and have\n  converged on a stable posterior.\n  \n#### Standardised predictor\n\n::::: {.panel-tabset}\n\n##### Traceplots\n\nPlots of the estimates of each parameter over the post-warmup length\nof each MCMC chain. Each chain is plotted in a different colour, with\neach parameter in its own facet. Ideally, each **trace** should just\nlook like noise without any discernible drift and each of the traces\nfor a specific parameter should look the same (i.e, should not be\ndisplaced above or below any other trace for that parameter).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1c.brm2$fit |> stan_trace()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_traceplots1c-1.png){width=576}\n:::\n\n```{.r .cell-code}\ndat1c.brm2$fit |> stan_trace(inc_warmup = TRUE)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_traceplots1c-2.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n    - the chains appear well mixed and very similar\n   \n##### Autocorrelation plots\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1c.brm2$fit |> stan_ac()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_acf1c-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there is no evidence of autocorrelation in the MCMC samples\n\n##### Rhat\n\nRhat is a **scale reduction factor** measure of convergence between\nthe chains. The closer the values are to 1, the more the chains have\nconverged. Values greater than 1.05 indicate a lack of convergence.\nThere will be an Rhat value for each parameter estimated.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1c.brm2$fit |> stan_rhat()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_rhat1c-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- all Rhat values are below 1.05, suggesting the chains have converged. \n\n##### Effective sample sizes\n\nThe number of effective samples - the ratio of the number of effective\nsamples (those not rejected by the sampler) to the number of samples\nprovides an indication of the effectiveness (and efficiency) of the\nMCMC sampler. Ratios that are less than 0.5 for a parameter suggest\nthat the sampler spent considerable time in difficult areas of the\nsampling domain and rejected more than half of the samples (replacing\nthem with the previous effective sample).\n  \nIf the ratios are low, tightening the priors may help.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1c.brm2$fit |> stan_ess()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ess1c-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- ratios are all very high\n\n:::::\n\n**Conclusions**:\n\n- all the diagnostics appear reasonable\n- we can conclude that the chains are all well mixed and have\n  converged on a stable posterior.\n  \n:::::::\n \n### Example 2 (categorical predictor)\n\n::::::: {.panel-tabset}\n#### Treatment contrasts\n::::: {.panel-tabset}\n\n##### Traceplots\n\nPlots of the estimates of each parameter over the post-warmup length\nof each MCMC chain. Each chain is plotted in a different colour, with\neach parameter in its own facet. Ideally, each **trace** should just\nlook like noise without any discernible drift and each of the traces\nfor a specific parameter should look the same (i.e, should not be\ndisplaced above or below any other trace for that parameter).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2a.brm2$fit |> stan_trace()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_traceplots2a-1.png){width=576}\n:::\n\n```{.r .cell-code}\ndat2a.brm2$fit |> stan_trace(inc_warmup = TRUE)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_traceplots2a-2.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n    - the chains appear well mixed and very similar\n   \n##### Autocorrelation plots\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2a.brm2$fit |> stan_ac()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_acf2a-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there is no evidence of autocorrelation in the MCMC samples\n\n##### Rhat\n\nRhat is a **scale reduction factor** measure of convergence between\nthe chains. The closer the values are to 1, the more the chains have\nconverged. Values greater than 1.05 indicate a lack of convergence.\nThere will be an Rhat value for each parameter estimated.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2a.brm2$fit |> stan_rhat()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_rhat2a-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- all Rhat values are below 1.05, suggesting the chains have converged. \n\n##### Effective sample sizes\n\nThe number of effective samples - the ratio of the number of effective\nsamples (those not rejected by the sampler) to the number of samples\nprovides an indication of the effectiveness (and efficiency) of the\nMCMC sampler. Ratios that are less than 0.5 for a parameter suggest\nthat the sampler spent considerable time in difficult areas of the\nsampling domain and rejected more than half of the samples (replacing\nthem with the previous effective sample).\n  \nIf the ratios are low, tightening the priors may help.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2a.brm2$fit |> stan_ess()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ess2a-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- ratios are all very high\n\n:::::\n\n**Conclusions**:\n\n- all the diagnostics appear reasonable\n- we can conclude that the chains are all well mixed and have\n  converged on a stable posterior.\n\n#### Means parameterisation\n\n::::: {.panel-tabset}\n\n##### Traceplots\n\nPlots of the estimates of each parameter over the post-warmup length\nof each MCMC chain. Each chain is plotted in a different colour, with\neach parameter in its own facet. Ideally, each **trace** should just\nlook like noise without any discernible drift and each of the traces\nfor a specific parameter should look the same (i.e, should not be\ndisplaced above or below any other trace for that parameter).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2b.brm2$fit |> stan_trace()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_traceplots2b-1.png){width=576}\n:::\n\n```{.r .cell-code}\ndat2b.brm2$fit |> stan_trace(inc_warmup = TRUE)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_traceplots2b-2.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n    - the chains appear well mixed and very similar\n   \n##### Autocorrelation plots\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2b.brm2$fit |> stan_ac()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_acf2b-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there is no evidence of autocorrelation in the MCMC samples\n\n##### Rhat\n\nRhat is a **scale reduction factor** measure of convergence between\nthe chains. The closer the values are to 1, the more the chains have\nconverged. Values greater than 1.05 indicate a lack of convergence.\nThere will be an Rhat value for each parameter estimated.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2b.brm2$fit |> stan_rhat()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_rhat2b-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- all Rhat values are below 1.05, suggesting the chains have converged. \n\n##### Effective sample sizes\n\nThe number of effective samples - the ratio of the number of effective\nsamples (those not rejected by the sampler) to the number of samples\nprovides an indication of the effectiveness (and efficiency) of the\nMCMC sampler. Ratios that are less than 0.5 for a parameter suggest\nthat the sampler spent considerable time in difficult areas of the\nsampling domain and rejected more than half of the samples (replacing\nthem with the previous effective sample).\n  \nIf the ratios are low, tightening the priors may help.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2b.brm2$fit |> stan_ess()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ess2b-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- ratios are all very high\n\n:::::\n\n**Conclusions**:\n\n- all the diagnostics appear reasonable\n- we can conclude that the chains are all well mixed and have\n  converged on a stable posterior.\n\n::::::: \n\n### Example 3 (Poisson data)\n\n::::::: {.panel-tabset}\n#### Raw predictor\n\n::::: {.panel-tabset}\n##### Traceplots\n\nPlots of the estimates of each parameter over the post-warmup length\nof each MCMC chain. Each chain is plotted in a different colour, with\neach parameter in its own facet. Ideally, each **trace** should just\nlook like noise without any discernible drift and each of the traces\nfor a specific parameter should look the same (i.e, should not be\ndisplaced above or below any other trace for that parameter).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3a.brm2$fit |> stan_trace()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_traceplots3a-1.png){width=576}\n:::\n\n```{.r .cell-code}\ndat3a.brm2$fit |> stan_trace(inc_warmup = TRUE)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_traceplots3a-2.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n    - the chains appear well mixed and very similar\n\n##### Autocorrelation plots\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3a.brm2$fit |> stan_ac()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_acf3a-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there is no evidence of autocorrelation in the MCMC samples\n\n##### Rhat\n\nRhat is a **scale reduction factor** measure of convergence between\nthe chains. The closer the values are to 1, the more the chains have\nconverged. Values greater than 1.05 indicate a lack of convergence.\nThere will be an Rhat value for each parameter estimated.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3a.brm2$fit |> stan_rhat()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_rhat3a-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- all Rhat values are below 1.05, suggesting the chains have converged. \n\n##### Effective sample sizes\n\nThe number of effective samples - the ratio of the number of effective\nsamples (those not rejected by the sampler) to the number of samples\nprovides an indication of the effectiveness (and efficiency) of the\nMCMC sampler. Ratios that are less than 0.5 for a parameter suggest\nthat the sampler spent considerable time in difficult areas of the\nsampling domain and rejected more than half of the samples (replacing\nthem with the previous effective sample).\n  \nIf the ratios are low, tightening the priors may help.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3a.brm2$fit |> stan_ess()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ess3a-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- ratios are all very high\n\n:::::\n\n**Conclusions**:\n\n- all the diagnostics appear reasonable\n- we can conclude that the chains are all well mixed and have\n  converged on a stable posterior.\n\n#### Centered predictor\n\n::::: {.panel-tabset}\n##### Traceplots\n\nPlots of the estimates of each parameter over the post-warmup length\nof each MCMC chain. Each chain is plotted in a different colour, with\neach parameter in its own facet. Ideally, each **trace** should just\nlook like noise without any discernible drift and each of the traces\nfor a specific parameter should look the same (i.e, should not be\ndisplaced above or below any other trace for that parameter).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3b.brm2$fit |> stan_trace()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_traceplots3b-1.png){width=576}\n:::\n\n```{.r .cell-code}\ndat3b.brm2$fit |> stan_trace(inc_warmup = TRUE)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_traceplots3b-2.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n    - the chains appear well mixed and very similar\n\n##### Autocorrelation plots\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3b.brm2$fit |> stan_ac()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_acf3b-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there is no evidence of autocorrelation in the MCMC samples\n\n##### Rhat\n\nRhat is a **scale reduction factor** measure of convergence between\nthe chains. The closer the values are to 1, the more the chains have\nconverged. Values greater than 1.05 indicate a lack of convergence.\nThere will be an Rhat value for each parameter estimated.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3b.brm2$fit |> stan_rhat()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_rhat3b-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- all Rhat values are below 1.05, suggesting the chains have converged. \n\n##### Effective sample sizes\n\nThe number of effective samples - the ratio of the number of effective\nsamples (those not rejected by the sampler) to the number of samples\nprovides an indication of the effectiveness (and efficiency) of the\nMCMC sampler. Ratios that are less than 0.5 for a parameter suggest\nthat the sampler spent considerable time in difficult areas of the\nsampling domain and rejected more than half of the samples (replacing\nthem with the previous effective sample).\n  \nIf the ratios are low, tightening the priors may help.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3b.brm2$fit |> stan_ess()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ess3b-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- ratios are all very high\n\n:::::\n\n**Conclusions**:\n\n- all the diagnostics appear reasonable\n- we can conclude that the chains are all well mixed and have\n  converged on a stable posterior.\n\n#### Standardised predictor\n\n::::: {.panel-tabset}\n##### Traceplots\n\nPlots of the estimates of each parameter over the post-warmup length\nof each MCMC chain. Each chain is plotted in a different colour, with\neach parameter in its own facet. Ideally, each **trace** should just\nlook like noise without any discernible drift and each of the traces\nfor a specific parameter should look the same (i.e, should not be\ndisplaced above or below any other trace for that parameter).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3c.brm2$fit |> stan_trace()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_traceplots3c-1.png){width=576}\n:::\n\n```{.r .cell-code}\ndat3c.brm2$fit |> stan_trace(inc_warmup = TRUE)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_traceplots3c-2.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n    - the chains appear well mixed and very similar\n\n##### Autocorrelation plots\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3c.brm2$fit |> stan_ac()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_acf3c-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there is no evidence of autocorrelation in the MCMC samples\n\n##### Rhat\n\nRhat is a **scale reduction factor** measure of convergence between\nthe chains. The closer the values are to 1, the more the chains have\nconverged. Values greater than 1.05 indicate a lack of convergence.\nThere will be an Rhat value for each parameter estimated.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3c.brm2$fit |> stan_rhat()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_rhat3c-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- all Rhat values are below 1.05, suggesting the chains have converged. \n\n##### Effective sample sizes\n\nThe number of effective samples - the ratio of the number of effective\nsamples (those not rejected by the sampler) to the number of samples\nprovides an indication of the effectiveness (and efficiency) of the\nMCMC sampler. Ratios that are less than 0.5 for a parameter suggest\nthat the sampler spent considerable time in difficult areas of the\nsampling domain and rejected more than half of the samples (replacing\nthem with the previous effective sample).\n  \nIf the ratios are low, tightening the priors may help.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3c.brm2$fit |> stan_ess()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ess3c-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- ratios are all very high\n\n:::::\n\n**Conclusions**:\n\n- all the diagnostics appear reasonable\n- we can conclude that the chains are all well mixed and have\n  converged on a stable posterior.\n\n\n:::::::\n\n### Example 4 (NB data)\n\n::::::: {.panel-tabset}\n#### Raw predictor\n\n::::: {.panel-tabset}\n##### Traceplots\n\nPlots of the estimates of each parameter over the post-warmup length\nof each MCMC chain. Each chain is plotted in a different colour, with\neach parameter in its own facet. Ideally, each **trace** should just\nlook like noise without any discernible drift and each of the traces\nfor a specific parameter should look the same (i.e, should not be\ndisplaced above or below any other trace for that parameter).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4a.brm2$fit |> stan_trace()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_traceplots4a-1.png){width=576}\n:::\n\n```{.r .cell-code}\ndat4a.brm2$fit |> stan_trace(inc_warmup = TRUE)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_traceplots4a-2.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n    - the chains appear well mixed and very similar\n\n##### Autocorrelation plots\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4a.brm2$fit |> stan_ac()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_acf4a-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there is no evidence of autocorrelation in the MCMC samples\n\n##### Rhat\n\nRhat is a **scale reduction factor** measure of convergence between\nthe chains. The closer the values are to 1, the more the chains have\nconverged. Values greater than 1.05 indicate a lack of convergence.\nThere will be an Rhat value for each parameter estimated.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4a.brm2$fit |> stan_rhat()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_rhat4a-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- all Rhat values are below 1.05, suggesting the chains have converged. \n\n##### Effective sample sizes\n\nThe number of effective samples - the ratio of the number of effective\nsamples (those not rejected by the sampler) to the number of samples\nprovides an indication of the effectiveness (and efficiency) of the\nMCMC sampler. Ratios that are less than 0.5 for a parameter suggest\nthat the sampler spent considerable time in difficult areas of the\nsampling domain and rejected more than half of the samples (replacing\nthem with the previous effective sample).\n  \nIf the ratios are low, tightening the priors may help.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4a.brm2$fit |> stan_ess()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ess4a-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- ratios are all very high\n\n:::::\n\n**Conclusions**:\n\n- all the diagnostics appear reasonable\n- we can conclude that the chains are all well mixed and have\n  converged on a stable posterior.\n\n#### Centered predictor\n\n::::: {.panel-tabset}\n##### Traceplots\n\nPlots of the estimates of each parameter over the post-warmup length\nof each MCMC chain. Each chain is plotted in a different colour, with\neach parameter in its own facet. Ideally, each **trace** should just\nlook like noise without any discernible drift and each of the traces\nfor a specific parameter should look the same (i.e, should not be\ndisplaced above or below any other trace for that parameter).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4b.brm2$fit |> stan_trace()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_traceplots4b-1.png){width=576}\n:::\n\n```{.r .cell-code}\ndat4b.brm2$fit |> stan_trace(inc_warmup = TRUE)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_traceplots4b-2.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n    - the chains appear well mixed and very similar\n\n##### Autocorrelation plots\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4b.brm2$fit |> stan_ac()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_acf4b-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there is no evidence of autocorrelation in the MCMC samples\n\n##### Rhat\n\nRhat is a **scale reduction factor** measure of convergence between\nthe chains. The closer the values are to 1, the more the chains have\nconverged. Values greater than 1.05 indicate a lack of convergence.\nThere will be an Rhat value for each parameter estimated.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4b.brm2$fit |> stan_rhat()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_rhat4b-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- all Rhat values are below 1.05, suggesting the chains have converged. \n\n##### Effective sample sizes\n\nThe number of effective samples - the ratio of the number of effective\nsamples (those not rejected by the sampler) to the number of samples\nprovides an indication of the effectiveness (and efficiency) of the\nMCMC sampler. Ratios that are less than 0.5 for a parameter suggest\nthat the sampler spent considerable time in difficult areas of the\nsampling domain and rejected more than half of the samples (replacing\nthem with the previous effective sample).\n  \nIf the ratios are low, tightening the priors may help.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4b.brm2$fit |> stan_ess()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ess4b-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- ratios are all very high\n\n:::::\n\n**Conclusions**:\n\n- all the diagnostics appear reasonable\n- we can conclude that the chains are all well mixed and have\n  converged on a stable posterior.\n\n#### Standardised predictor\n\n::::: {.panel-tabset}\n##### Traceplots\n\nPlots of the estimates of each parameter over the post-warmup length\nof each MCMC chain. Each chain is plotted in a different colour, with\neach parameter in its own facet. Ideally, each **trace** should just\nlook like noise without any discernible drift and each of the traces\nfor a specific parameter should look the same (i.e, should not be\ndisplaced above or below any other trace for that parameter).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4c.brm2$fit |> stan_trace()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_traceplots4c-1.png){width=576}\n:::\n\n```{.r .cell-code}\ndat4c.brm2$fit |> stan_trace(inc_warmup = TRUE)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_traceplots4c-2.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n    - the chains appear well mixed and very similar\n\n##### Autocorrelation plots\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4c.brm2$fit |> stan_ac()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_acf4c-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there is no evidence of autocorrelation in the MCMC samples\n\n##### Rhat\n\nRhat is a **scale reduction factor** measure of convergence between\nthe chains. The closer the values are to 1, the more the chains have\nconverged. Values greater than 1.05 indicate a lack of convergence.\nThere will be an Rhat value for each parameter estimated.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4c.brm2$fit |> stan_rhat()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_rhat4c-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- all Rhat values are below 1.05, suggesting the chains have converged. \n\n##### Effective sample sizes\n\nThe number of effective samples - the ratio of the number of effective\nsamples (those not rejected by the sampler) to the number of samples\nprovides an indication of the effectiveness (and efficiency) of the\nMCMC sampler. Ratios that are less than 0.5 for a parameter suggest\nthat the sampler spent considerable time in difficult areas of the\nsampling domain and rejected more than half of the samples (replacing\nthem with the previous effective sample).\n  \nIf the ratios are low, tightening the priors may help.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4c.brm2$fit |> stan_ess()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ess4c-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- ratios are all very high\n\n:::::\n\n**Conclusions**:\n\n- all the diagnostics appear reasonable\n- we can conclude that the chains are all well mixed and have\n  converged on a stable posterior.\n\n\n:::::::\n\n### Example 5 (Binary data)\n\n::::::: {.panel-tabset}\n#### Raw predictor\n\n::::: {.panel-tabset}\n##### Traceplots\n\nPlots of the estimates of each parameter over the post-warmup length\nof each MCMC chain. Each chain is plotted in a different colour, with\neach parameter in its own facet. Ideally, each **trace** should just\nlook like noise without any discernible drift and each of the traces\nfor a specific parameter should look the same (i.e, should not be\ndisplaced above or below any other trace for that parameter).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5a.brm2$fit |> stan_trace()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_traceplots5a-1.png){width=576}\n:::\n\n```{.r .cell-code}\ndat5a.brm2$fit |> stan_trace(inc_warmup = TRUE)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_traceplots5a-2.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n    - the chains appear well mixed and very similar\n\n##### Autocorrelation plots\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5a.brm2$fit |> stan_ac()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_acf5a-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there is no evidence of autocorrelation in the MCMC samples\n\n##### Rhat\n\nRhat is a **scale reduction factor** measure of convergence between\nthe chains. The closer the values are to 1, the more the chains have\nconverged. Values greater than 1.05 indicate a lack of convergence.\nThere will be an Rhat value for each parameter estimated.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5a.brm2$fit |> stan_rhat()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_rhat5a-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- all Rhat values are below 1.05, suggesting the chains have converged. \n\n##### Effective sample sizes\n\nThe number of effective samples - the ratio of the number of effective\nsamples (those not rejected by the sampler) to the number of samples\nprovides an indication of the effectiveness (and efficiency) of the\nMCMC sampler. Ratios that are less than 0.5 for a parameter suggest\nthat the sampler spent considerable time in difficult areas of the\nsampling domain and rejected more than half of the samples (replacing\nthem with the previous effective sample).\n  \nIf the ratios are low, tightening the priors may help.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5a.brm2$fit |> stan_ess()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ess5a-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- ratios are all very high\n\n:::::\n\n**Conclusions**:\n\n- all the diagnostics appear reasonable\n- we can conclude that the chains are all well mixed and have\n  converged on a stable posterior.\n\n#### Centered predictor\n\n::::: {.panel-tabset}\n##### Traceplots\n\nPlots of the estimates of each parameter over the post-warmup length\nof each MCMC chain. Each chain is plotted in a different colour, with\neach parameter in its own facet. Ideally, each **trace** should just\nlook like noise without any discernible drift and each of the traces\nfor a specific parameter should look the same (i.e, should not be\ndisplaced above or below any other trace for that parameter).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5b.brm2$fit |> stan_trace()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_traceplots5b-1.png){width=576}\n:::\n\n```{.r .cell-code}\ndat5b.brm2$fit |> stan_trace(inc_warmup = TRUE)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_traceplots5b-2.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n    - the chains appear well mixed and very similar\n\n##### Autocorrelation plots\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5b.brm2$fit |> stan_ac()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_acf5b-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there is no evidence of autocorrelation in the MCMC samples\n\n##### Rhat\n\nRhat is a **scale reduction factor** measure of convergence between\nthe chains. The closer the values are to 1, the more the chains have\nconverged. Values greater than 1.05 indicate a lack of convergence.\nThere will be an Rhat value for each parameter estimated.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5b.brm2$fit |> stan_rhat()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_rhat5b-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- all Rhat values are below 1.05, suggesting the chains have converged. \n\n##### Effective sample sizes\n\nThe number of effective samples - the ratio of the number of effective\nsamples (those not rejected by the sampler) to the number of samples\nprovides an indication of the effectiveness (and efficiency) of the\nMCMC sampler. Ratios that are less than 0.5 for a parameter suggest\nthat the sampler spent considerable time in difficult areas of the\nsampling domain and rejected more than half of the samples (replacing\nthem with the previous effective sample).\n  \nIf the ratios are low, tightening the priors may help.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5b.brm2$fit |> stan_ess()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ess5b-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- ratios are all very high\n\n:::::\n\n**Conclusions**:\n\n- all the diagnostics appear reasonable\n- we can conclude that the chains are all well mixed and have\n  converged on a stable posterior.\n\n#### Standardised predictor\n\n::::: {.panel-tabset}\n##### Traceplots\n\nPlots of the estimates of each parameter over the post-warmup length\nof each MCMC chain. Each chain is plotted in a different colour, with\neach parameter in its own facet. Ideally, each **trace** should just\nlook like noise without any discernible drift and each of the traces\nfor a specific parameter should look the same (i.e, should not be\ndisplaced above or below any other trace for that parameter).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5c.brm2$fit |> stan_trace()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_traceplots5c-1.png){width=576}\n:::\n\n```{.r .cell-code}\ndat5c.brm2$fit |> stan_trace(inc_warmup = TRUE)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_traceplots5c-2.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n    - the chains appear well mixed and very similar\n\n##### Autocorrelation plots\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5c.brm2$fit |> stan_ac()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_acf5c-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there is no evidence of autocorrelation in the MCMC samples\n\n##### Rhat\n\nRhat is a **scale reduction factor** measure of convergence between\nthe chains. The closer the values are to 1, the more the chains have\nconverged. Values greater than 1.05 indicate a lack of convergence.\nThere will be an Rhat value for each parameter estimated.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5c.brm2$fit |> stan_rhat()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_rhat5c-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- all Rhat values are below 1.05, suggesting the chains have converged. \n\n##### Effective sample sizes\n\nThe number of effective samples - the ratio of the number of effective\nsamples (those not rejected by the sampler) to the number of samples\nprovides an indication of the effectiveness (and efficiency) of the\nMCMC sampler. Ratios that are less than 0.5 for a parameter suggest\nthat the sampler spent considerable time in difficult areas of the\nsampling domain and rejected more than half of the samples (replacing\nthem with the previous effective sample).\n  \nIf the ratios are low, tightening the priors may help.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5c.brm2$fit |> stan_ess()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ess5c-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- ratios are all very high\n\n:::::\n\n**Conclusions**:\n\n- all the diagnostics appear reasonable\n- we can conclude that the chains are all well mixed and have\n  converged on a stable posterior.\n\n\n:::::::\n\n### Example 6 (Binomial data)\n\n::::::: {.panel-tabset}\n#### Raw predictor\n\n::::: {.panel-tabset}\n##### Traceplots\n\nPlots of the estimates of each parameter over the post-warmup length\nof each MCMC chain. Each chain is plotted in a different colour, with\neach parameter in its own facet. Ideally, each **trace** should just\nlook like noise without any discernible drift and each of the traces\nfor a specific parameter should look the same (i.e, should not be\ndisplaced above or below any other trace for that parameter).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6a.brm2$fit |> stan_trace()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_traceplots6a-1.png){width=576}\n:::\n\n```{.r .cell-code}\ndat6a.brm2$fit |> stan_trace(inc_warmup = TRUE)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_traceplots6a-2.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n    - the chains appear well mixed and very similar\n\n##### Autocorrelation plots\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6a.brm2$fit |> stan_ac()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_acf6a-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there is no evidence of autocorrelation in the MCMC samples\n\n##### Rhat\n\nRhat is a **scale reduction factor** measure of convergence between\nthe chains. The closer the values are to 1, the more the chains have\nconverged. Values greater than 1.05 indicate a lack of convergence.\nThere will be an Rhat value for each parameter estimated.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6a.brm2$fit |> stan_rhat()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_rhat6a-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- all Rhat values are below 1.05, suggesting the chains have converged. \n\n##### Effective sample sizes\n\nThe number of effective samples - the ratio of the number of effective\nsamples (those not rejected by the sampler) to the number of samples\nprovides an indication of the effectiveness (and efficiency) of the\nMCMC sampler. Ratios that are less than 0.5 for a parameter suggest\nthat the sampler spent considerable time in difficult areas of the\nsampling domain and rejected more than half of the samples (replacing\nthem with the previous effective sample).\n  \nIf the ratios are low, tightening the priors may help.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6a.brm2$fit |> stan_ess()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ess6a-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- ratios are all very high\n\n:::::\n\n**Conclusions**:\n\n- all the diagnostics appear reasonable\n- we can conclude that the chains are all well mixed and have\n  converged on a stable posterior.\n\n#### Centered predictor\n\n::::: {.panel-tabset}\n##### Traceplots\n\nPlots of the estimates of each parameter over the post-warmup length\nof each MCMC chain. Each chain is plotted in a different colour, with\neach parameter in its own facet. Ideally, each **trace** should just\nlook like noise without any discernible drift and each of the traces\nfor a specific parameter should look the same (i.e, should not be\ndisplaced above or below any other trace for that parameter).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6b.brm2$fit |> stan_trace()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_traceplots6b-1.png){width=576}\n:::\n\n```{.r .cell-code}\ndat6b.brm2$fit |> stan_trace(inc_warmup = TRUE)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_traceplots6b-2.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n    - the chains appear well mixed and very similar\n\n##### Autocorrelation plots\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6b.brm2$fit |> stan_ac()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_acf6b-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there is no evidence of autocorrelation in the MCMC samples\n\n##### Rhat\n\nRhat is a **scale reduction factor** measure of convergence between\nthe chains. The closer the values are to 1, the more the chains have\nconverged. Values greater than 1.05 indicate a lack of convergence.\nThere will be an Rhat value for each parameter estimated.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6b.brm2$fit |> stan_rhat()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_rhat6b-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- all Rhat values are below 1.05, suggesting the chains have converged. \n\n##### Effective sample sizes\n\nThe number of effective samples - the ratio of the number of effective\nsamples (those not rejected by the sampler) to the number of samples\nprovides an indication of the effectiveness (and efficiency) of the\nMCMC sampler. Ratios that are less than 0.5 for a parameter suggest\nthat the sampler spent considerable time in difficult areas of the\nsampling domain and rejected more than half of the samples (replacing\nthem with the previous effective sample).\n  \nIf the ratios are low, tightening the priors may help.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6b.brm2$fit |> stan_ess()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ess6b-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- ratios are all very high\n\n:::::\n\n**Conclusions**:\n\n- all the diagnostics appear reasonable\n- we can conclude that the chains are all well mixed and have\n  converged on a stable posterior.\n\n#### Standardised predictor\n\n::::: {.panel-tabset}\n##### Traceplots\n\nPlots of the estimates of each parameter over the post-warmup length\nof each MCMC chain. Each chain is plotted in a different colour, with\neach parameter in its own facet. Ideally, each **trace** should just\nlook like noise without any discernible drift and each of the traces\nfor a specific parameter should look the same (i.e, should not be\ndisplaced above or below any other trace for that parameter).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6c.brm2$fit |> stan_trace()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_traceplots6c-1.png){width=576}\n:::\n\n```{.r .cell-code}\ndat6c.brm2$fit |> stan_trace(inc_warmup = TRUE)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_traceplots6c-2.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n    - the chains appear well mixed and very similar\n\n##### Autocorrelation plots\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6c.brm2$fit |> stan_ac()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_acf6c-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there is no evidence of autocorrelation in the MCMC samples\n\n##### Rhat\n\nRhat is a **scale reduction factor** measure of convergence between\nthe chains. The closer the values are to 1, the more the chains have\nconverged. Values greater than 1.05 indicate a lack of convergence.\nThere will be an Rhat value for each parameter estimated.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6c.brm2$fit |> stan_rhat()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_rhat6c-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- all Rhat values are below 1.05, suggesting the chains have converged. \n\n##### Effective sample sizes\n\nThe number of effective samples - the ratio of the number of effective\nsamples (those not rejected by the sampler) to the number of samples\nprovides an indication of the effectiveness (and efficiency) of the\nMCMC sampler. Ratios that are less than 0.5 for a parameter suggest\nthat the sampler spent considerable time in difficult areas of the\nsampling domain and rejected more than half of the samples (replacing\nthem with the previous effective sample).\n  \nIf the ratios are low, tightening the priors may help.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6c.brm2$fit |> stan_ess()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ess6c-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- ratios are all very high\n\n:::::\n\n**Conclusions**:\n\n- all the diagnostics appear reasonable\n- we can conclude that the chains are all well mixed and have\n  converged on a stable posterior.\n\n\n:::::::\n\n::::\n\n# Model validation\n\nModel validation involves exploring the model diagnostics and fit to\nensure that the model is broadly appropriate for the data. As such,\nexploration of the residuals should be routine.\n\nFor more complex models (those that contain multiple effects, it is\nalso advisable to plot the residuals against each of the individual\npredictors. For sampling designs that involve sample collection over\nspace or time, it is also a good idea to explore whether there are any\ntemporal or spatial patterns in the residuals.\n\nThere are numerous situations (e.g. when applying specific\nvariance-covariance structures to a model) where raw residuals do not\nreflect the interior workings of the model. Typically, this is because\nthey do not take into account the variance-covariance matrix or assume\na very simple variance-covariance matrix. Since the purpose of\nexploring residuals is to evaluate the model, for these cases, it is\narguably better to draw conclusions based on standardized (or\nstudentized) residuals.\n\nUnfortunately the definitions of standardised and studentised\nresiduals appears to vary and the two terms get used interchangeably.\nI will adopt the following definitions:\n\nStandardized residuals\n: the raw residuals divided by the **true** standard deviation of the\n  residuals (which of course is rarely known).\n\nStudentized residuals\n: the raw residuals divided by the standard deviation of the\n  residuals. Note that **externally studentised** residuals are\n  calculated by dividing the raw residuals by a unique standard\n  deviation for each observation that is calculated from regressions\n  having left each successive observation out.\n\nPearson residuals\n: the raw residuals divided by the standard deviation of the response\n  variable.\n\nThe mark of a good model is being able to predict well. In an ideal\nworld, we would have sufficiently large sample size as to permit us to\nhold a fraction (such as 25%) back thereby allowing us to train the\nmodel on 75% of the data and then see how well the model can predict\nthe withheld 25%. Unfortunately, such a luxury is still rare in\necology.\n\nThe next best option is to see how well the model can predict the\nobserved data. Models tend to struggle most with the extremes of\ntrends and have particular issues when the extremes approach logical\nboundaries (such as zero for count data and standard deviations). We\ncan use the fitted model to generate random predicted observations and\nthen explore some properties of these compared to the actual observed\ndata.\n\n\n| Package   | Description       | function                     | rstanarm                                               | brms                                               |\n|-----------|-------------------|------------------------------|-------------------------------------------------------|----------------------------------------------------|\n| bayesplot | Density overlay   | `ppc_dens_overlay`           | `pp_check(mod, plotfun='dens_overlay')`               | `pp_check(mod, type='dens_overlay')`               |\n|           | Obs vs Pred error | `ppc_error_scatter_avg`      | `pp_check(mod, plotfun='error_scatter_avg')`          | `pp_check(mod, type='error_scatter_avg')`          |\n|           | Pred error vs x   | `ppc_error_scatter_avg_vs_x` | `pp_check(mod, x=, plotfun='error_scatter_avg_vs_x')` | `pp_check(mod, x=, type='error_scatter_avg_vs_x')` |\n|           | Preds vs x        | `ppc_intervals`              | `pp_check(mod, x=, plotfun='intervals')`              | `pp_check(mod, x=, type='intervals')`              |\n|           | Partial plot       | `ppc_ribbon`                 | `pp_check(mod, x=, plotfun='ribbon')`                 | `pp_check(mod, x=, type='ribbon')`                 |\n|           |                   |                              |                                                       |                                                    |\n\n: {.primary .bordered .sm .paramsTable tbl-colwidths=\"[10,10, 20, 30, 30]\"}\n\n\n:::: {.callout-tip collapse=\"true\"}\n\n## More PPC modules (functions) available\n\n\n\n::: {.cell mhidden='true'}\n\n```{.r .cell-code}\navailable_ppc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nbayesplot PPC module:\n  ppc_bars\n  ppc_bars_grouped\n  ppc_boxplot\n  ppc_dens\n  ppc_dens_overlay\n  ppc_dens_overlay_grouped\n  ppc_ecdf_overlay\n  ppc_ecdf_overlay_grouped\n  ppc_error_binned\n  ppc_error_hist\n  ppc_error_hist_grouped\n  ppc_error_scatter\n  ppc_error_scatter_avg\n  ppc_error_scatter_avg_grouped\n  ppc_error_scatter_avg_vs_x\n  ppc_freqpoly\n  ppc_freqpoly_grouped\n  ppc_hist\n  ppc_intervals\n  ppc_intervals_grouped\n  ppc_km_overlay\n  ppc_km_overlay_grouped\n  ppc_loo_intervals\n  ppc_loo_pit\n  ppc_loo_pit_overlay\n  ppc_loo_pit_qq\n  ppc_loo_ribbon\n  ppc_pit_ecdf\n  ppc_pit_ecdf_grouped\n  ppc_ribbon\n  ppc_ribbon_grouped\n  ppc_rootogram\n  ppc_scatter\n  ppc_scatter_avg\n  ppc_scatter_avg_grouped\n  ppc_stat\n  ppc_stat_2d\n  ppc_stat_freqpoly\n  ppc_stat_freqpoly_grouped\n  ppc_stat_grouped\n  ppc_violin_grouped\n```\n\n\n:::\n:::\n\n\n\n::::\n\n\n:::: {.panel-tabset}\n\n### Example 1 (Gaussian data)\n\n::: {.callout-note}\n\nBayesian samplers involve many calls to randomisation functions. As a\nresult, the estimates will vary slightly each time the routines are\nrun. You should expect that the outputs that you obtain will differ\nslightly from those that I am displaying. Nevertheless, the main\nconclusions should remain robust across subsequent runs.\n\n:::\n\n::::::: {.panel-tabset}\n#### Raw predictor\n::::: {.panel-tabset}\n\n##### Simple residual plots\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid <- resid(dat1a.brm2)[, \"Estimate\"]\nfit <- fitted(dat1a.brm2)[, \"Estimate\"]\nggplot() +\n  geom_point(data = NULL, aes(y = resid, x = fit))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_resid1a-1.png){width=384}\n:::\n:::\n\n\n\nWe should also plot the residuals against each of the predictor\nvariables (as well as any other important unmodelled predictors -\nparticularly time and space if relevant and available).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid <- resid(dat1a.brm2)[, \"Estimate\"]\nggplot() +\n  geom_point(data = NULL, aes(y = resid, x = dat$x))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_resid21a-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there does not appear to be any pattern in the residuals\n\n##### Posterior probability checks\n\nPost predictive checks provide additional diagnostics about the fit of\nthe model. Specifically, they provide a comparison between predictions\ndrawn from the model and the observed data used to train the model.\n\n\n**Density overlay**\n\nThese are plots of the density distribution of the observed data\n(black line) overlayed on top of 50 density distributions generated\nfrom draws from the model (light blue). Ideally, the 50 realisations\nshould be roughly consistent with the observed data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1a.brm2 |> pp_check(type = 'dens_overlay', ndraws = 100)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ppdensity1a-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the model draws appear to be consistent with the observed data\n\n**Error scatter**\n\nThese are plots of the observed values against the average residuals.\nSimilar to a residual plot, we do not want to see any patterns in this\nplot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1a.brm2 |> pp_check(type = 'error_scatter_avg')\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'error_scatter_avg' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_pperror1a-1.png){width=288}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there is no obvious pattern in the residuals.\n\n**Intervals**\n\nThese are plots of the observed data overlayed on top of posterior\npredictions associated with each level of the predictor. Ideally, the\nobserved data should all fall within the predictive intervals.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1a.brm2 |> pp_check(type = 'intervals', x = \"x\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'intervals' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ppintervals1a-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the posterior predictions are not inconsistent with the observed\n  data\n\n**Ribbon**\n\nThese are just an alternative way of expressing the interval plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1a.brm2 |> pp_check(type = 'ribbon', x = \"x\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'ribbon' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ppribbon1a-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the posterior predictions are not inconsistent with the observed\n  data\n\n:::: {.callout-tip collapse=\"true\"}\n\n####### Using `shiny` to explore a wide range of MCMC and validation diagnostics\n\nThe `shinystan` package allows the full suite of MCMC diagnostics and posterior\npredictive checks to be accessed via a web interface.\n\n\n\n::: {.cell mhidden='true'}\n\n```{.r .cell-code}\nlibrary(shinystan)\nlaunch_shinystan(dat1a.brm2)\n```\n:::\n\n\n\n::::\n\n##### DHARMa (simulated) residuals\n\nDHARMa residuals provide very useful diagnostics. Unfortunately, we\ncannot directly use the `simulateResiduals()` function to generate the\nsimulated residuals. However, if we are willing to calculate some of\nthe components yourself, we can still obtain the simulated residuals\nfrom the fitted stan model.\n\nWe need to supply:\n\n- simulated (predicted) responses associated with each observation.\n- observed values\n- fitted (predicted) responses (averaged) associated with each\n  observation\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1a.resids <- make_brms_dharma_res(dat1a.brm2, integerResponse = FALSE)\nwrap_elements(~ testUniformity(dat1a.resids)) +\n    wrap_elements(~ plotResiduals(dat1a.resids)) +\n    wrap_elements(~ testDispersion(dat1a.resids)) +\n    plot_layout(nrow = 1)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_DHARMa1a-1.png){width=8in}\n:::\n:::\n\n\n\n:::: {.callout-note}\n\nIf you are using Rstudio (particularly with a quarto document), the above code may produce a graphic that is too large for the currently available device.  The document should still render without problems, it is just the live display within Rstudio that cannot accommodate the figure.\n\nTo address this, you can either:\n\n- break the multi-panel figure up into four separate figures by\n  removing the outer `wrap_elements()` and `plot_layout()` functions.\n- copy the above code into the console and view in a larger graphics\n  device\n::::\n\n**Conclusions**:\n\n- the Q-Q plot looks reasonable (points broadly follow the angled\n  line)\n- there are no flagged issues with the\n  - KS test: conformity to the nominated distribution (family)\n  - Dispersion test: would not normally expect this to be an issue for\n    a Gaussian family unless there are other issues with the residuals\n  - Outlier test: the influence of each observation\n- there does not appear to be any patterns in the residuals - each of\n  the three quantile trends are considered flat and centered around\n  1/3, 1/2 and 2/3\n- the observed dispersion is well within the simulated range\n  indicating that there is no issue with dispersion (again this is\n  expected for a Gaussian model)\n\n:::::\n\n**Conclusions**:\n- there is no evidence of a lack of fit\n- the model is likely to be reliable\n\n#### Centered predictor\n\n::::: {.panel-tabset}\n\n##### Simple residual plots\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid <- resid(dat1b.brm2)[, \"Estimate\"]\nfit <- fitted(dat1b.brm2)[, \"Estimate\"]\nggplot() +\n  geom_point(data = NULL, aes(y = resid, x = fit))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_resid1b-1.png){width=384}\n:::\n:::\n\n\n\nWe should also plot the residuals against each of the predictor\nvariables (as well as any other important unmodelled predictors -\nparticularly time and space if relevant and available).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid <- resid(dat1b.brm2)[, \"Estimate\"]\nggplot() +\n  geom_point(data = NULL, aes(y = resid, x = dat$x))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_resid21b-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there does not appear to be any pattern in the residuals\n\n##### Posterior probability checks\n\nPost predictive checks provide additional diagnostics about the fit of\nthe model. Specifically, they provide a comparison between predictions\ndrawn from the model and the observed data used to train the model.\n\n\n**Density overlay**\n\nThese are plots of the density distribution of the observed data\n(black line) overlayed on top of 50 density distributions generated\nfrom draws from the model (light blue). Ideally, the 50 realisations\nshould be roughly consistent with the observed data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1b.brm2 |> pp_check(type = 'dens_overlay', ndraws = 100)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ppdensity1b-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the model draws appear to be consistent with the observed data\n\n**Error scatter**\n\nThese are plots of the observed values against the average residuals.\nSimilar to a residual plot, we do not want to see any patterns in this\nplot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1b.brm2 |> pp_check(type = 'error_scatter_avg')\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'error_scatter_avg' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_pperror1b-1.png){width=288}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there is no obvious pattern in the residuals.\n\n**Intervals**\n\nThese are plots of the observed data overlayed on top of posterior\npredictions associated with each level of the predictor. Ideally, the\nobserved data should all fall within the predictive intervals.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1b.brm2 |> pp_check(type = 'intervals', x = \"x\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'intervals' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ppintervals1b-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the posterior predictions are not inconsistent with the observed\n  data\n\n**Ribbon**\n\nThese are just an alternative way of expressing the interval plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1b.brm2 |> pp_check(type = 'ribbon', x = \"x\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'ribbon' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ppribbon1b-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the posterior predictions are not inconsistent with the observed\n  data\n\n:::: {.callout-tip collapse=\"true\"}\n\n###### Using `shiny` to explore a wide range of MCMC and validation diagnostics\n\nThe `shinystan` package allows the full suite of MCMC diagnostics and posterior\npredictive checks to be accessed via a web interface.\n\n\n\n::: {.cell mhidden='true'}\n\n```{.r .cell-code}\nlibrary(shinystan)\nlaunch_shinystan(dat1b.brm2)\n```\n:::\n\n\n\n::::\n\n##### DHARMa (simulated) residuals\n\nDHARMa residuals provide very useful diagnostics. Unfortunately, we\ncannot directly use the `simulateResiduals()` function to generate the\nsimulated residuals. However, if we are willing to calculate some of\nthe components yourself, we can still obtain the simulated residuals\nfrom the fitted stan model.\n\nWe need to supply:\n\n- simulated (predicted) responses associated with each observation.\n- observed values\n- fitted (predicted) responses (averaged) associated with each\n  observation\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1b.resids <- make_brms_dharma_res(dat1b.brm2, integerResponse = FALSE)\nwrap_elements(~ testUniformity(dat1b.resids)) +\n    wrap_elements(~ plotResiduals(dat1b.resids)) +\n    wrap_elements(~ testDispersion(dat1b.resids)) +\n    plot_layout(nrow = 1)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_DHARMa1b-1.png){width=8in}\n:::\n:::\n\n\n\n:::: {.callout-note}\n\nIf you are using Rstudio (particularly with a quarto document), the above code may produce a graphic that is too large for the currently available device.  The document should still render without problems, it is just the live display within Rstudio that cannot accommodate the figure.\n\nTo address this, you can either:\n\n- break the multi-panel figure up into four separate figures by\n  removing the outer `wrap_elements()` and `plot_layout()` functions.\n- copy the above code into the console and view in a larger graphics\n  device\n::::\n\n**Conclusions**:\n\n- the Q-Q plot looks reasonable (points broadly follow the angled\n  line)\n- there are no flagged issues with the\n  - KS test: conformity to the nominated distribution (family)\n  - Dispersion test: would not normally expect this to be an issue for\n    a Gaussian family unless there are other issues with the residuals\n  - Outlier test: the influence of each observation\n- there does not appear to be any patterns in the residuals - each of\n  the three quantile trends are considered flat and centered around\n  1/3, 1/2 and 2/3\n- the observed dispersion is well within the simulated range\n  indicating that there is no issue with dispersion (again this is\n  expected for a Gaussian model)\n\n:::::\n\n**Conclusions**:\n- there is no evidence of a lack of fit\n- the model is likely to be reliable\n\n#### Standardised predictor\n\n::::: {.panel-tabset}\n\n##### Simple residual plots\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid <- resid(dat1c.brm2)[, \"Estimate\"]\nfit <- fitted(dat1c.brm2)[, \"Estimate\"]\nggplot() +\n  geom_point(data = NULL, aes(y = resid, x = fit))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_resid1c-1.png){width=384}\n:::\n:::\n\n\n\nWe should also plot the residuals against each of the predictor\nvariables (as well as any other important unmodelled predictors -\nparticularly time and space if relevant and available).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid <- resid(dat1c.brm2)[, \"Estimate\"]\nggplot() +\n  geom_point(data = NULL, aes(y = resid, x = dat$x))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_resid21c-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there does not appear to be any pattern in the residuals\n\n##### Posterior probability checks\n\nPost predictive checks provide additional diagnostics about the fit of\nthe model. Specifically, they provide a comparison between predictions\ndrawn from the model and the observed data used to train the model.\n\n\n**Density overlay**\n\nThese are plots of the density distribution of the observed data\n(black line) overlayed on top of 50 density distributions generated\nfrom draws from the model (light blue). Ideally, the 50 realisations\nshould be roughly consistent with the observed data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1c.brm2 |> pp_check(type = 'dens_overlay', ndraws = 100)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ppdensity1c-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the model draws appear to be consistent with the observed data\n\n**Error scatter**\n\nThese are plots of the observed values against the average residuals.\nSimilar to a residual plot, we do not want to see any patterns in this\nplot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1c.brm2 |> pp_check(type = 'error_scatter_avg')\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'error_scatter_avg' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_pperror1c-1.png){width=288}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there is no obvious pattern in the residuals.\n\n**Intervals**\n\nThese are plots of the observed data overlayed on top of posterior\npredictions associated with each level of the predictor. Ideally, the\nobserved data should all fall within the predictive intervals.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1c.brm2 |> pp_check(type = 'intervals', x = \"x\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'intervals' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ppintervals1c-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the posterior predictions are not inconsistent with the observed\n  data\n\n**Ribbon**\n\nThese are just an alternative way of expressing the interval plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1c.brm2 |> pp_check(type = 'ribbon', x = \"x\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'ribbon' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ppribbon1c-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the posterior predictions are not inconsistent with the observed\n  data\n\n:::: {.callout-tip collapse=\"true\"}\n\n###### Using `shiny` to explore a wide range of MCMC and validation diagnostics\n\nThe `shinystan` package allows the full suite of MCMC diagnostics and posterior\npredictive checks to be accessed via a web interface.\n\n\n\n::: {.cell mhidden='true'}\n\n```{.r .cell-code}\nlibrary(shinystan)\nlaunch_shinystan(dat1c.brm2)\n```\n:::\n\n\n\n::::\n\n##### DHARMa (simulated) residuals\n\nDHARMa residuals provide very useful diagnostics. Unfortunately, we\ncannot directly use the `simulateResiduals()` function to generate the\nsimulated residuals. However, if we are willing to calculate some of\nthe components yourself, we can still obtain the simulated residuals\nfrom the fitted stan model.\n\nWe need to supply:\n\n- simulated (predicted) responses associated with each observation.\n- observed values\n- fitted (predicted) responses (averaged) associated with each\n  observation\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1c.resids <- make_brms_dharma_res(dat1c.brm2, integerResponse = FALSE)\nwrap_elements(~ testUniformity(dat1c.resids)) +\n    wrap_elements(~ plotResiduals(dat1c.resids)) +\n    wrap_elements(~ testDispersion(dat1c.resids)) +\n    plot_layout(nrow = 1)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_DHARMa1c-1.png){width=8in}\n:::\n:::\n\n\n\n:::: {.callout-note}\n\nIf you are using Rstudio (particularly with a quarto document), the above code may produce a graphic that is too large for the currently available device.  The document should still render without problems, it is just the live display within Rstudio that cannot accommodate the figure.\n\nTo address this, you can either:\n\n- break the multi-panel figure up into four separate figures by\n  removing the outer `wrap_elements()` and `plot_layout()` functions.\n- copy the above code into the console and view in a larger graphics\n  device\n::::\n\n**Conclusions**:\n\n- the Q-Q plot looks reasonable (points broadly follow the angled\n  line)\n- there are no flagged issues with the\n  - KS test: conformity to the nominated distribution (family)\n  - Dispersion test: would not normally expect this to be an issue for\n    a Gaussian family unless there are other issues with the residuals\n  - Outlier test: the influence of each observation\n- there does not appear to be any patterns in the residuals - each of\n  the three quantile trends are considered flat and centered around\n  1/3, 1/2 and 2/3\n- the observed dispersion is well within the simulated range\n  indicating that there is no issue with dispersion (again this is\n  expected for a Gaussian model)\n\n:::::\n\n**Conclusions**:\n- there is no evidence of a lack of fit\n- the model is likely to be reliable\n\n:::::::\n\n### Example 2 (categorical predictor)\n\n::::::: {.panel-tabset}\n\n#### Treatment contrasts\n\n::::: {.panel-tabset}\n\n##### Simple residual plots\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid <- resid(dat2a.brm2)[, \"Estimate\"]\nfit <- fitted(dat2a.brm2)[, \"Estimate\"]\nggplot() +\n  geom_point(data = NULL, aes(y = resid, x = fit))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_resid2a-1.png){width=384}\n:::\n:::\n\n\n\nWe should also plot the residuals against each of the predictor\nvariables (as well as any other important unmodelled predictors -\nparticularly time and space if relevant and available).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid <- resid(dat2a.brm2)[, \"Estimate\"]\nggplot() +\n  geom_point(data = NULL, aes(y = resid, x = dat2$x))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_resid22a-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there does not appear to be any pattern in the residuals\n\n##### Posterior probability checks\n\nPost predictive checks provide additional diagnostics about the fit of\nthe model. Specifically, they provide a comparison between predictions\ndrawn from the model and the observed data used to train the model.\n\n\n**Density overlay**\n\nThese are plots of the density distribution of the observed data\n(black line) overlayed on top of 50 density distributions generated\nfrom draws from the model (light blue). Ideally, the 50 realisations\nshould be roughly consistent with the observed data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2a.brm2 |> pp_check(type = 'dens_overlay', ndraws = 100)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ppdensity2a-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the model draws appear to be consistent with the observed data\n\n**Error scatter**\n\nThese are plots of the observed values against the average residuals.\nSimilar to a residual plot, we do not want to see any patterns in this\nplot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2a.brm2 |> pp_check(type = 'error_scatter_avg')\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'error_scatter_avg' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_pperror2a-1.png){width=288}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there is no obvious pattern in the residuals.\n\n\n:::: {.callout-tip collapse=\"true\"}\n\n##### Using `shiny` to explore a wide range of MCMC and validation diagnostics\n\nThe `shinystan` package allows the full suite of MCMC diagnostics and posterior\npredictive checks to be accessed via a web interface.\n\n\n\n::: {.cell mhidden='true'}\n\n```{.r .cell-code}\nlibrary(shinystan)\nlaunch_shinystan(dat2a.brm2)\n```\n:::\n\n\n\n::::\n\n##### DHARMa (simulated) residuals\n\nDHARMa residuals provide very useful diagnostics. Unfortunately, we\ncannot directly use the `simulateResiduals()` function to generate the\nsimulated residuals. However, if we are willing to calculate some of\nthe components yourself, we can still obtain the simulated residuals\nfrom the fitted stan model.\n\nWe need to supply:\n\n- simulated (predicted) responses associated with each observation.\n- observed values\n- fitted (predicted) responses (averaged) associated with each\n  observation\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2a.resids <- make_brms_dharma_res(dat2a.brm2, integerResponse = FALSE)\nwrap_elements(~ testUniformity(dat2a.resids)) +\n    wrap_elements(~ plotResiduals(dat2a.resids)) +\n    wrap_elements(~ testDispersion(dat2a.resids)) +\n    plot_layout(nrow = 1)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_DHARMa2a-1.png){width=8in}\n:::\n:::\n\n\n\n:::: {.callout-note}\n\nIf you are using Rstudio (particularly with a quarto document), the above code may produce a graphic that is too large for the currently available device.  The document should still render without problems, it is just the live display within Rstudio that cannot accommodate the figure.\n\nTo address this, you can either:\n\n- break the multi-panel figure up into four separate figures by\n  removing the outer `wrap_elements()` and `plot_layout()` functions.\n- copy the above code into the console and view in a larger graphics\n  device\n::::\n\n**Conclusions**:\n\n- the Q-Q plot looks reasonable (points broadly follow the angled\n  line)\n- there are no flagged issues with the\n  - KS test: conformity to the nominated distribution (family)\n  - Dispersion test: would not normally expect this to be an issue for\n    a Gaussian family unless there are other issues with the residuals\n  - Outlier test: the influence of each observation\n- there does not appear to be any patterns in the residuals - each of\n  the three quantile trends are considered flat and centered around\n  1/3, 1/2 and 2/3\n- the observed dispersion is well within the simulated range\n  indicating that there is no issue with dispersion (again this is\n  expected for a Gaussian model)\n\n:::::\n\n**Conclusions**:\n- there is no evidence of a lack of fit\n- the model is likely to be reliable\n\n#### Means parameterisation\n\n::::: {.panel-tabset}\n\n##### Simple residual plots\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid <- resid(dat2b.brm2)[, \"Estimate\"]\nfit <- fitted(dat2b.brm2)[, \"Estimate\"]\nggplot() +\n  geom_point(data = NULL, aes(y = resid, x = fit))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_resid2b-1.png){width=384}\n:::\n:::\n\n\n\nWe should also plot the residuals against each of the predictor\nvariables (as well as any other important unmodelled predictors -\nparticularly time and space if relevant and available).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid <- resid(dat2b.brm2)[, \"Estimate\"]\nggplot() +\n  geom_point(data = NULL, aes(y = resid, x = dat2$x))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_resid22b-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there does not appear to be any pattern in the residuals\n\n##### Posterior probability checks\n\nPost predictive checks provide additional diagnostics about the fit of\nthe model. Specifically, they provide a comparison between predictions\ndrawn from the model and the observed data used to train the model.\n\n\n**Density overlay**\n\nThese are plots of the density distribution of the observed data\n(black line) overlayed on top of 50 density distributions generated\nfrom draws from the model (light blue). Ideally, the 50 realisations\nshould be roughly consistent with the observed data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2b.brm2 |> pp_check(type = 'dens_overlay', ndraws = 100)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ppdensity2b-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the model draws appear to be consistent with the observed data\n\n**Error scatter**\n\nThese are plots of the observed values against the average residuals.\nSimilar to a residual plot, we do not want to see any patterns in this\nplot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2b.brm2 |> pp_check(type = 'error_scatter_avg')\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'error_scatter_avg' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_pperror2b-1.png){width=288}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there is no obvious pattern in the residuals.\n\n\n:::: {.callout-tip collapse=\"true\"}\n\n##### Using `shiny` to explore a wide range of MCMC and validation diagnostics\n\nThe `shinystan` package allows the full suite of MCMC diagnostics and posterior\npredictive checks to be accessed via a web interface.\n\n\n\n::: {.cell mhidden='true'}\n\n```{.r .cell-code}\nlibrary(shinystan)\nlaunch_shinystan(dat2b.brm2)\n```\n:::\n\n\n\n::::\n\n##### DHARMa (simulated) residuals\n\nDHARMa residuals provide very useful diagnostics. Unfortunately, we\ncannot directly use the `simulateResiduals()` function to generate the\nsimulated residuals. However, if we are willing to calculate some of\nthe components yourself, we can still obtain the simulated residuals\nfrom the fitted stan model.\n\nWe need to supply:\n\n- simulated (predicted) responses associated with each observation.\n- observed values\n- fitted (predicted) responses (averaged) associated with each\n  observation\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2b.resids <- make_brms_dharma_res(dat2b.brm2, integerResponse = FALSE)\nwrap_elements(~ testUniformity(dat2b.resids)) +\n    wrap_elements(~ plotResiduals(dat2b.resids)) +\n    wrap_elements(~ testDispersion(dat2b.resids)) +\n    plot_layout(nrow = 1)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_DHARMa2b-1.png){width=8in}\n:::\n:::\n\n\n\n:::: {.callout-note}\n\nIf you are using Rstudio (particularly with a quarto document), the above code may produce a graphic that is too large for the currently available device.  The document should still render without problems, it is just the live display within Rstudio that cannot accommodate the figure.\n\nTo address this, you can either:\n\n- break the multi-panel figure up into four separate figures by\n  removing the outer `wrap_elements()` and `plot_layout()` functions.\n- copy the above code into the console and view in a larger graphics\n  device\n::::\n\n**Conclusions**:\n\n- the Q-Q plot looks reasonable (points broadly follow the angled\n  line)\n- there are no flagged issues with the\n  - KS test: conformity to the nominated distribution (family)\n  - Dispersion test: would not normally expect this to be an issue for\n    a Gaussian family unless there are other issues with the residuals\n  - Outlier test: the influence of each observation\n- there does not appear to be any patterns in the residuals - each of\n  the three quantile trends are considered flat and centered around\n  1/3, 1/2 and 2/3\n- the observed dispersion is well within the simulated range\n  indicating that there is no issue with dispersion (again this is\n  expected for a Gaussian model)\n\n:::::\n\n**Conclusions**:\n- there is no evidence of a lack of fit\n- the model is likely to be reliable\n\n:::::::\n\n### Example 3 (Poisson data)\n\n::::::: {.panel-tabset}\n\n#### Raw predictor\n\n::::: {.panel-tabset}\n\n##### Simple residual plots\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid <- resid(dat3a.brm2)[, \"Estimate\"]\nfit <- fitted(dat3a.brm2)[, \"Estimate\"]\nggplot() +\n  geom_point(data = NULL, aes(y = resid, x = fit))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelvalidation_resid3a-1.png){width=384}\n:::\n:::\n\n\n\nWe should also plot the residuals against each of the predictor\nvariables (as well as any other important unmodelled predictors -\nparticularly time and space if relevant and available).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid <- resid(dat3a.brm2)[, \"Estimate\"]\nggplot() +\n  geom_point(data = NULL, aes(y = resid, x = dat3$x))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelvalidation_resid23a-1.png){width=384}\n:::\n:::\n\n\n\n**conclusions**:\n\n- there does not appear to be any pattern in the residuals\n\n##### Posterior Probability Checks\n\n**density overlay**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3a.brm2 |> pp_check(type = 'dens_overlay', ndraws = 100)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelvalidation_ppdensity3a-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the model draws appear to be consistent with the observed data\n\n**Error scatter**\n\nThese are plots of the observed values against the average residuals.\nSimilar to a residual plot, we do not want to see any patterns in this\nplot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3a.brm2 |> pp_check(type = 'error_scatter_avg')\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'error_scatter_avg' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_pperror3a-1.png){width=288}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there is no obvious pattern in the residuals.\n\n**Intervals**\n\nThese are plots of the observed data overlayed on top of posterior\npredictions associated with each level of the predictor. Ideally, the\nobserved data should all fall within the predictive intervals.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3a.brm2 |> pp_check(type = 'intervals', x = \"x\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'intervals' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ppintervals3a-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the posterior predictions are not inconsistent with the observed\n  data\n\n**Ribbon**\n\nThese are just an alternative way of expressing the interval plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3a.brm2 |> pp_check(type = 'ribbon', x = \"x\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'ribbon' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ppribbon3a-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the posterior predictions are not inconsistent with the observed\n  data\n\n:::: {.callout-tip collapse=\"true\"}\n\n####### Using `shiny` to explore a wide range of MCMC and validation diagnostics\n\nThe `shinystan` package allows the full suite of MCMC diagnostics and posterior\npredictive checks to be accessed via a web interface.\n\n\n\n::: {.cell mhidden='true'}\n\n```{.r .cell-code}\nlibrary(shinystan)\nlaunch_shinystan(dat3a.brm2)\n```\n:::\n\n\n\n::::\n\n\n##### DHARMa (simulated) residuals\n\nDHARMa residuals provide very useful diagnostics. Unfortunately, we\ncannot directly use the `simulateResiduals()` function to generate the\nsimulated residuals. However, if we are willing to calculate some of\nthe components yourself, we can still obtain the simulated residuals\nfrom the fitted stan model.\n\nWe need to supply:\n\n- simulated (predicted) responses associated with each observation.\n- observed values\n- fitted (predicted) responses (averaged) associated with each\n  observation\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3a.resids <- make_brms_dharma_res(dat3a.brm2, integerResponse = FALSE)\nwrap_elements(~ testUniformity(dat3a.resids)) +\n    wrap_elements(~ plotResiduals(dat3a.resids)) +\n    wrap_elements(~ testDispersion(dat3a.resids)) +\n    plot_layout(nrow = 1)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_DHARMa3a-1.png){width=8in}\n:::\n:::\n\n\n\n:::: {.callout-note}\n\nIf you are using Rstudio (particularly with a quarto document), the above code may produce a graphic that is too large for the currently available device.  The document should still render without problems, it is just the live display within Rstudio that cannot accommodate the figure.\n\nTo address this, you can either:\n\n- break the multi-panel figure up into four separate figures by\n  removing the outer `wrap_elements()` and `plot_layout()` functions.\n- copy the above code into the console and view in a larger graphics\n  device\n::::\n\n**Conclusions**:\n\n- the Q-Q plot looks reasonable (points broadly follow the angled\n  line)\n- there are no flagged issues with the\n  - KS test: conformity to the nominated distribution (family)\n  - Dispersion test: would not normally expect this to be an issue for\n    a Gaussian family unless there are other issues with the residuals\n  - Outlier test: the influence of each observation\n- there does not appear to be any patterns in the residuals - each of\n  the three quantile trends are considered flat and centered around\n  1/3, 1/2 and 2/3\n- the observed dispersion is well within the simulated range\n  indicating that there is no issue with dispersion (again this is\n  expected for a Gaussian model)\n\n:::::\n\n**Conclusions**:\n- there is no evidence of a lack of fit\n- the model is likely to be reliable\n\n#### Centered predictor\n\n::::: {.panel-tabset}\n\n##### Simple residual plots\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid <- resid(dat3b.brm2)[, \"Estimate\"]\nfit <- fitted(dat3b.brm2)[, \"Estimate\"]\nggplot() +\n  geom_point(data = NULL, aes(y = resid, x = fit))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelvalidation_resid3b-1.png){width=384}\n:::\n:::\n\n\n\nWe should also plot the residuals against each of the predictor\nvariables (as well as any other important unmodelled predictors -\nparticularly time and space if relevant and available).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid <- resid(dat3b.brm2)[, \"Estimate\"]\nggplot() +\n  geom_point(data = NULL, aes(y = resid, x = dat3$x))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelvalidation_resid23b-1.png){width=384}\n:::\n:::\n\n\n\n**conclusions**:\n\n- there does not appear to be any pattern in the residuals\n\n##### Posterior Probability Checks\n\n**density overlay**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3b.brm2 |> pp_check(type = 'dens_overlay', ndraws = 100)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelvalidation_ppdensity3b-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the model draws appear to be consistent with the observed data\n\n**Error scatter**\n\nThese are plots of the observed values against the average residuals.\nSimilar to a residual plot, we do not want to see any patterns in this\nplot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3b.brm2 |> pp_check(type = 'error_scatter_avg')\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'error_scatter_avg' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_pperror3b-1.png){width=288}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there is no obvious pattern in the residuals.\n\n**Intervals**\n\nThese are plots of the observed data overlayed on top of posterior\npredictions associated with each level of the predictor. Ideally, the\nobserved data should all fall within the predictive intervals.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3b.brm2 |> pp_check(type = 'intervals', x = \"x\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'intervals' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ppintervals3b-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the posterior predictions are not inconsistent with the observed\n  data\n\n**Ribbon**\n\nThese are just an alternative way of expressing the interval plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3b.brm2 |> pp_check(type = 'ribbon', x = \"x\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'ribbon' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ppribbon3b-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the posterior predictions are not inconsistent with the observed\n  data\n\n:::: {.callout-tip collapse=\"true\"}\n\n####### Using `shiny` to explore a wide range of MCMC and validation diagnostics\n\nThe `shinystan` package allows the full suite of MCMC diagnostics and posterior\npredictive checks to be accessed via a web interface.\n\n\n\n::: {.cell mhidden='true'}\n\n```{.r .cell-code}\nlibrary(shinystan)\nlaunch_shinystan(dat3b.brm2)\n```\n:::\n\n\n\n::::\n\n\n##### DHARMa (simulated) residuals\n\nDHARMa residuals provide very useful diagnostics. Unfortunately, we\ncannot directly use the `simulateResiduals()` function to generate the\nsimulated residuals. However, if we are willing to calculate some of\nthe components yourself, we can still obtain the simulated residuals\nfrom the fitted stan model.\n\nWe need to supply:\n\n- simulated (predicted) responses associated with each observation.\n- observed values\n- fitted (predicted) responses (averaged) associated with each\n  observation\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3b.resids <- make_brms_dharma_res(dat3b.brm2, integerResponse = FALSE)\nwrap_elements(~ testUniformity(dat3b.resids)) +\n    wrap_elements(~ plotResiduals(dat3b.resids)) +\n    wrap_elements(~ testDispersion(dat3b.resids)) +\n    plot_layout(nrow = 1)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_DHARMa3b-1.png){width=8in}\n:::\n:::\n\n\n\n:::: {.callout-note}\n\nIf you are using Rstudio (particularly with a quarto document), the above code may produce a graphic that is too large for the currently available device.  The document should still render without problems, it is just the live display within Rstudio that cannot accommodate the figure.\n\nTo address this, you can either:\n\n- break the multi-panel figure up into four separate figures by\n  removing the outer `wrap_elements()` and `plot_layout()` functions.\n- copy the above code into the console and view in a larger graphics\n  device\n::::\n\n**Conclusions**:\n\n- the Q-Q plot looks reasonable (points broadly follow the angled\n  line)\n- there are no flagged issues with the\n  - KS test: conformity to the nominated distribution (family)\n  - Dispersion test: would not normally expect this to be an issue for\n    a Gaussian family unless there are other issues with the residuals\n  - Outlier test: the influence of each observation\n- there does not appear to be any patterns in the residuals - each of\n  the three quantile trends are considered flat and centered around\n  1/3, 1/2 and 2/3\n- the observed dispersion is well within the simulated range\n  indicating that there is no issue with dispersion (again this is\n  expected for a Gaussian model)\n\n:::::\n\n**Conclusions**:\n- there is no evidence of a lack of fit\n- the model is likely to be reliable\n\n#### Standardised predictor\n\n::::: {.panel-tabset}\n\n##### Simple residual plots\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid <- resid(dat3c.brm2)[, \"Estimate\"]\nfit <- fitted(dat3c.brm2)[, \"Estimate\"]\nggplot() +\n  geom_point(data = NULL, aes(y = resid, x = fit))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelvalidation_resid3c-1.png){width=384}\n:::\n:::\n\n\n\nWe should also plot the residuals against each of the predictor\nvariables (as well as any other important unmodelled predictors -\nparticularly time and space if relevant and available).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid <- resid(dat3c.brm2)[, \"Estimate\"]\nggplot() +\n  geom_point(data = NULL, aes(y = resid, x = dat3$x))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelvalidation_resid23c-1.png){width=384}\n:::\n:::\n\n\n\n**conclusions**:\n\n- there does not appear to be any pattern in the residuals\n\n##### Posterior Probability Checks\n\n**density overlay**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3c.brm2 |> pp_check(type = 'dens_overlay', ndraws = 100)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelvalidation_ppdensity3c-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the model draws appear to be consistent with the observed data\n\n**Error scatter**\n\nThese are plots of the observed values against the average residuals.\nSimilar to a residual plot, we do not want to see any patterns in this\nplot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3c.brm2 |> pp_check(type = 'error_scatter_avg')\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'error_scatter_avg' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_pperror3c-1.png){width=288}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there is no obvious pattern in the residuals.\n\n**Intervals**\n\nThese are plots of the observed data overlayed on top of posterior\npredictions associated with each level of the predictor. Ideally, the\nobserved data should all fall within the predictive intervals.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3c.brm2 |> pp_check(type = 'intervals', x = \"x\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'intervals' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ppintervals3c-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the posterior predictions are not inconsistent with the observed\n  data\n\n**Ribbon**\n\nThese are just an alternative way of expressing the interval plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3c.brm2 |> pp_check(type = 'ribbon', x = \"x\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'ribbon' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ppribbon3c-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the posterior predictions are not inconsistent with the observed\n  data\n\n:::: {.callout-tip collapse=\"true\"}\n\n####### Using `shiny` to explore a wide range of MCMC and validation diagnostics\n\nThe `shinystan` package allows the full suite of MCMC diagnostics and posterior\npredictive checks to be accessed via a web interface.\n\n\n\n::: {.cell mhidden='true'}\n\n```{.r .cell-code}\nlibrary(shinystan)\nlaunch_shinystan(dat3c.brm2)\n```\n:::\n\n\n\n::::\n\n\n##### DHARMa (simulated) residuals\n\nDHARMa residuals provide very useful diagnostics. Unfortunately, we\ncannot directly use the `simulateResiduals()` function to generate the\nsimulated residuals. However, if we are willing to calculate some of\nthe components yourself, we can still obtain the simulated residuals\nfrom the fitted stan model.\n\nWe need to supply:\n\n- simulated (predicted) responses associated with each observation.\n- observed values\n- fitted (predicted) responses (averaged) associated with each\n  observation\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3c.resids <- make_brms_dharma_res(dat3c.brm2, integerResponse = FALSE)\nwrap_elements(~ testUniformity(dat3c.resids)) +\n    wrap_elements(~ plotResiduals(dat3c.resids)) +\n    wrap_elements(~ testDispersion(dat3c.resids)) +\n    plot_layout(nrow = 1)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_DHARMa3c-1.png){width=8in}\n:::\n:::\n\n\n\n:::: {.callout-note}\n\nIf you are using Rstudio (particularly with a quarto document), the above code may produce a graphic that is too large for the currently available device.  The document should still render without problems, it is just the live display within Rstudio that cannot accommodate the figure.\n\nTo address this, you can either:\n\n- break the multi-panel figure up into four separate figures by\n  removing the outer `wrap_elements()` and `plot_layout()` functions.\n- copy the above code into the console and view in a larger graphics\n  device\n::::\n\n**Conclusions**:\n\n- the Q-Q plot looks reasonable (points broadly follow the angled\n  line)\n- there are no flagged issues with the\n  - KS test: conformity to the nominated distribution (family)\n  - Dispersion test: would not normally expect this to be an issue for\n    a Gaussian family unless there are other issues with the residuals\n  - Outlier test: the influence of each observation\n- there does not appear to be any patterns in the residuals - each of\n  the three quantile trends are considered flat and centered around\n  1/3, 1/2 and 2/3\n- the observed dispersion is well within the simulated range\n  indicating that there is no issue with dispersion (again this is\n  expected for a Gaussian model)\n\n:::::\n\n**Conclusions**:\n- there is no evidence of a lack of fit\n- the model is likely to be reliable\n\n:::::::\n\n### Example 4 (NB data)\n\n::::::: {.panel-tabset}\n\n#### Raw predictor\n\n::::: {.panel-tabset}\n\n##### Simple residual plots\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid <- resid(dat4a.brm2)[, \"Estimate\"]\nfit <- fitted(dat4a.brm2)[, \"Estimate\"]\nggplot() +\n  geom_point(data = NULL, aes(y = resid, x = fit))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelvalidation_resid4a-1.png){width=384}\n:::\n:::\n\n\n\nWe should also plot the residuals against each of the predictor\nvariables (as well as any other important unmodelled predictors -\nparticularly time and space if relevant and available).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid <- resid(dat4a.brm2)[, \"Estimate\"]\nggplot() +\n  geom_point(data = NULL, aes(y = resid, x = dat4$x))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelvalidation_resid24a-1.png){width=384}\n:::\n:::\n\n\n\n**conclusions**:\n\n- there does not appear to be any pattern in the residuals\n\n##### Posterior Probability Checks\n\n**density overlay**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4a.brm2 |> pp_check(type = 'dens_overlay', ndraws = 100)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelvalidation_ppdensity4a-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the model draws appear to be consistent with the observed data\n\n**Error scatter**\n\nThese are plots of the observed values against the average residuals.\nSimilar to a residual plot, we do not want to see any patterns in this\nplot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4a.brm2 |> pp_check(type = 'error_scatter_avg')\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'error_scatter_avg' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_pperror4a-1.png){width=288}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there is no obvious pattern in the residuals.\n\n**Intervals**\n\nThese are plots of the observed data overlayed on top of posterior\npredictions associated with each level of the predictor. Ideally, the\nobserved data should all fall within the predictive intervals.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4a.brm2 |> pp_check(type = 'intervals', x = \"x\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'intervals' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ppintervals4a-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the posterior predictions are not inconsistent with the observed\n  data\n\n**Ribbon**\n\nThese are just an alternative way of expressing the interval plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4a.brm2 |> pp_check(type = 'ribbon', x = \"x\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'ribbon' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ppribbon4a-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the posterior predictions are not inconsistent with the observed\n  data\n\n:::: {.callout-tip collapse=\"true\"}\n\n####### Using `shiny` to explore a wide range of MCMC and validation diagnostics\n\nThe `shinystan` package allows the full suite of MCMC diagnostics and posterior\npredictive checks to be accessed via a web interface.\n\n\n\n::: {.cell mhidden='true'}\n\n```{.r .cell-code}\nlibrary(shinystan)\nlaunch_shinystan(dat4a.brm2)\n```\n:::\n\n\n\n::::\n\n\n##### DHARMa (simulated) residuals\n\nDHARMa residuals provide very useful diagnostics. Unfortunately, we\ncannot directly use the `simulateResiduals()` function to generate the\nsimulated residuals. However, if we are willing to calculate some of\nthe components yourself, we can still obtain the simulated residuals\nfrom the fitted stan model.\n\nWe need to supply:\n\n- simulated (predicted) responses associated with each observation.\n- observed values\n- fitted (predicted) responses (averaged) associated with each\n  observation\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4a.resids <- make_brms_dharma_res(dat4a.brm2, integerResponse = FALSE)\nwrap_elements(~ testUniformity(dat4a.resids)) +\n    wrap_elements(~ plotResiduals(dat4a.resids)) +\n    wrap_elements(~ testDispersion(dat4a.resids)) +\n    plot_layout(nrow = 1)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_DHARMa4a-1.png){width=8in}\n:::\n:::\n\n\n\n:::: {.callout-note}\n\nIf you are using Rstudio (particularly with a quarto document), the above code may produce a graphic that is too large for the currently available device.  The document should still render without problems, it is just the live display within Rstudio that cannot accommodate the figure.\n\nTo address this, you can either:\n\n- break the multi-panel figure up into four separate figures by\n  removing the outer `wrap_elements()` and `plot_layout()` functions.\n- copy the above code into the console and view in a larger graphics\n  device\n::::\n\n**Conclusions**:\n\n- the Q-Q plot looks reasonable (points broadly follow the angled\n  line)\n- there are no flagged issues with the\n  - KS test: conformity to the nominated distribution (family)\n  - Dispersion test: would not normally expect this to be an issue for\n    a Gaussian family unless there are other issues with the residuals\n  - Outlier test: the influence of each observation\n- there does not appear to be any patterns in the residuals - each of\n  the three quantile trends are considered flat and centered around\n  1/3, 1/2 and 2/3\n- the observed dispersion is well within the simulated range\n  indicating that there is no issue with dispersion (again this is\n  expected for a Gaussian model)\n\n:::::\n\n**Conclusions**:\n- there is no evidence of a lack of fit\n- the model is likely to be reliable\n\n#### Centered predictor\n\n::::: {.panel-tabset}\n\n##### Simple residual plots\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid <- resid(dat4b.brm2)[, \"Estimate\"]\nfit <- fitted(dat4b.brm2)[, \"Estimate\"]\nggplot() +\n  geom_point(data = NULL, aes(y = resid, x = fit))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelvalidation_resid4b-1.png){width=384}\n:::\n:::\n\n\n\nWe should also plot the residuals against each of the predictor\nvariables (as well as any other important unmodelled predictors -\nparticularly time and space if relevant and available).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid <- resid(dat4b.brm2)[, \"Estimate\"]\nggplot() +\n  geom_point(data = NULL, aes(y = resid, x = dat4$x))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelvalidation_resid24b-1.png){width=384}\n:::\n:::\n\n\n\n**conclusions**:\n\n- there does not appear to be any pattern in the residuals\n\n##### Posterior Probability Checks\n\n**density overlay**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4b.brm2 |> pp_check(type = 'dens_overlay', ndraws = 100)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelvalidation_ppdensity4b-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the model draws appear to be consistent with the observed data\n\n**Error scatter**\n\nThese are plots of the observed values against the average residuals.\nSimilar to a residual plot, we do not want to see any patterns in this\nplot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4b.brm2 |> pp_check(type = 'error_scatter_avg')\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'error_scatter_avg' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_pperror4b-1.png){width=288}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there is no obvious pattern in the residuals.\n\n**Intervals**\n\nThese are plots of the observed data overlayed on top of posterior\npredictions associated with each level of the predictor. Ideally, the\nobserved data should all fall within the predictive intervals.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4b.brm2 |> pp_check(type = 'intervals', x = \"x\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'intervals' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ppintervals4b-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the posterior predictions are not inconsistent with the observed\n  data\n\n**Ribbon**\n\nThese are just an alternative way of expressing the interval plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4b.brm2 |> pp_check(type = 'ribbon', x = \"x\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'ribbon' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ppribbon4b-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the posterior predictions are not inconsistent with the observed\n  data\n\n:::: {.callout-tip collapse=\"true\"}\n\n####### Using `shiny` to explore a wide range of MCMC and validation diagnostics\n\nThe `shinystan` package allows the full suite of MCMC diagnostics and posterior\npredictive checks to be accessed via a web interface.\n\n\n\n::: {.cell mhidden='true'}\n\n```{.r .cell-code}\nlibrary(shinystan)\nlaunch_shinystan(dat4b.brm2)\n```\n:::\n\n\n\n::::\n\n\n##### DHARMa (simulated) residuals\n\nDHARMa residuals provide very useful diagnostics. Unfortunately, we\ncannot directly use the `simulateResiduals()` function to generate the\nsimulated residuals. However, if we are willing to calculate some of\nthe components yourself, we can still obtain the simulated residuals\nfrom the fitted stan model.\n\nWe need to supply:\n\n- simulated (predicted) responses associated with each observation.\n- observed values\n- fitted (predicted) responses (averaged) associated with each\n  observation\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4b.resids <- make_brms_dharma_res(dat4b.brm2, integerResponse = FALSE)\nwrap_elements(~ testUniformity(dat4b.resids)) +\n    wrap_elements(~ plotResiduals(dat4b.resids)) +\n    wrap_elements(~ testDispersion(dat4b.resids)) +\n    plot_layout(nrow = 1)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_DHARMa4b-1.png){width=8in}\n:::\n:::\n\n\n\n:::: {.callout-note}\n\nIf you are using Rstudio (particularly with a quarto document), the above code may produce a graphic that is too large for the currently available device.  The document should still render without problems, it is just the live display within Rstudio that cannot accommodate the figure.\n\nTo address this, you can either:\n\n- break the multi-panel figure up into four separate figures by\n  removing the outer `wrap_elements()` and `plot_layout()` functions.\n- copy the above code into the console and view in a larger graphics\n  device\n::::\n\n**Conclusions**:\n\n- the Q-Q plot looks reasonable (points broadly follow the angled\n  line)\n- there are no flagged issues with the\n  - KS test: conformity to the nominated distribution (family)\n  - Dispersion test: would not normally expect this to be an issue for\n    a Gaussian family unless there are other issues with the residuals\n  - Outlier test: the influence of each observation\n- there does not appear to be any patterns in the residuals - each of\n  the three quantile trends are considered flat and centered around\n  1/3, 1/2 and 2/3\n- the observed dispersion is well within the simulated range\n  indicating that there is no issue with dispersion (again this is\n  expected for a Gaussian model)\n\n:::::\n\n**Conclusions**:\n- there is no evidence of a lack of fit\n- the model is likely to be reliable\n\n#### Standardised predictor\n\n::::: {.panel-tabset}\n\n##### Simple residual plots\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid <- resid(dat4c.brm2)[, \"Estimate\"]\nfit <- fitted(dat4c.brm2)[, \"Estimate\"]\nggplot() +\n  geom_point(data = NULL, aes(y = resid, x = fit))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelvalidation_resid4c-1.png){width=384}\n:::\n:::\n\n\n\nWe should also plot the residuals against each of the predictor\nvariables (as well as any other important unmodelled predictors -\nparticularly time and space if relevant and available).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid <- resid(dat4c.brm2)[, \"Estimate\"]\nggplot() +\n  geom_point(data = NULL, aes(y = resid, x = dat4$x))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelvalidation_resid24c-1.png){width=384}\n:::\n:::\n\n\n\n**conclusions**:\n\n- there does not appear to be any pattern in the residuals\n\n##### Posterior Probability Checks\n\n**density overlay**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4c.brm2 |> pp_check(type = 'dens_overlay', ndraws = 100)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelvalidation_ppdensity4c-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the model draws appear to be consistent with the observed data\n\n**Error scatter**\n\nThese are plots of the observed values against the average residuals.\nSimilar to a residual plot, we do not want to see any patterns in this\nplot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4c.brm2 |> pp_check(type = 'error_scatter_avg')\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'error_scatter_avg' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_pperror4c-1.png){width=288}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there is no obvious pattern in the residuals.\n\n**Intervals**\n\nThese are plots of the observed data overlayed on top of posterior\npredictions associated with each level of the predictor. Ideally, the\nobserved data should all fall within the predictive intervals.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4c.brm2 |> pp_check(type = 'intervals', x = \"x\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'intervals' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ppintervals4c-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the posterior predictions are not inconsistent with the observed\n  data\n\n**Ribbon**\n\nThese are just an alternative way of expressing the interval plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4c.brm2 |> pp_check(type = 'ribbon', x = \"x\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'ribbon' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ppribbon4c-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the posterior predictions are not inconsistent with the observed\n  data\n\n:::: {.callout-tip collapse=\"true\"}\n\n####### Using `shiny` to explore a wide range of MCMC and validation diagnostics\n\nThe `shinystan` package allows the full suite of MCMC diagnostics and posterior\npredictive checks to be accessed via a web interface.\n\n\n\n::: {.cell mhidden='true'}\n\n```{.r .cell-code}\nlibrary(shinystan)\nlaunch_shinystan(dat4c.brm2)\n```\n:::\n\n\n\n::::\n\n\n##### DHARMa (simulated) residuals\n\nDHARMa residuals provide very useful diagnostics. Unfortunately, we\ncannot directly use the `simulateResiduals()` function to generate the\nsimulated residuals. However, if we are willing to calculate some of\nthe components yourself, we can still obtain the simulated residuals\nfrom the fitted stan model.\n\nWe need to supply:\n\n- simulated (predicted) responses associated with each observation.\n- observed values\n- fitted (predicted) responses (averaged) associated with each\n  observation\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4c.resids <- make_brms_dharma_res(dat4c.brm2, integerResponse = FALSE)\nwrap_elements(~ testUniformity(dat4c.resids)) +\n    wrap_elements(~ plotResiduals(dat4c.resids)) +\n    wrap_elements(~ testDispersion(dat4c.resids)) +\n    plot_layout(nrow = 1)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_DHARMa4c-1.png){width=8in}\n:::\n:::\n\n\n\n:::: {.callout-note}\n\nIf you are using Rstudio (particularly with a quarto document), the above code may produce a graphic that is too large for the currently available device.  The document should still render without problems, it is just the live display within Rstudio that cannot accommodate the figure.\n\nTo address this, you can either:\n\n- break the multi-panel figure up into four separate figures by\n  removing the outer `wrap_elements()` and `plot_layout()` functions.\n- copy the above code into the console and view in a larger graphics\n  device\n::::\n\n**Conclusions**:\n\n- the Q-Q plot looks reasonable (points broadly follow the angled\n  line)\n- there are no flagged issues with the\n  - KS test: conformity to the nominated distribution (family)\n  - Dispersion test: would not normally expect this to be an issue for\n    a Gaussian family unless there are other issues with the residuals\n  - Outlier test: the influence of each observation\n- there does not appear to be any patterns in the residuals - each of\n  the three quantile trends are considered flat and centered around\n  1/3, 1/2 and 2/3\n- the observed dispersion is well within the simulated range\n  indicating that there is no issue with dispersion (again this is\n  expected for a Gaussian model)\n\n:::::\n\n**Conclusions**:\n- there is no evidence of a lack of fit\n- the model is likely to be reliable\n\n:::::::\n\n### Example 5 (Binary data)\n\n::::::: {.panel-tabset}\n\n#### Raw predictor\n\n::::: {.panel-tabset}\n\n##### Simple residual plots\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid <- resid(dat5a.brm2)[, \"Estimate\"]\nfit <- fitted(dat5a.brm2)[, \"Estimate\"]\nggplot() +\n  geom_point(data = NULL, aes(y = resid, x = fit))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelvalidation_resid5a-1.png){width=384}\n:::\n:::\n\n\n\nWe should also plot the residuals against each of the predictor\nvariables (as well as any other important unmodelled predictors -\nparticularly time and space if relevant and available).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid <- resid(dat5a.brm2)[, \"Estimate\"]\nggplot() +\n  geom_point(data = NULL, aes(y = resid, x = dat3$x))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelvalidation_resid25a-1.png){width=384}\n:::\n:::\n\n\n\n**conclusions**:\n\n- the above plots are almost impossible to interpret for binary data.\n- they will always feature two curved lines (one for the zeros, the\n  other for the ones)\n- it is virtually impossible to diagnose any issues from such plots.\n\n##### Posterior Probability Checks\n\n**density overlay**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5a.brm2 |> pp_check(type = 'dens_overlay', ndraws = 100)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelvalidation_ppdensity5a-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the model draws appear to be consistent with the observed data\n- note that these density plots are going to be too crude to be completely useful\n- all the mass should be at either 0 or 1\n\n**Error scatter**\n\nThese are plots of the observed values against the average residuals.\nSimilar to a residual plot, we do not want to see any patterns in this\nplot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5a.brm2 |> pp_check(type = 'error_scatter_avg')\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'error_scatter_avg' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_pperror5a-1.png){width=288}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there is no obvious pattern in the residuals.\n- this sort of plot is of very little value for binary data\n\n**Intervals**\n\nThese are plots of the observed data overlayed on top of posterior\npredictions associated with each level of the predictor. Ideally, the\nobserved data should all fall within the predictive intervals.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5a.brm2 |> pp_check(type = 'intervals', x = \"x\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'intervals' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ppintervals5a-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the posterior predictions are not inconsistent with the observed\n  data\n- this sort of plot is of very little value for binary data\n\n**Ribbon**\n\nThese are just an alternative way of expressing the interval plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5a.brm2 |> pp_check(type = 'ribbon', x = \"x\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'ribbon' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ppribbon5a-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the posterior predictions are not inconsistent with the observed\n  data\n- this sort of plot is of very little value for binary data\n\n:::: {.callout-tip collapse=\"true\"}\n\n####### Using `shiny` to explore a wide range of MCMC and validation diagnostics\n\nThe `shinystan` package allows the full suite of MCMC diagnostics and posterior\npredictive checks to be accessed via a web interface.\n\n\n\n::: {.cell mhidden='true'}\n\n```{.r .cell-code}\nlibrary(shinystan)\nlaunch_shinystan(dat5a.brm2)\n```\n:::\n\n\n\n::::\n\n\n##### DHARMa (simulated) residuals\n\nDHARMa residuals provide very useful diagnostics. Unfortunately, we\ncannot directly use the `simulateResiduals()` function to generate the\nsimulated residuals. However, if we are willing to calculate some of\nthe components yourself, we can still obtain the simulated residuals\nfrom the fitted stan model.\n\nWe need to supply:\n\n- simulated (predicted) responses associated with each observation.\n- observed values\n- fitted (predicted) responses (averaged) associated with each\n  observation\n\nIn the code below, I have instructed the residual plot to not apply\nquantile regression to the residuals due to a lack of unique data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5a.resids <- make_brms_dharma_res(dat5a.brm2, integerResponse = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nOnly 2 levels detected so that family 'bernoulli' might be a more efficient choice.\n```\n\n\n:::\n\n```{.r .cell-code}\nwrap_elements(~ testUniformity(dat5a.resids)) +\n    wrap_elements(~ plotResiduals(dat5a.resids, quantreg = FALSE)) +\n    wrap_elements(~ testDispersion(dat5a.resids)) +\n    plot_layout(nrow = 1)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_DHARMa5a-1.png){width=8in}\n:::\n:::\n\n\n\n:::: {.callout-note}\n\nIf you are using Rstudio (particularly with a quarto document), the above code may produce a graphic that is too large for the currently available device.  The document should still render without problems, it is just the live display within Rstudio that cannot accommodate the figure.\n\nTo address this, you can either:\n\n- break the multi-panel figure up into four separate figures by\n  removing the outer `wrap_elements()` and `plot_layout()` functions.\n- copy the above code into the console and view in a larger graphics\n  device\n::::\n\n**Conclusions**:\n\n- the Q-Q plot looks reasonable (points broadly follow the angled\n  line)\n- there are no flagged issues with the\n  - KS test: conformity to the nominated distribution (family)\n  - Dispersion test: would not normally expect this to be an issue for\n    a Gaussian family unless there are other issues with the residuals\n  - Outlier test: the influence of each observation\n- there does not appear to be any patterns in the residuals \n- the observed dispersion is well within the simulated range\n  indicating that there is no issue with dispersion (again this is\n  expected for a Gaussian model)\n\n:::::\n\n**Conclusions**:\n- there is no evidence of a lack of fit\n- the model is likely to be reliable\n\n#### Centered predictor\n\n::::: {.panel-tabset}\n\n##### Simple residual plots\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid <- resid(dat5b.brm2)[, \"Estimate\"]\nfit <- fitted(dat5b.brm2)[, \"Estimate\"]\nggplot() +\n  geom_point(data = NULL, aes(y = resid, x = fit))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelvalidation_resid5b-1.png){width=384}\n:::\n:::\n\n\n\nWe should also plot the residuals against each of the predictor\nvariables (as well as any other important unmodelled predictors -\nparticularly time and space if relevant and available).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid <- resid(dat5b.brm2)[, \"Estimate\"]\nggplot() +\n  geom_point(data = NULL, aes(y = resid, x = dat3$x))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelvalidation_resid25b-1.png){width=384}\n:::\n:::\n\n\n\n**conclusions**:\n\n- the above plots are almost impossible to interpret for binary data.\n- they will always feature two curved lines (one for the zeros, the\n  other for the ones)\n- it is virtually impossible to diagnose any issues from such plots.\n\n##### Posterior Probability Checks\n\n**density overlay**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5b.brm2 |> pp_check(type = 'dens_overlay', ndraws = 100)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelvalidation_ppdensity5b-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the model draws appear to be consistent with the observed data\n- note that these density plots are going to be too crude to be completely useful\n- all the mass should be at either 0 or 1\n\n**Error scatter**\n\nThese are plots of the observed values against the average residuals.\nSimilar to a residual plot, we do not want to see any patterns in this\nplot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5b.brm2 |> pp_check(type = 'error_scatter_avg')\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'error_scatter_avg' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_pperror5b-1.png){width=288}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there is no obvious pattern in the residuals.\n- this sort of plot is of very little value for binary data\n\n**Intervals**\n\nThese are plots of the observed data overlayed on top of posterior\npredictions associated with each level of the predictor. Ideally, the\nobserved data should all fall within the predictive intervals.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5b.brm2 |> pp_check(type = 'intervals', x = \"x\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'intervals' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ppintervals5b-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the posterior predictions are not inconsistent with the observed\n  data\n- this sort of plot is of very little value for binary data\n\n**Ribbon**\n\nThese are just an alternative way of expressing the interval plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5b.brm2 |> pp_check(type = 'ribbon', x = \"x\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'ribbon' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ppribbon5b-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the posterior predictions are not inconsistent with the observed\n  data\n- this sort of plot is of very little value for binary data\n\n:::: {.callout-tip collapse=\"true\"}\n\n####### Using `shiny` to explore a wide range of MCMC and validation diagnostics\n\nThe `shinystan` package allows the full suite of MCMC diagnostics and posterior\npredictive checks to be accessed via a web interface.\n\n\n\n::: {.cell mhidden='true'}\n\n```{.r .cell-code}\nlibrary(shinystan)\nlaunch_shinystan(dat5b.brm2)\n```\n:::\n\n\n\n::::\n\n\n##### DHARMa (simulated) residuals\n\nDHARMa residuals provide very useful diagnostics. Unfortunately, we\ncannot directly use the `simulateResiduals()` function to generate the\nsimulated residuals. However, if we are willing to calculate some of\nthe components yourself, we can still obtain the simulated residuals\nfrom the fitted stan model.\n\nWe need to supply:\n\n- simulated (predicted) responses associated with each observation.\n- observed values\n- fitted (predicted) responses (averaged) associated with each\n  observation\n\nIn the code below, I have instructed the residual plot to not apply\nquantile regression to the residuals due to a lack of unique data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5b.resids <- make_brms_dharma_res(dat5b.brm2, integerResponse = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nOnly 2 levels detected so that family 'bernoulli' might be a more efficient choice.\n```\n\n\n:::\n\n```{.r .cell-code}\nwrap_elements(~ testUniformity(dat5b.resids)) +\n    wrap_elements(~ plotResiduals(dat5b.resids, quantreg = FALSE)) +\n    wrap_elements(~ testDispersion(dat5b.resids)) +\n    plot_layout(nrow = 1)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_DHARMa5b-1.png){width=8in}\n:::\n:::\n\n\n\n:::: {.callout-note}\n\nIf you are using Rstudio (particularly with a quarto document), the above code may produce a graphic that is too large for the currently available device.  The document should still render without problems, it is just the live display within Rstudio that cannot accommodate the figure.\n\nTo address this, you can either:\n\n- break the multi-panel figure up into four separate figures by\n  removing the outer `wrap_elements()` and `plot_layout()` functions.\n- copy the above code into the console and view in a larger graphics\n  device\n::::\n\n**Conclusions**:\n\n- the Q-Q plot looks reasonable (points broadly follow the angled\n  line)\n- there are no flagged issues with the\n  - KS test: conformity to the nominated distribution (family)\n  - Dispersion test: would not normally expect this to be an issue for\n    a Gaussian family unless there are other issues with the residuals\n  - Outlier test: the influence of each observation\n- there does not appear to be any patterns in the residuals \n- the observed dispersion is well within the simulated range\n  indicating that there is no issue with dispersion (again this is\n  expected for a Gaussian model)\n\n:::::\n\n**Conclusions**:\n- there is no evidence of a lack of fit\n- the model is likely to be reliable\n\n#### Standaridised predictor\n\n::::: {.panel-tabset}\n\n##### Simple residual plots\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid <- resid(dat5c.brm2)[, \"Estimate\"]\nfit <- fitted(dat5c.brm2)[, \"Estimate\"]\nggplot() +\n  geom_point(data = NULL, aes(y = resid, x = fit))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelvalidation_resid5c-1.png){width=384}\n:::\n:::\n\n\n\nWe should also plot the residuals against each of the predictor\nvariables (as well as any other important unmodelled predictors -\nparticularly time and space if relevant and available).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid <- resid(dat5c.brm2)[, \"Estimate\"]\nggplot() +\n  geom_point(data = NULL, aes(y = resid, x = dat3$x))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelvalidation_resid25c-1.png){width=384}\n:::\n:::\n\n\n\n**conclusions**:\n\n- the above plots are almost impossible to interpret for binary data.\n- they will always feature two curved lines (one for the zeros, the\n  other for the ones)\n- it is virtually impossible to diagnose any issues from such plots.\n\n##### Posterior Probability Checks\n\n**density overlay**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5c.brm2 |> pp_check(type = 'dens_overlay', ndraws = 100)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelvalidation_ppdensity5c-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the model draws appear to be consistent with the observed data\n- note that these density plots are going to be too crude to be completely useful\n- all the mass should be at either 0 or 1\n\n**Error scatter**\n\nThese are plots of the observed values against the average residuals.\nSimilar to a residual plot, we do not want to see any patterns in this\nplot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5c.brm2 |> pp_check(type = 'error_scatter_avg')\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'error_scatter_avg' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_pperror5c-1.png){width=288}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there is no obvious pattern in the residuals.\n- this sort of plot is of very little value for binary data\n\n**Intervals**\n\nThese are plots of the observed data overlayed on top of posterior\npredictions associated with each level of the predictor. Ideally, the\nobserved data should all fall within the predictive intervals.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5c.brm2 |> pp_check(type = 'intervals', x = \"x\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'intervals' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ppintervals5c-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the posterior predictions are not inconsistent with the observed\n  data\n- this sort of plot is of very little value for binary data\n\n**Ribbon**\n\nThese are just an alternative way of expressing the interval plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5c.brm2 |> pp_check(type = 'ribbon', x = \"x\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'ribbon' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ppribbon5c-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the posterior predictions are not inconsistent with the observed\n  data\n- this sort of plot is of very little value for binary data\n\n:::: {.callout-tip collapse=\"true\"}\n\n####### Using `shiny` to explore a wide range of MCMC and validation diagnostics\n\nThe `shinystan` package allows the full suite of MCMC diagnostics and posterior\npredictive checks to be accessed via a web interface.\n\n\n\n::: {.cell mhidden='true'}\n\n```{.r .cell-code}\nlibrary(shinystan)\nlaunch_shinystan(dat5c.brm2)\n```\n:::\n\n\n\n::::\n\n\n##### DHARMa (simulated) residuals\n\nDHARMa residuals provide very useful diagnostics. Unfortunately, we\ncannot directly use the `simulateResiduals()` function to generate the\nsimulated residuals. However, if we are willing to calculate some of\nthe components yourself, we can still obtain the simulated residuals\nfrom the fitted stan model.\n\nWe need to supply:\n\n- simulated (predicted) responses associated with each observation.\n- observed values\n- fitted (predicted) responses (averaged) associated with each\n  observation\n\nIn the code below, I have instructed the residual plot to not apply\nquantile regression to the residuals due to a lack of unique data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5c.resids <- make_brms_dharma_res(dat5c.brm2, integerResponse = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nOnly 2 levels detected so that family 'bernoulli' might be a more efficient choice.\n```\n\n\n:::\n\n```{.r .cell-code}\nwrap_elements(~ testUniformity(dat5c.resids)) +\n    wrap_elements(~ plotResiduals(dat5c.resids, quantreg = FALSE)) +\n    wrap_elements(~ testDispersion(dat5c.resids)) +\n    plot_layout(nrow = 1)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_DHARMa5c-1.png){width=8in}\n:::\n:::\n\n\n\n:::: {.callout-note}\n\nIf you are using Rstudio (particularly with a quarto document), the above code may produce a graphic that is too large for the currently available device.  The document should still render without problems, it is just the live display within Rstudio that cannot accommodate the figure.\n\nTo address this, you can either:\n\n- break the multi-panel figure up into four separate figures by\n  removing the outer `wrap_elements()` and `plot_layout()` functions.\n- copy the above code into the console and view in a larger graphics\n  device\n::::\n\n**Conclusions**:\n\n- the Q-Q plot looks reasonable (points broadly follow the angled\n  line)\n- there are no flagged issues with the\n  - KS test: conformity to the nominated distribution (family)\n  - Dispersion test: would not normally expect this to be an issue for\n    a Gaussian family unless there are other issues with the residuals\n  - Outlier test: the influence of each observation\n- there does not appear to be any patterns in the residuals \n- the observed dispersion is well within the simulated range\n  indicating that there is no issue with dispersion (again this is\n  expected for a Gaussian model)\n\n:::::\n\n**Conclusions**:\n- there is no evidence of a lack of fit\n- the model is likely to be reliable\n\n:::::::\n\n### Example 6 (Binomial data)\n\n::::::: {.panel-tabset}\n\n#### Raw predictor\n\n::::: {.panel-tabset}\n\n##### Simple residual plots\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid <- resid(dat6a.brm2)[, \"Estimate\"]\nfit <- fitted(dat6a.brm2)[, \"Estimate\"]\nggplot() +\n  geom_point(data = NULL, aes(y = resid, x = fit))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelvalidation_resid6a-1.png){width=384}\n:::\n:::\n\n\n\nWe should also plot the residuals against each of the predictor\nvariables (as well as any other important unmodelled predictors -\nparticularly time and space if relevant and available).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid <- resid(dat6a.brm2)[, \"Estimate\"]\nggplot() +\n  geom_point(data = NULL, aes(y = resid, x = dat6$x))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelvalidation_resid26a-1.png){width=384}\n:::\n:::\n\n\n\n**conclusions**:\n\n- there does not appear to be any pattern in the residuals\n\n##### Posterior Probability Checks\n\n**density overlay**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6a.brm2 |> pp_check(type = 'dens_overlay', ndraws = 100)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelvalidation_ppdensity6a-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the model draws appear to be consistent with the observed data\n\n**Error scatter**\n\nThese are plots of the observed values against the average residuals.\nSimilar to a residual plot, we do not want to see any patterns in this\nplot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6a.brm2 |> pp_check(type = 'error_scatter_avg')\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'error_scatter_avg' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_pperror6a-1.png){width=288}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there is no obvious pattern in the residuals.\n\n**Intervals**\n\nThese are plots of the observed data overlayed on top of posterior\npredictions associated with each level of the predictor. Ideally, the\nobserved data should all fall within the predictive intervals.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6a.brm2 |> pp_check(type = 'intervals', x = \"x\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'intervals' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ppintervals6a-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the posterior predictions are not inconsistent with the observed\n  data\n\n**Ribbon**\n\nThese are just an alternative way of expressing the interval plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6a.brm2 |> pp_check(type = 'ribbon', x = \"x\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'ribbon' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ppribbon6a-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the posterior predictions are not inconsistent with the observed\n  data\n\n:::: {.callout-tip collapse=\"true\"}\n\n####### Using `shiny` to explore a wide range of MCMC and validation diagnostics\n\nThe `shinystan` package allows the full suite of MCMC diagnostics and posterior\npredictive checks to be accessed via a web interface.\n\n\n\n::: {.cell mhidden='true'}\n\n```{.r .cell-code}\nlibrary(shinystan)\nlaunch_shinystan(dat6a.brm2)\n```\n:::\n\n\n\n::::\n\n\n##### DHARMa (simulated) residuals\n\nDHARMa residuals provide very useful diagnostics. Unfortunately, we\ncannot directly use the `simulateResiduals()` function to generate the\nsimulated residuals. However, if we are willing to calculate some of\nthe components yourself, we can still obtain the simulated residuals\nfrom the fitted stan model.\n\nWe need to supply:\n\n- simulated (predicted) responses associated with each observation.\n- observed values\n- fitted (predicted) responses (averaged) associated with each\n  observation\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6a.resids <- make_brms_dharma_res(dat6a.brm2, integerResponse = FALSE)\nwrap_elements(~ testUniformity(dat6a.resids)) +\n    wrap_elements(~ plotResiduals(dat6a.resids)) +\n    wrap_elements(~ testDispersion(dat6a.resids)) +\n    plot_layout(nrow = 1)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_DHARMa6a-1.png){width=8in}\n:::\n:::\n\n\n\n:::: {.callout-note}\n\nIf you are using Rstudio (particularly with a quarto document), the above code may produce a graphic that is too large for the currently available device.  The document should still render without problems, it is just the live display within Rstudio that cannot accommodate the figure.\n\nTo address this, you can either:\n\n- break the multi-panel figure up into four separate figures by\n  removing the outer `wrap_elements()` and `plot_layout()` functions.\n- copy the above code into the console and view in a larger graphics\n  device\n::::\n\n**Conclusions**:\n\n- the Q-Q plot looks reasonable (points broadly follow the angled\n  line)\n- there are no flagged issues with the\n  - KS test: conformity to the nominated distribution (family)\n  - Dispersion test: would not normally expect this to be an issue for\n    a Gaussian family unless there are other issues with the residuals\n  - Outlier test: the influence of each observation\n- there does not appear to be any patterns in the residuals - each of\n  the three quantile trends are considered flat and centered around\n  1/3, 1/2 and 2/3\n- the observed dispersion is well within the simulated range\n  indicating that there is no issue with dispersion (again this is\n  expected for a Gaussian model)\n\n:::::\n\n**Conclusions**:\n- there is no evidence of a lack of fit\n- the model is likely to be reliable\n\n#### Centered predictor\n\n::::: {.panel-tabset}\n\n##### Simple residual plots\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid <- resid(dat6b.brm2)[, \"Estimate\"]\nfit <- fitted(dat6b.brm2)[, \"Estimate\"]\nggplot() +\n  geom_point(data = NULL, aes(y = resid, x = fit))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelvalidation_resid6b-1.png){width=384}\n:::\n:::\n\n\n\nWe should also plot the residuals against each of the predictor\nvariables (as well as any other important unmodelled predictors -\nparticularly time and space if relevant and available).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid <- resid(dat6b.brm2)[, \"Estimate\"]\nggplot() +\n  geom_point(data = NULL, aes(y = resid, x = dat6$x))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelvalidation_resid26b-1.png){width=384}\n:::\n:::\n\n\n\n**conclusions**:\n\n- there does not appear to be any pattern in the residuals\n\n##### Posterior Probability Checks\n\n**density overlay**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6b.brm2 |> pp_check(type = 'dens_overlay', ndraws = 100)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelvalidation_ppdensity6b-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the model draws appear to be consistent with the observed data\n\n**Error scatter**\n\nThese are plots of the observed values against the average residuals.\nSimilar to a residual plot, we do not want to see any patterns in this\nplot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6b.brm2 |> pp_check(type = 'error_scatter_avg')\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'error_scatter_avg' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_pperror6b-1.png){width=288}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there is no obvious pattern in the residuals.\n\n**Intervals**\n\nThese are plots of the observed data overlayed on top of posterior\npredictions associated with each level of the predictor. Ideally, the\nobserved data should all fall within the predictive intervals.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6b.brm2 |> pp_check(type = 'intervals', x = \"x\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'intervals' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ppintervals6b-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the posterior predictions are not inconsistent with the observed\n  data\n\n**Ribbon**\n\nThese are just an alternative way of expressing the interval plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6b.brm2 |> pp_check(type = 'ribbon', x = \"x\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'ribbon' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ppribbon6b-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the posterior predictions are not inconsistent with the observed\n  data\n\n:::: {.callout-tip collapse=\"true\"}\n\n####### Using `shiny` to explore a wide range of MCMC and validation diagnostics\n\nThe `shinystan` package allows the full suite of MCMC diagnostics and posterior\npredictive checks to be accessed via a web interface.\n\n\n\n::: {.cell mhidden='true'}\n\n```{.r .cell-code}\nlibrary(shinystan)\nlaunch_shinystan(dat6b.brm2)\n```\n:::\n\n\n\n::::\n\n\n##### DHARMa (simulated) residuals\n\nDHARMa residuals provide very useful diagnostics. Unfortunately, we\ncannot directly use the `simulateResiduals()` function to generate the\nsimulated residuals. However, if we are willing to calculate some of\nthe components yourself, we can still obtain the simulated residuals\nfrom the fitted stan model.\n\nWe need to supply:\n\n- simulated (predicted) responses associated with each observation.\n- observed values\n- fitted (predicted) responses (averaged) associated with each\n  observation\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6b.resids <- make_brms_dharma_res(dat6b.brm2, integerResponse = FALSE)\nwrap_elements(~ testUniformity(dat6b.resids)) +\n    wrap_elements(~ plotResiduals(dat6b.resids)) +\n    wrap_elements(~ testDispersion(dat6b.resids)) +\n    plot_layout(nrow = 1)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_DHARMa6b-1.png){width=8in}\n:::\n:::\n\n\n\n:::: {.callout-note}\n\nIf you are using Rstudio (particularly with a quarto document), the above code may produce a graphic that is too large for the currently available device.  The document should still render without problems, it is just the live display within Rstudio that cannot accommodate the figure.\n\nTo address this, you can either:\n\n- break the multi-panel figure up into four separate figures by\n  removing the outer `wrap_elements()` and `plot_layout()` functions.\n- copy the above code into the console and view in a larger graphics\n  device\n::::\n\n**Conclusions**:\n\n- the Q-Q plot looks reasonable (points broadly follow the angled\n  line)\n- there are no flagged issues with the\n  - KS test: conformity to the nominated distribution (family)\n  - Dispersion test: would not normally expect this to be an issue for\n    a Gaussian family unless there are other issues with the residuals\n  - Outlier test: the influence of each observation\n- there does not appear to be any patterns in the residuals - each of\n  the three quantile trends are considered flat and centered around\n  1/3, 1/2 and 2/3\n- the observed dispersion is well within the simulated range\n  indicating that there is no issue with dispersion (again this is\n  expected for a Gaussian model)\n\n:::::\n\n**Conclusions**:\n- there is no evidence of a lack of fit\n- the model is likely to be reliable\n\n#### Standardised predictor\n\n::::: {.panel-tabset}\n\n##### Simple residual plots\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid <- resid(dat6c.brm2)[, \"Estimate\"]\nfit <- fitted(dat6c.brm2)[, \"Estimate\"]\nggplot() +\n  geom_point(data = NULL, aes(y = resid, x = fit))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelvalidation_resid6c-1.png){width=384}\n:::\n:::\n\n\n\nWe should also plot the residuals against each of the predictor\nvariables (as well as any other important unmodelled predictors -\nparticularly time and space if relevant and available).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid <- resid(dat6c.brm2)[, \"Estimate\"]\nggplot() +\n  geom_point(data = NULL, aes(y = resid, x = dat6$x))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelvalidation_resid26c-1.png){width=384}\n:::\n:::\n\n\n\n**conclusions**:\n\n- there does not appear to be any pattern in the residuals\n\n##### Posterior Probability Checks\n\n**density overlay**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6c.brm2 |> pp_check(type = 'dens_overlay', ndraws = 100)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelvalidation_ppdensity6c-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the model draws appear to be consistent with the observed data\n\n**Error scatter**\n\nThese are plots of the observed values against the average residuals.\nSimilar to a residual plot, we do not want to see any patterns in this\nplot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6c.brm2 |> pp_check(type = 'error_scatter_avg')\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'error_scatter_avg' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_pperror6c-1.png){width=288}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there is no obvious pattern in the residuals.\n\n**Intervals**\n\nThese are plots of the observed data overlayed on top of posterior\npredictions associated with each level of the predictor. Ideally, the\nobserved data should all fall within the predictive intervals.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6c.brm2 |> pp_check(type = 'intervals', x = \"x\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'intervals' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ppintervals6c-1.png){width=576}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the posterior predictions are not inconsistent with the observed\n  data\n\n**Ribbon**\n\nThese are just an alternative way of expressing the interval plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6c.brm2 |> pp_check(type = 'ribbon', x = \"x\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing all posterior draws for ppc type 'ribbon' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_ppribbon6c-1.png){width=384}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the posterior predictions are not inconsistent with the observed\n  data\n\n:::: {.callout-tip collapse=\"true\"}\n\n####### Using `shiny` to explore a wide range of MCMC and validation diagnostics\n\nThe `shinystan` package allows the full suite of MCMC diagnostics and posterior\npredictive checks to be accessed via a web interface.\n\n\n\n::: {.cell mhidden='true'}\n\n```{.r .cell-code}\nlibrary(shinystan)\nlaunch_shinystan(dat6c.brm2)\n```\n:::\n\n\n\n::::\n\n\n##### DHARMa (simulated) residuals\n\nDHARMa residuals provide very useful diagnostics. Unfortunately, we\ncannot directly use the `simulateResiduals()` function to generate the\nsimulated residuals. However, if we are willing to calculate some of\nthe components yourself, we can still obtain the simulated residuals\nfrom the fitted stan model.\n\nWe need to supply:\n\n- simulated (predicted) responses associated with each observation.\n- observed values\n- fitted (predicted) responses (averaged) associated with each\n  observation\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6c.resids <- make_brms_dharma_res(dat6c.brm2, integerResponse = FALSE)\nwrap_elements(~ testUniformity(dat6c.resids)) +\n    wrap_elements(~ plotResiduals(dat6c.resids)) +\n    wrap_elements(~ testDispersion(dat6c.resids)) +\n    plot_layout(nrow = 1)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/modelValidation_DHARMa6c-1.png){width=8in}\n:::\n:::\n\n\n\n:::: {.callout-note}\n\nIf you are using Rstudio (particularly with a quarto document), the above code may produce a graphic that is too large for the currently available device.  The document should still render without problems, it is just the live display within Rstudio that cannot accommodate the figure.\n\nTo address this, you can either:\n\n- break the multi-panel figure up into four separate figures by\n  removing the outer `wrap_elements()` and `plot_layout()` functions.\n- copy the above code into the console and view in a larger graphics\n  device\n::::\n\n**Conclusions**:\n\n- the Q-Q plot looks reasonable (points broadly follow the angled\n  line)\n- there are no flagged issues with the\n  - KS test: conformity to the nominated distribution (family)\n  - Dispersion test: would not normally expect this to be an issue for\n    a Gaussian family unless there are other issues with the residuals\n  - Outlier test: the influence of each observation\n- there does not appear to be any patterns in the residuals - each of\n  the three quantile trends are considered flat and centered around\n  1/3, 1/2 and 2/3\n- the observed dispersion is well within the simulated range\n  indicating that there is no issue with dispersion (again this is\n  expected for a Gaussian model)\n\n:::::\n\n**Conclusions**:\n- there is no evidence of a lack of fit\n- the model is likely to be reliable\n\n:::::::\n\n::::\n\n# Partial effects plots\n\nPrior to exploring the modelled numerical estimates, it is worth\nreviewing simple plots of the predicted trends associated with each\npredictor. Importantly, they typically express the trends on the scale\nof the response, although for some, it is possible to force the trends\nto be expressed on the link scale. Such plots provides a final visual\ncheck of whether the model has yielded sensible outcomes. Furthermore,\nthey usually assist in the interpretation of the major estimated\nparameters.\n\n:::: {.panel-tabset}\n\n### Example 1 (Gaussian data)\n\n::::::: {.panel-tabset}\n#### Raw predictor\n::::: {.panel-tabset}\n\n##### Conditional effects\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1a.brm2 |>\n  conditional_effects() |>\n  plot(points = TRUE)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/partial_plots1a-1.png){width=384}\n:::\n\n```{.r .cell-code}\n#OR\ndat1a.brm2 |>\n    conditional_effects(spaghetti = TRUE, ndraws = 200) |>\n    plot(points = TRUE) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/partial_plots1a-2.png){width=384}\n:::\n:::\n\n\n\n:::::\n\n#### Centered predictor\n::::: {.panel-tabset}\n\n##### Conditional effects\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1b.brm2 |>\n  conditional_effects() |>\n  plot(points = TRUE)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/partial_plots1b-1.png){width=384}\n:::\n\n```{.r .cell-code}\n#OR\ndat1b.brm2 |>\n    conditional_effects(spaghetti = TRUE, ndraws = 200) |>\n    plot(points = TRUE) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/partial_plots1b-2.png){width=384}\n:::\n:::\n\n\n\nNotice that although we had centered the predictor, because we did so\nin the model formula, `conditional_effects` is able to backtransform\n$x$ onto the original scale when producing the partial plot.\n\n:::::\n\n#### Standardised predictor\n::::: {.panel-tabset}\n\n##### Conditional effects\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1c.brm2 |>\n  conditional_effects() |>\n  plot(points = TRUE)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/partial_plots1c-1.png){width=384}\n:::\n\n```{.r .cell-code}\n#OR\ndat1c.brm2 |>\n    conditional_effects(spaghetti = TRUE, ndraws = 200) |>\n    plot(points = TRUE) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/partial_plots1c-2.png){width=384}\n:::\n:::\n\n\n\nNotice that although we had centered the predictor, because we did so\nin the model formula, `conditional_effects` is able to backtransform\n$x$ onto the original scale when producing the partial plot.\n\n:::::\n\n:::::::\n\n### Example 2 (categorical predictor)\n\n::::::: {.panel-tabset}\n\n#### Treatment contrasts\n\n::::: {.panel-tabset}\n\n##### Conditional effects\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2a.brm2 |>\n  conditional_effects() |>\n  plot(points = TRUE)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/partial_plots2a-1.png){width=384}\n:::\n\n```{.r .cell-code}\n#OR\ndat2a.brm2 |>\n    conditional_effects(spaghetti = TRUE, ndraws = 200) |>\n    plot(points = TRUE) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/partial_plots2a-2.png){width=384}\n:::\n:::\n\n\n\n:::::\n\n#### Means parameterisation\n\n::::: {.panel-tabset}\n\n##### Conditional effects\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2b.brm2 |>\n  conditional_effects() |>\n  plot(points = TRUE)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/partial_plots2b-1.png){width=384}\n:::\n\n```{.r .cell-code}\n#OR\ndat2b.brm2 |>\n    conditional_effects(spaghetti = TRUE, ndraws = 200) |>\n    plot(points = TRUE) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/partial_plots2b-2.png){width=384}\n:::\n:::\n\n\n\n:::::\n\n:::::::\n\n### Example 3 (Poisson data)\n\n::::::: {.panel-tabset}\n#### Raw predictor\n::::: {.panel-tabset}\n\n##### Conditional effects\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3a.brm2 |>\n  conditional_effects() |>\n  plot(points = TRUE)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/partial_plots3a-1.png){width=384}\n:::\n\n```{.r .cell-code}\n#OR\ndat3a.brm2 |>\n    conditional_effects(spaghetti = TRUE, ndraws = 200) |>\n    plot(points = TRUE) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/partial_plots3a-2.png){width=384}\n:::\n:::\n\n\n\n:::::\n\n#### Centered predictor\n::::: {.panel-tabset}\n\n##### Conditional effects\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3b.brm2 |>\n  conditional_effects() |>\n  plot(points = TRUE)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/partial_plots3b-1.png){width=384}\n:::\n\n```{.r .cell-code}\n#OR\ndat3b.brm2 |>\n    conditional_effects(spaghetti = TRUE, ndraws = 200) |>\n    plot(points = TRUE) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/partial_plots3b-2.png){width=384}\n:::\n:::\n\n\n\nNotice that although we had centered the predictor, because we did so\nin the model formula, `conditional_effects` is able to backtransform\n$x$ onto the original scale when producing the partial plot.\n\n:::::\n\n#### Standardised predictor\n::::: {.panel-tabset}\n\n##### Conditional effects\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3c.brm2 |>\n  conditional_effects() |>\n  plot(points = TRUE)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/partial_plots3c-1.png){width=384}\n:::\n\n```{.r .cell-code}\n#OR\ndat3c.brm2 |>\n    conditional_effects(spaghetti = TRUE, ndraws = 200) |>\n    plot(points = TRUE) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/partial_plots3c-2.png){width=384}\n:::\n:::\n\n\n\nNotice that although we had centered the predictor, because we did so\nin the model formula, `conditional_effects` is able to backtransform\n$x$ onto the original scale when producing the partial plot.\n\n:::::\n\n:::::::\n\n### Example 4 (NB data)\n\n::::::: {.panel-tabset}\n#### Raw predictor\n::::: {.panel-tabset}\n\n##### Conditional effects\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4a.brm2 |>\n  conditional_effects() |>\n  plot(points = TRUE)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/partial_plots4a-1.png){width=384}\n:::\n\n```{.r .cell-code}\n#OR\ndat4a.brm2 |>\n    conditional_effects(spaghetti = TRUE, ndraws = 200) |>\n    plot(points = TRUE) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/partial_plots4a-2.png){width=384}\n:::\n:::\n\n\n\n:::::\n\n#### Centered predictor\n::::: {.panel-tabset}\n\n##### Conditional effects\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4b.brm2 |>\n  conditional_effects() |>\n  plot(points = TRUE)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/partial_plots4b-1.png){width=384}\n:::\n\n```{.r .cell-code}\n#OR\ndat4b.brm2 |>\n    conditional_effects(spaghetti = TRUE, ndraws = 200) |>\n    plot(points = TRUE) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/partial_plots4b-2.png){width=384}\n:::\n:::\n\n\n\nNotice that although we had centered the predictor, because we did so\nin the model formula, `conditional_effects` is able to backtransform\n$x$ onto the original scale when producing the partial plot.\n\n:::::\n\n#### Standardised predictor\n::::: {.panel-tabset}\n\n##### Conditional effects\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4c.brm2 |>\n  conditional_effects() |>\n  plot(points = TRUE)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/partial_plots4c-1.png){width=384}\n:::\n\n```{.r .cell-code}\n#OR\ndat4c.brm2 |>\n    conditional_effects(spaghetti = TRUE, ndraws = 200) |>\n    plot(points = TRUE) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/partial_plots4c-2.png){width=384}\n:::\n:::\n\n\n\nNotice that although we had centered the predictor, because we did so\nin the model formula, `conditional_effects` is able to backtransform\n$x$ onto the original scale when producing the partial plot.\n\n:::::\n\n:::::::\n\n### Example 5 (Binary data)\n\n::::::: {.panel-tabset}\n#### Raw predictor\n::::: {.panel-tabset}\n\n##### Conditional effects\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5a.brm2 |>\n  conditional_effects() |>\n  plot(points = TRUE)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/partial_plots5a-1.png){width=384}\n:::\n\n```{.r .cell-code}\n#OR\ndat5a.brm2 |>\n    conditional_effects(spaghetti = TRUE, ndraws = 200) |>\n    plot(points = TRUE) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/partial_plots5a-2.png){width=384}\n:::\n:::\n\n\n\n:::::\n\n#### Centered predictor\n::::: {.panel-tabset}\n\n##### Conditional effects\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5b.brm2 |>\n  conditional_effects() |>\n  plot(points = TRUE)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/partial_plots5b-1.png){width=384}\n:::\n\n```{.r .cell-code}\n#OR\ndat5b.brm2 |>\n    conditional_effects(spaghetti = TRUE, ndraws = 200) |>\n    plot(points = TRUE) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/partial_plots5b-2.png){width=384}\n:::\n:::\n\n\n\nNotice that although we had centered the predictor, because we did so\nin the model formula, `conditional_effects` is able to backtransform\n$x$ onto the original scale when producing the partial plot.\n\n:::::\n\n#### Standardised predictor\n::::: {.panel-tabset}\n\n##### Conditional effects\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5c.brm2 |>\n  conditional_effects() |>\n  plot(points = TRUE)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/partial_plots5c-1.png){width=384}\n:::\n\n```{.r .cell-code}\n#OR\ndat5c.brm2 |>\n    conditional_effects(spaghetti = TRUE, ndraws = 200) |>\n    plot(points = TRUE) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/partial_plots5c-2.png){width=384}\n:::\n:::\n\n\n\nNotice that although we had centered the predictor, because we did so\nin the model formula, `conditional_effects` is able to backtransform\n$x$ onto the original scale when producing the partial plot.\n\n:::::\n\n:::::::\n\n### Example 6 (Binomial data)\n\n::::::: {.panel-tabset}\n#### Raw predictor\n::::: {.panel-tabset}\n\n##### Conditional effects\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6a.brm2 |>\n  conditional_effects(conditions = data.frame(total = dat6$total)) |>\n  plot(points = TRUE)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/partial_plots6a-1.png){width=384}\n:::\n\n```{.r .cell-code}\n#OR\ndat6a.brm2 |>\n    conditional_effects(conditions = data.frame(total = dat6$total), \n    spaghetti = TRUE, ndraws = 200) |>\n    plot(points = TRUE) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/partial_plots6a-2.png){width=384}\n:::\n:::\n\n\n\n:::::\n\n#### Centered predictor\n::::: {.panel-tabset}\n\n##### Conditional effects\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6b.brm2 |>\n  conditional_effects(conditions = data.frame(total = dat6$total)) |>\n  plot(points = TRUE)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/partial_plots6b-1.png){width=384}\n:::\n\n```{.r .cell-code}\n#OR\ndat6b.brm2 |>\n    conditional_effects(conditions = data.frame(total = dat6$total), \n    spaghetti = TRUE, ndraws = 200) |>\n    plot(points = TRUE) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/partial_plots6b-2.png){width=384}\n:::\n:::\n\n\n\nNotice that although we had centered the predictor, because we did so\nin the model formula, `conditional_effects` is able to backtransform\n$x$ onto the original scale when producing the partial plot.\n\n:::::\n\n#### Standardised predictor\n::::: {.panel-tabset}\n\n##### Conditional effects\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6c.brm2 |>\n  conditional_effects(conditions = data.frame(total = dat6$total)) |>\n  plot(points = TRUE)\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/partial_plots6c-1.png){width=384}\n:::\n\n```{.r .cell-code}\n#OR\ndat6c.brm2 |>\n    conditional_effects(conditions = data.frame(total = dat6$total), \n    spaghetti = TRUE, ndraws = 200) |>\n    plot(points = TRUE) \n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/partial_plots6c-2.png){width=384}\n:::\n:::\n\n\n\nNotice that although we had centered and scaled the predictor, because we did so\nin the model formula, `conditional_effects` is able to backtransform\n$x$ onto the original scale when producing the partial plot.\n\n:::::\n\n:::::::\n\n::::\n\n\n# Model investigation\n\nRather than simply return point estimates of each of the model\nparameters, Bayesian analyses capture the full posterior of each\nparameter. These are typically stored within the `list` structure of\nthe output object.\n\nAs with most statistical routines, the overloaded `summary()` function\nprovides an overall summary of the model parameters. Typically, the\nsummaries will include the means / medians along with credibility\nintervals and perhaps convergence diagnostics (such as R hat).\nHowever, more thorough investigation and analysis of the parameter\nposteriors requires access to the full posteriors.\n\nThere is currently a plethora of functions for extracting the full\nposteriors from models. In part, this is a reflection of a rapidly\nevolving space with numerous packages providing near equivalent\nfunctionality (it should also be noted, that over time, many of the\nfunctions have been deprecated due to inconsistencies in their names).\nBroadly speaking, the functions focus on draws from the posterior of\neither the parameters (intercept, slope, standard deviation etc),\n**linear predictor**, **expected values** or **predicted values**. The\ndistinction between the latter three are highlighted in the following\ntable.\n\n| Property          | Description                                                                                  |\n|-------------------|----------------------------------------------------------------------------------------------|\n| linear predictors | values predicted on the link scale                                                           |\n| expected values   | predictions (on response scale) without residual error (predicting expected mean outcome(s)) |\n| predicted values  | predictions (on response scale) that incorporate residual error                              |\n| fitted values     | predictions on the response scale                                                            |\n\n: {.primary .bordered .sm .paramsTable tbl-colwidths=\"[30, 70]\"}\n\n\n:::: {.panel-tabset}\n\n### Example 1 (Gaussian data)\n\n::::::: {.panel-tabset}\n#### Raw predictor\n::::: {.panel-tabset}\n\n##### Summary\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1a.brm2 |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ x \n   Data: dat (Number of observations: 10) \n  Draws: 3 chains, each with iter = 5000; warmup = 1000; thin = 5;\n         total post-warmup draws = 2400\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.27      0.38    -0.46     1.06 1.00     2134     2352\nx            -0.08      0.46    -0.97     0.82 1.00     2459     2330\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.12      0.36     0.66     2.07 1.00     2402     2248\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `Intercept`: when $x=0$, the expected value of $y$ is \n  0.266 and we are 95% confident that the true\n  value is between -0.461 and \n  1.057. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. So for every one unit change in $x$, $y$ increases by (on\n  average) -0.08 units and we are 95%\n  confident that this change is between -0.967\n  and 0.816\n- sigma is estimated to be 1.12\n\n**Note, the estimates are means and quantiles.**\n\n##### Summarise draws\n\nAs an alternative to the regular summary (which is designed to\nresemble that traditionally provided by frequentist analyses - and\nthus be instantly familiar), it is possible to define exactly how each\nof the parameters are summarised.\n\nIn the following, I am nominating that I want to summarise each\nparameter posterior by:\n\n- the median\n- the 95% highest probability density interval (credibility interval)\n- Rhat\n- total number of draws\n- bulk and tail effective sample sizes\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1a.brm2 |>\n  posterior::summarise_draws(\n    median,\n    HDInterval::hdi,\n    rhat,\n    length,\n    ess_bulk, ess_tail\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 9 × 8\n  variable          median     lower   upper  rhat length ess_bulk ess_tail\n  <chr>              <dbl>     <dbl>   <dbl> <dbl>  <dbl>    <dbl>    <dbl>\n1 b_Intercept       0.262   -0.501     1.01   1.00   2400    2134.    2352.\n2 b_x              -0.0803  -0.973     0.806  1.00   2400    2459.    2330.\n3 sigma             1.03     0.588     1.84   1.00   2400    2402.    2248.\n4 Intercept         0.277   -0.443     1.04   1.00   2400    2085.    2348.\n5 prior_Intercept  30.9     -0.201    59.8    1.00   2400    2231.    2286.\n6 prior_b           0.0169 -13.1      11.0    1.00   2400    1968.    2220.\n7 prior_sigma      12.0      0.00537  46.8    1.00   2400    2399.    2330.\n8 lprior          -11.2    -11.3     -11.1    1.00   2400    2148.    2103.\n9 lp__            -25.1    -28.6     -23.7    1.00   2400    2241.    2233.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `b_Intercept`: when $x=0$, the expected value of $y$ is \n  0.262 and we are 95% confident that the true\n  value is between -0.501 and \n  1.013. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `b_x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. So for every one unit change in $x$, $y$ increases by (on\n  average) -0.08 units and we are 95%\n  confident that this change is between -0.973\n  and 0.806\n- sigma is estimated to be 1.03\n\n##### As draws\n\nAs yet a more flexible alternative, if we first extract the full\nposterior draws (with `as_draws_df()`), we can then use the various\n`tidyverse` functions to focus on just the most ecologically\ninterpretable parameters before summarising.\n\nIn the following, I will use `select()` with a regex (regular\nexpression) to match only the columns that:\n\n- start with (`^`) \"b_\" followed by any amount of (`*`) any character\n  (`.`)\n- start with (`^`) \"sigma\"\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1a.brm2 |>\n    brms::as_draws_df() |>\n    dplyr::select(matches(\"^b_.*|^sigma\")) |>\n    posterior::summarise_draws(\n        median,\n        HDInterval::hdi,\n        rhat,\n        length,\n        ess_bulk, ess_tail\n    )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 8\n  variable     median  lower upper  rhat length ess_bulk ess_tail\n  <chr>         <dbl>  <dbl> <dbl> <dbl>  <dbl>    <dbl>    <dbl>\n1 b_Intercept  0.262  -0.501 1.01   1.00   2400    2100.    2344.\n2 b_x         -0.0803 -0.973 0.806  1.00   2400    2454.    2322.\n3 sigma        1.03    0.588 1.84   1.00   2400    2402.    2236.\n```\n\n\n:::\n:::\n\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `b_Intercept`: when $x=0$, the expected value of $y$ is \n  0.262 and we are 95% confident that the true\n  value is between -0.501 and \n  1.013. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `b_x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. So for every one unit change in $x$, $y$ increases by (on\n  average) -0.08 units and we are 95%\n  confident that this change is between -0.973\n  and 0.806\n- sigma is estimated to be 1.03\n\n##### Density plots (via `gather_draws`)\n\nThe `gather_draws()` function performs the equivalent of an\n`as_draws_df()` followed by a `pivot_longer()` in order to return the\nfull posteriors in long format where they are more suitable for\ngraphing.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1a.brm2 |>\n    gather_draws(b_Intercept, b_x, sigma) |>\n    ggplot() +\n    geom_histogram(aes(x = .value)) +\n    facet_wrap(~.variable, scales = \"free\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/gather_draws1a-1.png){width=768}\n:::\n:::\n\n\n\nAlternatively, there are various other representations supported by\nthe `ggdist` package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1a.brm2 |>\n  gather_draws(b_Intercept, b_x, sigma) |>\n  ggplot() +\n  stat_halfeye(aes(x = .value, y = .variable))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/gather_draws1a2-1.png){width=480}\n:::\n:::\n\n\n\n##### R²\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1a.brm2 |>\n    bayes_R2(summary = FALSE) |>\n    median_hdci()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           y         ymin     ymax .width .point .interval\n1 0.06517101 2.242159e-09 0.313515   0.95 median      hdci\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- 6.517% of the total variability in $y$ can be\n  explained by its relationship to $x$\n- we are 95% confident that the strength of this relationship is\n  between 0% and 31.352%\n  \n:::::\n\n#### Centered predictor\n::::: {.panel-tabset}\n\n##### Summary\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1b.brm2 |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ scale(x, scale = FALSE) \n   Data: dat (Number of observations: 10) \n  Draws: 3 chains, each with iter = 5000; warmup = 1000; thin = 5;\n         total post-warmup draws = 2400\n\nRegression Coefficients:\n                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept              0.28      0.36    -0.40     0.98 1.00     2309     2290\nscalexscaleEQFALSE    -0.09      0.46    -0.97     0.85 1.00     2380     2251\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.10      0.34     0.66     1.94 1.00     2363     2409\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `Intercept`: when $x=0$ (its average since it is centered), the expected value of $y$ is \n  0.283 and we are 95% confident that the true\n  value is between -0.404 and \n  0.981. So $y$ is expected to be \n  0.283 at the average $x$.\n- `x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. So for every one unit change in $x$, $y$ increases by (on\n  average) -0.089 units and we are 95%\n  confident that this change is between -0.971\n  and 0.849\n- sigma is estimated to be 1.1\n\n**Note, the estimates are means and quantiles.**\n\n##### Summarise draws\n\nAs an alternative to the regular summary (which is designed to\nresemble that traditionally provided by frequentist analyses - and\nthus be instantly familiar), it is possible to define exactly how each\nof the parameters are summarised.\n\nIn the following, I am nominating that I want to summarise each\nparameter posterior by:\n\n- the median\n- the 95% highest probability density interval (credibility interval)\n- Rhat\n- total number of draws\n- bulk and tail effective sample sizes\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1b.brm2 |>\n  posterior::summarise_draws(\n    median,\n    HDInterval::hdi,\n    rhat,\n    length,\n    ess_bulk, ess_tail\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 9 × 8\n  variable               median     lower   upper  rhat length ess_bulk ess_tail\n  <chr>                   <dbl>     <dbl>   <dbl> <dbl>  <dbl>    <dbl>    <dbl>\n1 b_Intercept            0.277   -0.437     0.938  1.00   2400    2309.    2290.\n2 b_scalexscaleEQFALSE  -0.0841  -0.997     0.803  1.00   2400    2381.    2251.\n3 sigma                  1.03     0.596     1.76   1.00   2400    2364.    2409.\n4 Intercept              0.277   -0.437     0.938  1.00   2400    2309.    2290.\n5 prior_Intercept       31.7      1.66     60.9    1.00   2400    2219.    2150.\n6 prior_b               -0.0664 -12.9      13.2    1.00   2400    2408.    2328.\n7 prior_sigma           11.2      0.00997  49.3    1.00   2400    2373.    2459.\n8 lprior               -11.2    -11.3     -11.1    1.00   2400    2447.    2293.\n9 lp__                 -25.0    -28.2     -23.7    1.00   2400    2069.    2035.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `b_Intercept`: when $x=0$ (its average since it is centered), the expected value of $y$ is \n  0.277 and we are 95% confident that the true\n  value is between -0.437 and \n  0.938. So $y$ is expected to be \n  0.283 at the average $x$.\n- `b_x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. So for every one unit change in $x$, $y$ increases by (on\n  average) -0.084 units and we are 95%\n  confident that this change is between -0.997\n  and 0.803\n- sigma is estimated to be 1.03\n\n##### As draws\n\nAs yet a more flexible alternative, if we first extract the full\nposterior draws (with `as_draws_df()`), we can then use the various\n`tidyverse` functions to focus on just the most ecologically\ninterpretable parameters before summarising.\n\nIn the following, I will use `select()` with a regex (regular\nexpression) to match only the columns that:\n\n- start with (`^`) \"b_\" followed by any amount of (`*`) any character\n  (`.`)\n- start with (`^`) \"sigma\"\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1b.brm2 |>\n    brms::as_draws_df() |>\n    dplyr::select(matches(\"^b_.*|^sigma\")) |>\n    posterior::summarise_draws(\n        median,\n        HDInterval::hdi,\n        rhat,\n        length,\n        ess_bulk, ess_tail\n    )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 8\n  variable              median  lower upper  rhat length ess_bulk ess_tail\n  <chr>                  <dbl>  <dbl> <dbl> <dbl>  <dbl>    <dbl>    <dbl>\n1 b_Intercept           0.277  -0.437 0.938  1.00   2400    2270.    2282.\n2 b_scalexscaleEQFALSE -0.0841 -0.997 0.803  1.00   2400    2371.    2243.\n3 sigma                 1.03    0.596 1.76   1.00   2400    2362.    2402.\n```\n\n\n:::\n:::\n\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `b_Intercept`: when $x=0$ (its average since it is centered), the expected value of $y$ is \n  0.277 and we are 95% confident that the true\n  value is between -0.437 and \n  0.938. So $y$ is expected to be \n  0.283 at the average $x$.\n- `b_x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. So for every one unit change in $x$, $y$ increases by (on\n  average) -0.084 units and we are 95%\n  confident that this change is between -0.997\n  and 0.803\n- sigma is estimated to be 1.03\n\n##### Density plots (via `gather_draws`)\n\nThe `gather_draws()` function performs the equivalent of an\n`as_draws_df()` followed by a `pivot_longer()` in order to return the\nfull posteriors in long format where they are more suitable for\ngraphing. Note, that the name of the slope parameter gets very awkward\nwhen $x$ is centered, so it is more convenient to refer to this\nparameter via a regular expression.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1b.brm2 |> get_variables()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"          \"b_scalexscaleEQFALSE\" \"sigma\"               \n [4] \"Intercept\"            \"prior_Intercept\"      \"prior_b\"             \n [7] \"prior_sigma\"          \"lprior\"               \"lp__\"                \n[10] \"accept_stat__\"        \"stepsize__\"           \"treedepth__\"         \n[13] \"n_leapfrog__\"         \"divergent__\"          \"energy__\"            \n```\n\n\n:::\n\n```{.r .cell-code}\ndat1b.brm2 |>\n  gather_draws(`b_Intercept`,`b_.*x.*`, `sigma`, regex = TRUE) |> \n  ggplot() +\n  geom_histogram(aes(x = .value)) +\n  facet_wrap(~.variable, scales = \"free\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/gather_draws1b-1.png){width=768}\n:::\n:::\n\n\n\nAlternatively, there are various other representations supported by\nthe `ggdist` package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1b.brm2 |>\n  gather_draws(`b_Intercept`,`b_.*x.*`, `sigma`, regex = TRUE) |> \n  ggplot() +\n  stat_halfeye(aes(x = .value, y = .variable))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/gather_draws1b2-1.png){width=480}\n:::\n:::\n\n\n\n##### R²\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1b.brm2 |>\n    bayes_R2(summary = FALSE) |>\n    median_hdci()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           y         ymin      ymax .width .point .interval\n1 0.06623776 2.645785e-07 0.3103406   0.95 median      hdci\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- 6.624% of the total variability in $y$ can be\n  explained by its relationship to $x$\n- we are 95% confident that the strength of this relationship is\n  between 0% and 31.034%\n  \n:::::\n\n#### Standardised predictor\n::::: {.panel-tabset}\n\n##### Summary\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1c.brm2 |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ scale(x) \n   Data: dat (Number of observations: 10) \n  Draws: 3 chains, each with iter = 5000; warmup = 1000; thin = 5;\n         total post-warmup draws = 2400\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.28      0.37    -0.43     1.02 1.00     2230     1989\nscalex       -0.07      0.41    -0.87     0.71 1.00     2201     2292\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.14      0.37     0.65     2.01 1.00     2300     2224\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `Intercept`: when $x=0$ (its average since it is standardised), the expected value of $y$ is \n  0.284 and we are 95% confident that the true\n  value is between -0.428 and \n  1.017. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. Recall that since $x$ is standardised, 1 unit representes a span of 1 standard deviation of $x$. So for every one standard deviation unit change in $x$, $y$ increases by (on\n  average) -0.069 units and we are 95%\n  confident that this change is between -0.87\n  and 0.712\n- sigma is estimated to be 1.14\n\n**Note, the estimates are means and quantiles.**\n\n##### Summarise draws\n\nAs an alternative to the regular summary (which is designed to\nresemble that traditionally provided by frequentist analyses - and\nthus be instantly familiar), it is possible to define exactly how each\nof the parameters are summarised.\n\nIn the following, I am nominating that I want to summarise each\nparameter posterior by:\n\n- the median\n- the 95% highest probability density interval (credibility interval)\n- Rhat\n- total number of draws\n- bulk and tail effective sample sizes\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1c.brm2 |>\n  posterior::summarise_draws(\n    median,\n    HDInterval::hdi,\n    rhat,\n    length,\n    ess_bulk, ess_tail\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 9 × 8\n  variable          median       lower   upper  rhat length ess_bulk ess_tail\n  <chr>              <dbl>       <dbl>   <dbl> <dbl>  <dbl>    <dbl>    <dbl>\n1 b_Intercept       0.279   -0.426       1.02  1.00    2400    2230.    1989.\n2 b_scalex         -0.0750  -0.813       0.752 1.00    2400    2201.    2292.\n3 sigma             1.06     0.550       1.80  0.999   2400    2300.    2224.\n4 Intercept         0.279   -0.426       1.02  1.00    2400    2230.    1989.\n5 prior_Intercept  31.8      3.60       62.5   1.00    2400    2343.    2023.\n6 prior_b          -0.359  -44.6        46.7   1.00    2400    2334.    2368.\n7 prior_sigma      11.6      0.0000123  47.0   1.00    2400    2421.    2290.\n8 lprior          -12.5    -12.6       -12.4   1.00    2400    2226.    1977.\n9 lp__            -26.5    -29.6       -25.0   1.00    2400    2390.    2328.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `b_Intercept`: when $x=0$ (its average since it is standardised), the expected value of $y$ is \n  0.279 and we are 95% confident that the true\n  value is between -0.426 and \n  1.019. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `b_x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. Recall that since $x$ is standardised, 1 unit representes a span of 1 standard deviation of $x$. So for every one standard deviation unit change in $x$, $y$ increases by (on\n  average)-0.075 units and we are 95%\n  confident that this change is between -0.813\n  and 0.752\n- sigma is estimated to be 1.06\n\n##### As draws\n\nAs yet a more flexible alternative, if we first extract the full\nposterior draws (with `as_draws_df()`), we can then use the various\n`tidyverse` functions to focus on just the most ecologically\ninterpretable parameters before summarising.\n\nIn the following, I will use `select()` with a regex (regular\nexpression) to match only the columns that:\n\n- start with (`^`) \"b_\" followed by any amount of (`*`) any character\n  (`.`)\n- start with (`^`) \"sigma\"\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1c.brm2 |>\n    brms::as_draws_df() |>\n    dplyr::select(matches(\"^b_.*|^sigma\")) |>\n    posterior::summarise_draws(\n        median,\n        HDInterval::hdi,\n        rhat,\n        length,\n        ess_bulk, ess_tail\n    )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 8\n  variable     median  lower upper  rhat length ess_bulk ess_tail\n  <chr>         <dbl>  <dbl> <dbl> <dbl>  <dbl>    <dbl>    <dbl>\n1 b_Intercept  0.279  -0.426 1.02   1.00   2400    2235.    1985.\n2 b_scalex    -0.0750 -0.813 0.752  1.00   2400    2193.    2280.\n3 sigma        1.06    0.550 1.80   1.00   2400    2290.    2197.\n```\n\n\n:::\n:::\n\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `b_Intercept`: when $x=0$ (its average since it is standardised), the expected value of $y$ is \n  0.279 and we are 95% confident that the true\n  value is between -0.426 and \n  1.019. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `b_x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. Recall that since $x$ is standardised, 1 unit representes a span of 1 standard deviation of $x$. So for every one standard deviation unit change in $x$, $y$ increases by (on\n  average) -0.075 units and we are 95%\n  confident that this change is between -0.813\n  and 0.752\n- sigma is estimated to be 1.06\n\n##### Density plots (via `gather_draws`)\n\nThe `gather_draws()` function performs the equivalent of an\n`as_draws_df()` followed by a `pivot_longer()` in order to return the\nfull posteriors in long format where they are more suitable for\ngraphing. Note, that the name of the slope parameter gets very awkward\nwhen $x$ is centered, so it is more convenient to refer to this\nparameter via a regular expression.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1c.brm2 |> get_variables()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"     \"b_scalex\"        \"sigma\"           \"Intercept\"      \n [5] \"prior_Intercept\" \"prior_b\"         \"prior_sigma\"     \"lprior\"         \n [9] \"lp__\"            \"accept_stat__\"   \"stepsize__\"      \"treedepth__\"    \n[13] \"n_leapfrog__\"    \"divergent__\"     \"energy__\"       \n```\n\n\n:::\n\n```{.r .cell-code}\ndat1c.brm2 |>\n  gather_draws(`b_Intercept`,`b_.*x`, `sigma`, regex = TRUE) |> \n  ggplot() +\n  geom_histogram(aes(x = .value)) +\n  facet_wrap(~.variable, scales = \"free\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/gather_draws1c-1.png){width=768}\n:::\n:::\n\n\n\nAlternatively, there are various other representations supported by\nthe `ggdist` package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1c.brm2 |>\n  gather_draws(`b_Intercept`,`b_.*x`, `sigma`, regex = TRUE) |> \n  ggplot() +\n  stat_halfeye(aes(x = .value, y = .variable))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/gather_draws1c2-1.png){width=480}\n:::\n:::\n\n\n\n##### R²\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1c.brm2 |>\n    bayes_R2(summary = FALSE) |>\n    median_hdci()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           y         ymin      ymax .width .point .interval\n1 0.06728658 1.499005e-09 0.3197585   0.95 median      hdci\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- 6.729% of the total variability in $y$ can be\n  explained by its relationship to $x$\n- we are 95% confident that the strength of this relationship is\n  between 0% and 31.976%\n  \n:::::\n\n:::::::\n\n### Example 2 (categorical predictor)\n\n::::::: {.panel-tabset}\n#### Treatment contrasts\n::::: {.panel-tabset}\n\n##### Summary\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2a.brm2 |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ x \n   Data: dat2 (Number of observations: 12) \n  Draws: 3 chains, each with iter = 5000; warmup = 1000; thin = 5;\n         total post-warmup draws = 2400\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    20.89      2.03    16.62    24.89 1.00     2291     2382\nxmedium      -0.83      2.85    -6.31     5.12 1.00     2347     2180\nxhigh        -8.72      3.09   -14.74    -2.34 1.00     2321     2345\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     4.48      1.11     2.86     7.18 1.00     2605     2498\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `Intercept`: when $x$ is \"control\", the expected value of $y$ is \n  20.89 and we are 95% confident that the true\n  value is between 16.616 and \n  24.891.\n- `x*`: (the slopes) - the change (effect) in $y$ between the first (control) group unit (=1) and each other $x$ level. \n  - `xmedium`: $y$ is (on\n  average) 0.829 units (95%\n  confident that this change is between -6.312\n  and 5.116 less in the \"medium\" group compared to the \"control\" group.\n  - `xhigh`: $y$ is (on\n  average) 8.715 units and we are 95%\n  confident that this change is between -14.736\n  and -2.344 less in the \"high\" group compared to the \"control\" group.\n- sigma is estimated to be 4.48\n\n**Note, the estimates are means and quantiles.**\n\n##### Summarise draws\n\nAs an alternative to the regular summary (which is designed to\nresemble that traditionally provided by frequentist analyses - and\nthus be instantly familiar), it is possible to define exactly how each\nof the parameters are summarised.\n\nIn the following, I am nominating that I want to summarise each\nparameter posterior by:\n\n- the median\n- the 95% highest probability density interval (credibility interval)\n- Rhat\n- total number of draws\n- bulk and tail effective sample sizes\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2a.brm2 |>\n  posterior::summarise_draws(\n    median,\n    HDInterval::hdi,\n    rhat,\n    length,\n    ess_bulk, ess_tail\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 8\n   variable          median     lower  upper  rhat length ess_bulk ess_tail\n   <chr>              <dbl>     <dbl>  <dbl> <dbl>  <dbl>    <dbl>    <dbl>\n 1 b_Intercept      20.9     17.0      25.1   1.00   2400    2290.    2382.\n 2 b_xmedium        -0.889   -6.44      4.89  1.00   2400    2347.    2180.\n 3 b_xhigh          -8.73   -14.8      -2.38  1.00   2400    2320.    2345.\n 4 sigma             4.30     2.67      6.71  1.00   2400    2605.    2498.\n 5 Intercept        17.7     15.7      20.0   1.00   2400    2485.    2204.\n 6 prior_Intercept  19.7     16.1      23.2   1.00   2400    2516.    2283.\n 7 prior_b           0.0306 -18.9      20.7   1.00   2400    2384.    2381.\n 8 prior_sigma       3.55     0.00499  14.0   1.00   2400    2374.    2369.\n 9 lprior          -11.4    -13.1     -10.1   1.00   2400    2361.    2414.\n10 lp__            -44.3    -47.8     -42.5   1.00   2400    2231.    2367.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `Intercept`: when $x$ is \"control\", the expected value of $y$ is \n  20.875 and we are 95% confident that the true\n  value is between 16.993 and \n  25.064.\n- `x*`: (the slopes) - the change (effect) in $y$ between the first (control) group unit (=1) and each other $x$ level. \n  - `xmedium`: $y$ is (on\n  average) 0.889 units (95%\n  confident that this change is between -6.445\n  and 4.886 less in the \"medium\" group compared to the \"control\" group.\n  - `xhigh`: $y$ is (on\n  average) 8.726 units and we are 95%\n  confident that this change is between -14.756\n  and -2.383 less in the \"high\" group compared to the \"control\" group.\n- sigma is estimated to be 4.48\n\n##### As draws\n\nAs yet a more flexible alternative, if we first extract the full\nposterior draws (with `as_draws_df()`), we can then use the various\n`tidyverse` functions to focus on just the most ecologically\ninterpretable parameters before summarising.\n\nIn the following, I will use `select()` with a regex (regular\nexpression) to match only the columns that:\n\n- start with (`^`) \"b_\" followed by any amount of (`*`) any character\n  (`.`)\n- start with (`^`) \"sigma\"\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2a.brm2 |>\n    brms::as_draws_df() |>\n    dplyr::select(matches(\"^b_.*|^sigma\")) |>\n    posterior::summarise_draws(\n        median,\n        HDInterval::hdi,\n        rhat,\n        length,\n        ess_bulk, ess_tail\n    )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 8\n  variable    median  lower upper  rhat length ess_bulk ess_tail\n  <chr>        <dbl>  <dbl> <dbl> <dbl>  <dbl>    <dbl>    <dbl>\n1 b_Intercept 20.9    17.0  25.1   1.00   2400    2270.    2367.\n2 b_xmedium   -0.889  -6.44  4.89  1.00   2400    2329.    2158.\n3 b_xhigh     -8.73  -14.8  -2.38  1.00   2400    2294.    2322.\n4 sigma        4.30    2.67  6.71  1.00   2400    2597.    2492.\n```\n\n\n:::\n:::\n\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `Intercept`: when $x$ is \"control\", the expected value of $y$ is \n  20.875 and we are 95% confident that the true\n  value is between 16.993 and \n  25.064.\n- `x*`: (the slopes) - the change (effect) in $y$ between the first (control) group unit (=1) and each other $x$ level. \n  - `xmedium`: $y$ is (on\n  average) 0.889 units (95%\n  confident that this change is between -6.445\n  and 4.886 less in the \"medium\" group compared to the \"control\" group.\n  - `xhigh`: $y$ is (on\n  average) 8.726 units and we are 95%\n  confident that this change is between -14.756\n  and -2.383 less in the \"high\" group compared to the \"control\" group.\n- sigma is estimated to be 4.48\n\n##### Density plots (via `gather_draws`)\n\nThe `gather_draws()` function performs the equivalent of an\n`as_draws_df()` followed by a `pivot_longer()` in order to return the\nfull posteriors in long format where they are more suitable for\ngraphing.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2a.brm2 |> get_variables()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"     \"b_xmedium\"       \"b_xhigh\"         \"sigma\"          \n [5] \"Intercept\"       \"prior_Intercept\" \"prior_b\"         \"prior_sigma\"    \n [9] \"lprior\"          \"lp__\"            \"accept_stat__\"   \"stepsize__\"     \n[13] \"treedepth__\"     \"n_leapfrog__\"    \"divergent__\"     \"energy__\"       \n```\n\n\n:::\n\n```{.r .cell-code}\ndat2a.brm2 |>\n    gather_draws(`b_Intercept`, `b_x.*`, `sigma`, regex = TRUE) |>\n    ggplot() +\n    geom_histogram(aes(x = .value)) +\n    facet_wrap(~.variable, scales = \"free\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/gather_draws2a-1.png){width=768}\n:::\n:::\n\n\n\nAlternatively, there are various other representations supported by\nthe `ggdist` package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2a.brm2 |>\n  gather_draws(`b_x.*`, regex = TRUE) |>\n  ggplot() +\n  stat_halfeye(aes(x = .value, y = .variable))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/gather_draws2a2-1.png){width=480}\n:::\n:::\n\n\n\n##### R²\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2a.brm2 |>\n    bayes_R2(summary = FALSE) |>\n    median_hdci()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          y      ymin      ymax .width .point .interval\n1 0.5424268 0.1636153 0.7253359   0.95 median      hdci\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- 54.243% of the total variability in $y$ can be\n  explained by its relationship to $x$\n- we are 95% confident that the strength of this relationship is\n  between 16.362% and 72.534%\n \n\n:::::\n\n#### Means parameterisation\n::::: {.panel-tabset}\n\n##### Summary\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2b.brm2 |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ -1 + x \n   Data: dat2 (Number of observations: 12) \n  Draws: 3 chains, each with iter = 5000; warmup = 1000; thin = 5;\n         total post-warmup draws = 2400\n\nRegression Coefficients:\n         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nxcontrol    20.26      2.17    15.79    24.43 1.00     2277     2411\nxmedium     18.68      2.15    14.38    23.03 1.00     2406     2232\nxhigh       11.10      2.23     6.67    15.83 1.00     2547     2347\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     4.45      1.14     2.82     7.24 1.00     2385     2312\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `x*`: (the group means). \n  - `xcontrol`: the expected value of $y$ in the \"control\" group is \n    2.174 (95% credibility interval is\n    between 15.788 and \n    24.425)\n  - `xmedium`: the expected value of $y$ in the \"control\" group is \n    2.15 (95% credibility interval is\n    between 14.383 and \n    23.026)\n  - `xhigh`: the expected value of $y$ in the \"control\" group is \n    2.231 (95% credibility interval is\n    between 6.669 and \n    15.832)\n- sigma is estimated to be 4.45\n\n**Note, the estimates are means and quantiles.**\n\n##### Summarise draws\n\nAs an alternative to the regular summary (which is designed to\nresemble that traditionally provided by frequentist analyses - and\nthus be instantly familiar), it is possible to define exactly how each\nof the parameters are summarised.\n\nIn the following, I am nominating that I want to summarise each\nparameter posterior by:\n\n- the median\n- the 95% highest probability density interval (credibility interval)\n- Rhat\n- total number of draws\n- bulk and tail effective sample sizes\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2b.brm2 |>\n  posterior::summarise_draws(\n    median,\n    HDInterval::hdi,\n    rhat,\n    length,\n    ess_bulk, ess_tail\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 8 × 8\n  variable    median      lower  upper  rhat length ess_bulk ess_tail\n  <chr>        <dbl>      <dbl>  <dbl> <dbl>  <dbl>    <dbl>    <dbl>\n1 b_xcontrol   20.3   15.8       24.5  0.999   2400    2278.    2411.\n2 b_xmedium    18.6   14.7       23.4  1.00    2400    2406.    2232.\n3 b_xhigh      11.0    6.49      15.6  1.00    2400    2547.    2347.\n4 sigma         4.26   2.52       6.64 1.00    2400    2385.    2312.\n5 prior_b      16.2   -6.13      36.0  0.999   2400    2363.    2497.\n6 prior_sigma   3.53   0.000386  13.9  1.00    2400    2463.    2124.\n7 lprior      -11.8  -12.8      -11.1  1.00    2400    2174.    2293.\n8 lp__        -44.6  -48.3      -42.7  1.00    2400    2347.    2301.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `x*`: (the means of each group) \n  - `xcontrol`: the expected value of $y$ in the \"control\" group is \n    20.276 (95% credibility interval is\n    between 15.847 and \n    24.478)\n  - `xmedium`: the expected value of $y$ in the \"control\" group is \n    18.635 (95% credibility interval is\n    between 14.745 and \n    23.371)\n  - `xhigh`: the expected value of $y$ in the \"control\" group is \n    11.028 (95% credibility interval is\n    between 6.491 and \n    15.58)\n- sigma is estimated to be 4.45\n\n##### As draws\n\nAs yet a more flexible alternative, if we first extract the full\nposterior draws (with `as_draws_df()`), we can then use the various\n`tidyverse` functions to focus on just the most ecologically\ninterpretable parameters before summarising.\n\nIn the following, I will use `select()` with a regex (regular\nexpression) to match only the columns that:\n\n- start with (`^`) \"b_\" followed by any amount of (`*`) any character\n  (`.`)\n- start with (`^`) \"sigma\"\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2b.brm2 |>\n    brms::as_draws_df() |>\n    dplyr::select(matches(\"^b_.*|^sigma\")) |>\n    posterior::summarise_draws(\n        median,\n        HDInterval::hdi,\n        rhat,\n        length,\n        ess_bulk, ess_tail\n    )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 8\n  variable   median lower upper  rhat length ess_bulk ess_tail\n  <chr>       <dbl> <dbl> <dbl> <dbl>  <dbl>    <dbl>    <dbl>\n1 b_xcontrol  20.3  15.8  24.5   1.00   2400    2266.    2404.\n2 b_xmedium   18.6  14.7  23.4   1.00   2400    2387.    2190.\n3 b_xhigh     11.0   6.49 15.6   1.00   2400    2539.    2305.\n4 sigma        4.26  2.52  6.64  1.00   2400    2380.    2290.\n```\n\n\n:::\n:::\n\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `x*`: (the means of each group) \n  - `xcontrol`: the expected value of $y$ in the \"control\" group is \n    20.276 (95% credibility interval is\n    between 15.847 and \n    24.478)\n  - `xmedium`: the expected value of $y$ in the \"control\" group is \n    18.635 (95% credibility interval is\n    between 14.745 and \n    23.371)\n  - `xhigh`: the expected value of $y$ in the \"control\" group is \n    11.028 (95% credibility interval is\n    between 6.491 and \n    15.58)\n- sigma is estimated to be 4.45\n\n##### Density plots (via `gather_draws`)\n\nThe `gather_draws()` function performs the equivalent of an\n`as_draws_df()` followed by a `pivot_longer()` in order to return the\nfull posteriors in long format where they are more suitable for\ngraphing.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2b.brm2 |> get_variables()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_xcontrol\"    \"b_xmedium\"     \"b_xhigh\"       \"sigma\"        \n [5] \"prior_b\"       \"prior_sigma\"   \"lprior\"        \"lp__\"         \n [9] \"accept_stat__\" \"stepsize__\"    \"treedepth__\"   \"n_leapfrog__\" \n[13] \"divergent__\"   \"energy__\"     \n```\n\n\n:::\n\n```{.r .cell-code}\ndat2b.brm2 |>\n    gather_draws(`b_x.*`, `sigma`, regex = TRUE) |>\n    ggplot() +\n    geom_histogram(aes(x = .value)) +\n    facet_wrap(~.variable, scales = \"free\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/gather_draws2b-1.png){width=768}\n:::\n:::\n\n\n\nAlternatively, there are various other representations supported by\nthe `ggdist` package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2b.brm2 |>\n  gather_draws(`b_x.*`, regex = TRUE) |>\n  ggplot() +\n  stat_halfeye(aes(x = .value, y = .variable))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/gather_draws2b2-1.png){width=480}\n:::\n:::\n\n\n\n##### R²\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2b.brm2 |>\n    bayes_R2(summary = FALSE) |>\n    median_hdci()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          y      ymin      ymax .width .point .interval\n1 0.5540797 0.1893543 0.7253041   0.95 median      hdci\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- 55.408% of the total variability in $y$ can be\n  explained by its relationship to $x$\n- we are 95% confident that the strength of this relationship is\n  between 18.935% and 72.53%\n \n\n:::::\n\n:::::::\n\n\n### Example 3 (Poisson data)\n\n::::::: {.panel-tabset}\n#### Raw predictor\n::::: {.panel-tabset}\n\n##### Summary\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3a.brm2 |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: poisson \n  Links: mu = log \nFormula: y ~ x \n   Data: dat3 (Number of observations: 10) \n  Draws: 3 chains, each with iter = 5000; warmup = 1000; thin = 5;\n         total post-warmup draws = 2400\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.02      0.35    -0.69     0.67 1.00     2108     1861\nx             0.35      0.04     0.27     0.43 1.00     2142     2040\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n::: {.callout-important}\nThe results presented by the `summary()` function on the link scale \n(in this case, the log scale).  As such, they can be awkward to \ninterpret and are particularly punishing for your audience.\n:::\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `Intercept`: when $x=0$, the expected value of $y$ is \n  0.021 and we are 95% confident that the true\n  value is between -0.693 and \n  0.672. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. So for every one unit change in $x$, $y$ increases by (on\n  average) 0.345 units and we are 95%\n  confident that this change is between 0.266\n  and 0.432\n\n**Note, the estimates are means and quantiles.**\n\n##### Summarise draws\n\nAs an alternative to the regular summary (which is designed to\nresemble that traditionally provided by frequentist analyses - and\nthus be instantly familiar), it is possible to define exactly how each\nof the parameters are summarised.\n\nIn the following, I am nominating that I want to summarise each\nparameter posterior by:\n\n- the median\n- the 95% highest probability density interval (credibility interval)\n- Rhat\n- total number of draws\n- bulk and tail effective sample sizes\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3a.brm2 |>\n  posterior::summarise_draws(\n    median,\n    HDInterval::hdi,\n    rhat,\n    length,\n    ess_bulk, ess_tail\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7 × 8\n  variable          median   lower   upper  rhat length ess_bulk ess_tail\n  <chr>              <dbl>   <dbl>   <dbl> <dbl>  <dbl>    <dbl>    <dbl>\n1 b_Intercept       0.0368  -0.693   0.672  1.00   2400    2108.    1861.\n2 b_x               0.345    0.265   0.430  1.00   2400    2141.    2040.\n3 Intercept         1.93     1.63    2.18   1.00   2400    2160.    2129.\n4 prior_Intercept   2.01    -0.701   4.48   1.00   2400    1982.    1809.\n5 prior_b           0.0274  -6.08    6.64   1.00   2400    2399.    2209.\n6 lprior           -2.92    -2.96   -2.91   1.00   2400    2116.    2069.\n7 lp__            -26.8    -29.1   -26.1    1.00   2400    2158.    2326.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n::: {.callout-important}\nThe results presented by the above function on the link scale \n(in this case, the log scale).  As such, they can be awkward to \ninterpret and are particularly punishing for your audience.\n:::\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `b_Intercept`: when $x=0$, the expected value of $y$ is \n  1.929 and we are 95% confident that the true\n  value is between -0.693 and \n  0.672. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `b_x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. So for every one unit change in $x$, $y$ increases by (on\n  average) 0.345 units and we are 95%\n  confident that this change is between 0.265\n  and 0.43\n\n##### As draws\n\nAs yet a more flexible alternative, if we first extract the full\nposterior draws (with `as_draws_df()`), we can then use the various\n`tidyverse` functions to focus on just the most ecologically\ninterpretable parameters before summarising.\n\nIn the following, I will use `select()` with a regex (regular\nexpression) to match only the columns that:\n\n- start with (`^`) \"b_\" followed by any amount of (`*`) any character\n  (`.`)\n- start with (`^`) \"sigma\"\n\nIt will also use `mutate()` to transform the results back onto the scale\nof the response (by expoenentiating the the posteriors before summarising.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3a.brm2 |>\n    brms::as_draws_df() |>\n    dplyr::select(matches(\"^b_.*|^sigma\")) |>\n    mutate(across(everything(), exp)) |>\n    posterior::summarise_draws(\n        median,\n        HDInterval::hdi,\n        rhat,\n        length,\n        ess_bulk, ess_tail\n    )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 8\n  variable    median lower upper  rhat length ess_bulk ess_tail\n  <chr>        <dbl> <dbl> <dbl> <dbl>  <dbl>    <dbl>    <dbl>\n1 b_Intercept   1.04 0.437  1.84  1.00   2400    2103.    1798.\n2 b_x           1.41 1.30   1.53  1.00   2400    2136.    1970.\n```\n\n\n:::\n:::\n\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `b_Intercept`: when $x=0$, the expected value of $y$ is \n  1.038 and we are 95% confident that the true\n  value is between 0.5 and \n  1.958. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `b_x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. So for every one unit change in $x$, $y$ increases by (on\n  average) a factor of 1.411 and we are 95%\n  confident that this change is between 1.303\n  and 1.538.  This represents a (value -1) * 100\n  41.1 % increase in $y$ per \n  unit increase in $x$.\n\n##### Density plots (via `gather_draws`)\n\nThe `gather_draws()` function performs the equivalent of an\n`as_draws_df()` followed by a `pivot_longer()` in order to return the\nfull posteriors in long format where they are more suitable for\ngraphing.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3a.brm2 |>\n    gather_draws(b_Intercept, b_x) |>\n    mutate(across(everything(), exp)) |>\n    ggplot() +\n    geom_histogram(aes(x = .value)) +\n    facet_wrap(~.variable, scales = \"free\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/gather_draws3a-1.png){width=768}\n:::\n:::\n\n\n\nAlternatively, there are various other representations supported by\nthe `ggdist` package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3a.brm2 |>\n  gather_draws(b_Intercept, b_x) |>\n  mutate(across(everything(), exp)) |>\n  ggplot() +\n  stat_halfeye(aes(x = .value, y = .variable))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/gather_draws3a2-1.png){width=480}\n:::\n:::\n\n\n\n##### R²\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3a.brm2 |>\n    bayes_R2(summary = FALSE) |>\n    median_hdci()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         y      ymin      ymax .width .point .interval\n1 0.922404 0.8093995 0.9343621   0.95 median      hdci\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- 92.24% of the total variability in $y$ can be\n  explained by its relationship to $x$\n- we are 95% confident that the strength of this relationship is\n  between 80.94% and 93.436%\n  \n:::::\n\n#### Centered predictor\n::::: {.panel-tabset}\n\n##### Summary\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3b.brm2 |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: poisson \n  Links: mu = log \nFormula: y ~ scale(x, scale = FALSE) \n   Data: dat3 (Number of observations: 10) \n  Draws: 3 chains, each with iter = 5000; warmup = 1000; thin = 5;\n         total post-warmup draws = 2400\n\nRegression Coefficients:\n                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept              1.93      0.14     1.65     2.19 1.00     2222     2142\nscalexscaleEQFALSE     0.34      0.04     0.26     0.43 1.00     2132     1947\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n::: {.callout-important}\nThe results presented by the `summary()` function on the link scale \n(in this case, the log scale).  As such, they can be awkward to \ninterpret and are particularly punishing for your audience.\n:::\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `Intercept`: when $x=0$, the expected value of $y$ is \n  1.926 and we are 95% confident that the true\n  value is between 1.649 and \n  2.191. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. So for every one unit change in $x$, $y$ increases by (on\n  average) 0.342 units and we are 95%\n  confident that this change is between 0.257\n  and 0.433\n\n**Note, the estimates are means and quantiles.**\n\n##### Summarise draws\n\nAs an alternative to the regular summary (which is designed to\nresemble that traditionally provided by frequentist analyses - and\nthus be instantly familiar), it is possible to define exactly how each\nof the parameters are summarised.\n\nIn the following, I am nominating that I want to summarise each\nparameter posterior by:\n\n- the median\n- the 95% highest probability density interval (credibility interval)\n- Rhat\n- total number of draws\n- bulk and tail effective sample sizes\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3b.brm2 |>\n  posterior::summarise_draws(\n    median,\n    HDInterval::hdi,\n    rhat,\n    length,\n    ess_bulk, ess_tail\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7 × 8\n  variable               median   lower   upper  rhat length ess_bulk ess_tail\n  <chr>                   <dbl>   <dbl>   <dbl> <dbl>  <dbl>    <dbl>    <dbl>\n1 b_Intercept            1.93     1.65    2.19   1.00   2400    2222.    2142.\n2 b_scalexscaleEQFALSE   0.342    0.263   0.438  1.00   2400    2132.    1947.\n3 Intercept              1.93     1.65    2.19   1.00   2400    2222.    2142.\n4 prior_Intercept        2.00    -0.452   4.56   1.00   2400    2307.    2368.\n5 prior_b               -0.0445  -6.25    5.77   1.00   2400    2196.    2306.\n6 lprior                -2.92    -2.95   -2.91   1.00   2400    2217.    2187.\n7 lp__                 -26.8    -29.2   -26.1    1.00   2400    2241.    2284.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n::: {.callout-important}\nThe results presented by the above function on the link scale \n(in this case, the log scale).  As such, they can be awkward to \ninterpret and are particularly punishing for your audience.\n:::\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `b_Intercept`: when $x=0$, the expected value of $y$ is \n  1.928 and we are 95% confident that the true\n  value is between 1.646 and \n  2.187. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `b_x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. So for every one unit change in $x$, $y$ increases by (on\n  average) 0.342 units and we are 95%\n  confident that this change is between 0.263\n  and 0.438\n\n##### As draws\n\nAs yet a more flexible alternative, if we first extract the full\nposterior draws (with `as_draws_df()`), we can then use the various\n`tidyverse` functions to focus on just the most ecologically\ninterpretable parameters before summarising.\n\nIn the following, I will use `select()` with a regex (regular\nexpression) to match only the columns that:\n\n- start with (`^`) \"b_\" followed by any amount of (`*`) any character\n  (`.`)\n- start with (`^`) \"sigma\"\n\nIt will also use `mutate()` to transform the results back onto the scale\nof the response (by expoenentiating the the posteriors before summarising.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3b.brm2 |>\n    brms::as_draws_df() |>\n    dplyr::select(matches(\"^b_.*|^sigma\")) |>\n    mutate(across(everything(), exp)) |>\n    posterior::summarise_draws(\n        median,\n        HDInterval::hdi,\n        rhat,\n        length,\n        ess_bulk, ess_tail\n    )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 8\n  variable             median lower upper  rhat length ess_bulk ess_tail\n  <chr>                 <dbl> <dbl> <dbl> <dbl>  <dbl>    <dbl>    <dbl>\n1 b_Intercept            6.87  5.11  8.83  1.00   2400    2211.    2121.\n2 b_scalexscaleEQFALSE   1.41  1.29  1.54  1.00   2400    2123.    1929.\n```\n\n\n:::\n:::\n\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `b_Intercept`: when $x=0$, the expected value of $y$ is \n  6.873 and we are 95% confident that the true\n  value is between 5.188 and \n  8.912. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `b_x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. So for every one unit change in $x$, $y$ increases by (on\n  average) a factor of 1.408 and we are 95%\n  confident that this change is between 1.301\n  and 1.549.  This represents a (value -1) * 100\n  40.8 % increase in $y$ per \n  unit increase in $x$.\n\n##### Density plots (via `gather_draws`)\n\nThe `gather_draws()` function performs the equivalent of an\n`as_draws_df()` followed by a `pivot_longer()` in order to return the\nfull posteriors in long format where they are more suitable for\ngraphing.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3b.brm2 |>\n    gather_draws(`b_Intercept`, `b_.*x.*`, regex = TRUE) |>\n    mutate(across(everything(), exp)) |>\n    ggplot() +\n    geom_histogram(aes(x = .value)) +\n    facet_wrap(~.variable, scales = \"free\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/gather_draws3b-1.png){width=768}\n:::\n:::\n\n\n\nAlternatively, there are various other representations supported by\nthe `ggdist` package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3b.brm2 |>\n    gather_draws(`b_Intercept`, `b_.*x.*`, regex = TRUE) |>\n    mutate(across(everything(), exp)) |>\n  ggplot() +\n  stat_halfeye(aes(x = .value, y = .variable))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/gather_draws3b2-1.png){width=480}\n:::\n:::\n\n\n\n##### R²\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3b.brm2 |>\n    bayes_R2(summary = FALSE) |>\n    median_hdci()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          y      ymin      ymax .width .point .interval\n1 0.9215583 0.8041622 0.9343709   0.95 median      hdci\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- 92.156% of the total variability in $y$ can be\n  explained by its relationship to $x$\n- we are 95% confident that the strength of this relationship is\n  between 80.416% and 93.437%\n  \n:::::\n\n#### Standardised predictor\n::::: {.panel-tabset}\n\n##### Summary\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3c.brm2 |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: poisson \n  Links: mu = log \nFormula: y ~ scale(x) \n   Data: dat3 (Number of observations: 10) \n  Draws: 3 chains, each with iter = 5000; warmup = 1000; thin = 5;\n         total post-warmup draws = 2400\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     1.93      0.14     1.64     2.20 1.00     2142     2163\nscalex        1.03      0.13     0.78     1.30 1.00     2155     2327\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n::: {.callout-important}\nThe results presented by the `summary()` function on the link scale \n(in this case, the log scale).  As such, they can be awkward to \ninterpret and are particularly punishing for your audience.\n:::\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `Intercept`: when $x=0$ (its average since it is standardised), the expected value of $y$ is \n  1.929 and we are 95% confident that the true\n  value is between 1.643 and \n  2.195. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. Recall that since $x$ is standardised, 1 unit representes a span of 1 standard deviation of $x$. So for every one standard deviation unit change in $x$, $y$ increases by (on\n  average) 1.035 units and we are 95%\n  confident that this change is between 0.782\n  and 1.305\n\n**Note, the estimates are means and quantiles.**\n\n##### Summarise draws\n\nAs an alternative to the regular summary (which is designed to\nresemble that traditionally provided by frequentist analyses - and\nthus be instantly familiar), it is possible to define exactly how each\nof the parameters are summarised.\n\nIn the following, I am nominating that I want to summarise each\nparameter posterior by:\n\n- the median\n- the 95% highest probability density interval (credibility interval)\n- Rhat\n- total number of draws\n- bulk and tail effective sample sizes\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3c.brm2 |>\n  posterior::summarise_draws(\n    median,\n    HDInterval::hdi,\n    rhat,\n    length,\n    ess_bulk, ess_tail\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7 × 8\n  variable           median   lower  upper  rhat length ess_bulk ess_tail\n  <chr>               <dbl>   <dbl>  <dbl> <dbl>  <dbl>    <dbl>    <dbl>\n1 b_Intercept       1.93      1.66    2.22  1.00   2400    2141.    2163.\n2 b_scalex          1.03      0.768   1.28  1.00   2400    2155.    2327.\n3 Intercept         1.93      1.66    2.22  1.00   2400    2141.    2163.\n4 prior_Intercept   2.00     -0.830   4.47  1.00   2400    2402.    2329.\n5 prior_b           0.00164  -4.98    3.99  1.00   2400    2195.    2105.\n6 lprior           -2.86     -3.04   -2.70  1.00   2400    2139.    2328.\n7 lp__            -26.8     -28.9   -26.0   1.00   2400    2190.    2273.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `b_Intercept`: when $x=0$ (its average since it is standardised), the expected value of $y$ is \n  1.932 and we are 95% confident that the true\n  value is between 1.665 and \n  2.218. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `b_x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. Recall that since $x$ is standardised, 1 unit representes a span of 1 standard deviation of $x$. So for every one standard deviation unit change in $x$, $y$ increases by (on\n  average)1.034 units and we are 95%\n  confident that this change is between 0.768\n  and 1.279\n\n##### As draws\n\nAs yet a more flexible alternative, if we first extract the full\nposterior draws (with `as_draws_df()`), we can then use the various\n`tidyverse` functions to focus on just the most ecologically\ninterpretable parameters before summarising.\n\nIn the following, I will use `select()` with a regex (regular\nexpression) to match only the columns that:\n\n- start with (`^`) \"b_\" followed by any amount of (`*`) any character\n  (`.`)\n- start with (`^`) \"sigma\"\n\nIt will also use `mutate()` to transform the results back onto the scale\nof the response (by expoenentiating the the posteriors before summarising.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3c.brm2 |>\n    brms::as_draws_df() |>\n    dplyr::select(matches(\"^b_.*|^sigma\")) |>\n    mutate(across(everything(), exp)) |>\n    posterior::summarise_draws(\n        median,\n        HDInterval::hdi,\n        rhat,\n        length,\n        ess_bulk, ess_tail\n    )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 8\n  variable    median lower upper  rhat length ess_bulk ess_tail\n  <chr>        <dbl> <dbl> <dbl> <dbl>  <dbl>    <dbl>    <dbl>\n1 b_Intercept   6.90  5.05  8.85  1.00   2400    2126.    2156.\n2 b_scalex      2.81  2.13  3.55  1.00   2400    2121.    2322.\n```\n\n\n:::\n:::\n\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `b_Intercept`: when $x=0$ (its average since it is standardised), the expected value of $y$ is \n  6.903 and we are 95% confident that the true\n  value is between 5.285 and \n  9.185. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `b_x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. Recall that since $x$ is standardised, 1 unit representes a span of 1 standard deviation of $x$. So for every one standard deviation unit change in $x$, $y$ increases by (on\n  average) a factor of 2.812 units and we are 95%\n  confident that this change is between 2.156\n  and 3.593. This represents a ((value -1) * 100)\n  181.2% increase in $y$ per \n  unit increase in $x$.\n\n\n##### Density plots (via `gather_draws`)\n\nThe `gather_draws()` function performs the equivalent of an\n`as_draws_df()` followed by a `pivot_longer()` in order to return the\nfull posteriors in long format where they are more suitable for\ngraphing. Note, that the name of the slope parameter gets very awkward\nwhen $x$ is centered, so it is more convenient to refer to this\nparameter via a regular expression.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3c.brm2 |> get_variables()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"     \"b_scalex\"        \"Intercept\"       \"prior_Intercept\"\n [5] \"prior_b\"         \"lprior\"          \"lp__\"            \"accept_stat__\"  \n [9] \"stepsize__\"      \"treedepth__\"     \"n_leapfrog__\"    \"divergent__\"    \n[13] \"energy__\"       \n```\n\n\n:::\n\n```{.r .cell-code}\ndat3c.brm2 |>\n  gather_draws(`b_Intercept`,`b_.*x`, regex = TRUE) |> \n  mutate(across(everything(), exp)) |>\n  ggplot() +\n  geom_histogram(aes(x = .value)) +\n  facet_wrap(~.variable, scales = \"free\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/gather_draws3c-1.png){width=768}\n:::\n:::\n\n\n\nAlternatively, there are various other representations supported by\nthe `ggdist` package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3c.brm2 |>\n  gather_draws(`b_Intercept`,`b_.*x`, regex = TRUE) |> \n  mutate(across(everything(), exp)) |>\n  ggplot() +\n  stat_halfeye(aes(x = .value, y = .variable))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/gather_draws3c2-1.png){width=480}\n:::\n:::\n\n\n\n##### R²\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3c.brm2 |>\n    bayes_R2(summary = FALSE) |>\n    median_hdci()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          y      ymin      ymax .width .point .interval\n1 0.9211877 0.8045365 0.9343754   0.95 median      hdci\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- 92.119% of the total variability in $y$ can be\n  explained by its relationship to $x$\n- we are 95% confident that the strength of this relationship is\n  between 80.454% and 93.438%\n  \n:::::\n\n:::::::\n\n### Example 4 (NB data)\n\n::::::: {.panel-tabset}\n#### Raw predictor\n::::: {.panel-tabset}\n\n##### Summary\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4a.brm2 |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: negbinomial \n  Links: mu = log; shape = identity \nFormula: y ~ x \n   Data: dat4 (Number of observations: 10) \n  Draws: 3 chains, each with iter = 5000; warmup = 1000; thin = 5;\n         total post-warmup draws = 2400\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.36      0.40    -0.43     1.11 1.00     2545     2355\nx             0.28      0.05     0.18     0.39 1.00     2474     2236\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nshape    52.46     64.80     3.26   229.15 1.00     2142     2369\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n::: {.callout-important}\nThe results presented by the `summary()` function on the link scale \n(in this case, the log scale).  As such, they can be awkward to \ninterpret and are particularly punishing for your audience.\n:::\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `Intercept`: when $x=0$, the expected value of $y$ is \n  0.357 and we are 95% confident that the true\n  value is between -0.434 and \n  1.11. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. So for every one unit change in $x$, $y$ increases by (on\n  average) 0.284 units and we are 95%\n  confident that this change is between 0.183\n  and 0.393\n\n**Note, the estimates are means and quantiles.**\n\n##### Summarise draws\n\nAs an alternative to the regular summary (which is designed to\nresemble that traditionally provided by frequentist analyses - and\nthus be instantly familiar), it is possible to define exactly how each\nof the parameters are summarised.\n\nIn the following, I am nominating that I want to summarise each\nparameter posterior by:\n\n- the median\n- the 95% highest probability density interval (credibility interval)\n- Rhat\n- total number of draws\n- bulk and tail effective sample sizes\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4a.brm2 |>\n  posterior::summarise_draws(\n    median,\n    HDInterval::hdi,\n    rhat,\n    length,\n    ess_bulk, ess_tail\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 9 × 8\n  variable           median      lower   upper  rhat length ess_bulk ess_tail\n  <chr>               <dbl>      <dbl>   <dbl> <dbl>  <dbl>    <dbl>    <dbl>\n1 b_Intercept      3.66e- 1 -3.65e-  1   1.16  1.00    2400    2545.    2355.\n2 b_x              2.83e- 1  1.82e-  1   0.390 1.00    2400    2475.    2236.\n3 shape            3.04e+ 1  1.09e+  0 179.    1.00    2400    2143.    2369.\n4 Intercept        1.92e+ 0  1.62e+  0   2.25  1.00    2400    2559.    2144.\n5 prior_Intercept  1.97e+ 0 -1.75e-  1   3.74  1.00    2400    2326.    2298.\n6 prior_b          2.13e- 2 -4.67e+  0   4.70  1.00    2400    2275.    2180.\n7 prior_shape      2.15e-28  1.56e-305   0.250 1.00    2400    2329.    2291.\n8 lprior          -1.07e+ 1 -1.42e+  1  -8.01  1.00    2400    2137.    2369.\n9 lp__            -3.18e+ 1 -3.49e+  1 -30.6   0.999   2400    2105.    2325.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n::: {.callout-important}\nThe results presented by the above function on the link scale \n(in this case, the log scale).  As such, they can be awkward to \ninterpret and are particularly punishing for your audience.\n:::\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `b_Intercept`: when $x=0$, the expected value of $y$ is \n  30.415 and we are 95% confident that the true\n  value is between -0.365 and \n  1.162. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `b_x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. So for every one unit change in $x$, $y$ increases by (on\n  average) 0.283 units and we are 95%\n  confident that this change is between 0.182\n  and 0.39\n\n##### As draws\n\nAs yet a more flexible alternative, if we first extract the full\nposterior draws (with `as_draws_df()`), we can then use the various\n`tidyverse` functions to focus on just the most ecologically\ninterpretable parameters before summarising.\n\nIn the following, I will use `select()` with a regex (regular\nexpression) to match only the columns that:\n\n- start with (`^`) \"b_\" followed by any amount of (`*`) any character\n  (`.`)\n- start with (`^`) \"sigma\"\n\nIt will also use `mutate()` to transform the results back onto the scale\nof the response (by expoenentiating the the posteriors before summarising.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4a.brm2 |>\n    brms::as_draws_df() |>\n    dplyr::select(matches(\"^b_.*|^sigma\")) |>\n    mutate(across(everything(), exp)) |>\n    posterior::summarise_draws(\n        median,\n        HDInterval::hdi,\n        rhat,\n        length,\n        ess_bulk, ess_tail\n    )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 8\n  variable    median lower upper  rhat length ess_bulk ess_tail\n  <chr>        <dbl> <dbl> <dbl> <dbl>  <dbl>    <dbl>    <dbl>\n1 b_Intercept   1.44 0.568  2.83  1.00   2400    2533.    2330.\n2 b_x           1.33 1.20   1.48  1.00   2400    2465.    2218.\n```\n\n\n:::\n:::\n\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `b_Intercept`: when $x=0$, the expected value of $y$ is \n  1.442 and we are 95% confident that the true\n  value is between 0.694 and \n  3.196. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `b_x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. So for every one unit change in $x$, $y$ increases by (on\n  average) a factor of 1.327 and we are 95%\n  confident that this change is between 1.2\n  and 1.477.  This represents a (value -1) * 100\n  32.7 % increase in $y$ per \n  unit increase in $x$.\n\n##### Density plots (via `gather_draws`)\n\nThe `gather_draws()` function performs the equivalent of an\n`as_draws_df()` followed by a `pivot_longer()` in order to return the\nfull posteriors in long format where they are more suitable for\ngraphing.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4a.brm2 |>\n    gather_draws(b_Intercept, b_x) |>\n    mutate(across(everything(), exp)) |>\n    ggplot() +\n    geom_histogram(aes(x = .value)) +\n    facet_wrap(~.variable, scales = \"free\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/gather_draws4a-1.png){width=768}\n:::\n:::\n\n\n\nAlternatively, there are various other representations supported by\nthe `ggdist` package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4a.brm2 |>\n  gather_draws(b_Intercept, b_x) |>\n  mutate(across(everything(), exp)) |>\n  ggplot() +\n  stat_halfeye(aes(x = .value, y = .variable))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/gather_draws4a2-1.png){width=480}\n:::\n:::\n\n\n\n##### R²\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4a.brm2 |>\n    bayes_R2(summary = FALSE) |>\n    median_hdci()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          y      ymin      ymax .width .point .interval\n1 0.9003317 0.6621915 0.9214525   0.95 median      hdci\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- 90.033% of the total variability in $y$ can be\n  explained by its relationship to $x$\n- we are 95% confident that the strength of this relationship is\n  between 66.219% and 92.145%\n  \n:::::\n\n#### Centered predictor\n::::: {.panel-tabset}\n\n##### Summary\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4b.brm2 |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: negbinomial \n  Links: mu = log; shape = identity \nFormula: y ~ scale(x, scale = FALSE) \n   Data: dat4 (Number of observations: 10) \n  Draws: 3 chains, each with iter = 5000; warmup = 1000; thin = 5;\n         total post-warmup draws = 2400\n\nRegression Coefficients:\n                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept              1.92      0.15     1.61     2.21 1.00     2358     2422\nscalexscaleEQFALSE     0.28      0.05     0.18     0.40 1.00     2376     2327\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nshape    49.96     59.36     3.36   203.02 1.00     2163     1994\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n::: {.callout-important}\nThe results presented by the `summary()` function on the link scale \n(in this case, the log scale).  As such, they can be awkward to \ninterpret and are particularly punishing for your audience.\n:::\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `Intercept`: when $x=0$, the expected value of $y$ is \n  1.918 and we are 95% confident that the true\n  value is between 1.608 and \n  2.214. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. So for every one unit change in $x$, $y$ increases by (on\n  average) 0.285 units and we are 95%\n  confident that this change is between 0.18\n  and 0.396\n\n**Note, the estimates are means and quantiles.**\n\n##### Summarise draws\n\nAs an alternative to the regular summary (which is designed to\nresemble that traditionally provided by frequentist analyses - and\nthus be instantly familiar), it is possible to define exactly how each\nof the parameters are summarised.\n\nIn the following, I am nominating that I want to summarise each\nparameter posterior by:\n\n- the median\n- the 95% highest probability density interval (credibility interval)\n- Rhat\n- total number of draws\n- bulk and tail effective sample sizes\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4b.brm2 |>\n  posterior::summarise_draws(\n    median,\n    HDInterval::hdi,\n    rhat,\n    length,\n    ess_bulk, ess_tail\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 9 × 8\n  variable                median   lower   upper  rhat length ess_bulk ess_tail\n  <chr>                    <dbl>   <dbl>   <dbl> <dbl>  <dbl>    <dbl>    <dbl>\n1 b_Intercept           1.92e+ 0   1.62    2.22  1.00    2400    2358.    2422.\n2 b_scalexscaleEQFALSE  2.84e- 1   0.176   0.391 1.00    2400    2376.    2327.\n3 shape                 2.93e+ 1   0.992 164.    0.999   2400    2163.    1994.\n4 Intercept             1.92e+ 0   1.62    2.22  1.00    2400    2358.    2422.\n5 prior_Intercept       2.06e+ 0   0.190   3.91  1.00    2400    2310.    2313.\n6 prior_b               1.04e- 2  -4.52    4.81  1.00    2400    2340.    2167.\n7 prior_shape           1.07e-28   0       0.325 1.00    2400    1967.    2130.\n8 lprior               -1.05e+ 1 -13.9    -7.90  0.999   2400    2151.    1995.\n9 lp__                 -3.17e+ 1 -34.6   -30.5   1.00    2400    2377.    2328.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n::: {.callout-important}\nThe results presented by the above function on the link scale \n(in this case, the log scale).  As such, they can be awkward to \ninterpret and are particularly punishing for your audience.\n:::\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `b_Intercept`: when $x=0$, the expected value of $y$ is \n  29.253 and we are 95% confident that the true\n  value is between 1.617 and \n  2.221. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `b_x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. So for every one unit change in $x$, $y$ increases by (on\n  average) 0.284 units and we are 95%\n  confident that this change is between 0.176\n  and 0.391\n\n##### As draws\n\nAs yet a more flexible alternative, if we first extract the full\nposterior draws (with `as_draws_df()`), we can then use the various\n`tidyverse` functions to focus on just the most ecologically\ninterpretable parameters before summarising.\n\nIn the following, I will use `select()` with a regex (regular\nexpression) to match only the columns that:\n\n- start with (`^`) \"b_\" followed by any amount of (`*`) any character\n  (`.`)\n- start with (`^`) \"sigma\"\n\nIt will also use `mutate()` to transform the results back onto the scale\nof the response (by expoenentiating the the posteriors before summarising.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4b.brm2 |>\n    brms::as_draws_df() |>\n    dplyr::select(matches(\"^b_.*|^sigma\")) |>\n    mutate(across(everything(), exp)) |>\n    posterior::summarise_draws(\n        median,\n        HDInterval::hdi,\n        rhat,\n        length,\n        ess_bulk, ess_tail\n    )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 8\n  variable             median lower upper  rhat length ess_bulk ess_tail\n  <chr>                 <dbl> <dbl> <dbl> <dbl>  <dbl>    <dbl>    <dbl>\n1 b_Intercept            6.84  4.87  8.96  1.00   2400    2338.    2403.\n2 b_scalexscaleEQFALSE   1.33  1.19  1.48  1.00   2400    2366.    2320.\n```\n\n\n:::\n:::\n\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `b_Intercept`: when $x=0$, the expected value of $y$ is \n  6.839 and we are 95% confident that the true\n  value is between 5.04 and \n  9.217. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `b_x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. So for every one unit change in $x$, $y$ increases by (on\n  average) a factor of 1.328 and we are 95%\n  confident that this change is between 1.193\n  and 1.479.  This represents a (value -1) * 100\n  32.8 % increase in $y$ per \n  unit increase in $x$.\n\n##### Density plots (via `gather_draws`)\n\nThe `gather_draws()` function performs the equivalent of an\n`as_draws_df()` followed by a `pivot_longer()` in order to return the\nfull posteriors in long format where they are more suitable for\ngraphing.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4b.brm2 |>\n    gather_draws(`b_Intercept`, `b_.*x.*`, regex = TRUE) |>\n    mutate(across(everything(), exp)) |>\n    ggplot() +\n    geom_histogram(aes(x = .value)) +\n    facet_wrap(~.variable, scales = \"free\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/gather_draws4b-1.png){width=768}\n:::\n:::\n\n\n\nAlternatively, there are various other representations supported by\nthe `ggdist` package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4b.brm2 |>\n    gather_draws(`b_Intercept`, `b_.*x.*`, regex = TRUE) |>\n    mutate(across(everything(), exp)) |>\n  ggplot() +\n  stat_halfeye(aes(x = .value, y = .variable))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/gather_draws4b2-1.png){width=480}\n:::\n:::\n\n\n\n##### R²\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4b.brm2 |>\n    bayes_R2(summary = FALSE) |>\n    median_hdci()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          y     ymin      ymax .width .point .interval\n1 0.8982169 0.670107 0.9214525   0.95 median      hdci\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- 89.822% of the total variability in $y$ can be\n  explained by its relationship to $x$\n- we are 95% confident that the strength of this relationship is\n  between 67.011% and 92.145%\n  \n:::::\n\n#### Standardised predictor\n::::: {.panel-tabset}\n\n##### Summary\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4c.brm2 |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: negbinomial \n  Links: mu = log; shape = identity \nFormula: y ~ scale(x) \n   Data: dat4 (Number of observations: 10) \n  Draws: 3 chains, each with iter = 5000; warmup = 1000; thin = 5;\n         total post-warmup draws = 2400\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     1.93      0.16     1.60     2.24 1.00     2358     2298\nscalex        0.84      0.16     0.53     1.16 1.00     2470     2238\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nshape    49.05     56.26     3.31   207.81 1.00     2285     2062\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n::: {.callout-important}\nThe results presented by the `summary()` function on the link scale \n(in this case, the log scale).  As such, they can be awkward to \ninterpret and are particularly punishing for your audience.\n:::\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `Intercept`: when $x=0$ (its average since it is standardised), the expected value of $y$ is \n  1.927 and we are 95% confident that the true\n  value is between 1.601 and \n  2.238. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. Recall that since $x$ is standardised, 1 unit representes a span of 1 standard deviation of $x$. So for every one standard deviation unit change in $x$, $y$ increases by (on\n  average) 0.842 units and we are 95%\n  confident that this change is between 0.531\n  and 1.161\n\n**Note, the estimates are means and quantiles.**\n\n##### Summarise draws\n\nAs an alternative to the regular summary (which is designed to\nresemble that traditionally provided by frequentist analyses - and\nthus be instantly familiar), it is possible to define exactly how each\nof the parameters are summarised.\n\nIn the following, I am nominating that I want to summarise each\nparameter posterior by:\n\n- the median\n- the 95% highest probability density interval (credibility interval)\n- Rhat\n- total number of draws\n- bulk and tail effective sample sizes\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4c.brm2 |>\n  posterior::summarise_draws(\n    median,\n    HDInterval::hdi,\n    rhat,\n    length,\n    ess_bulk, ess_tail\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 9 × 8\n  variable           median   lower   upper  rhat length ess_bulk ess_tail\n  <chr>               <dbl>   <dbl>   <dbl> <dbl>  <dbl>    <dbl>    <dbl>\n1 b_Intercept      1.93e+ 0   1.58    2.21   1.00   2400    2359.    2298.\n2 b_scalex         8.39e- 1   0.521   1.14   1.00   2400    2471.    2238.\n3 shape            2.99e+ 1   0.934 161.     1.00   2400    2285.    2062.\n4 Intercept        1.93e+ 0   1.58    2.21   1.00   2400    2359.    2298.\n5 prior_Intercept  2.07e+ 0   0.280   3.79   1.00   2400    2361.    2244.\n6 prior_b         -2.50e- 2  -4.69    4.45   1.00   2400    2327.    2298.\n7 prior_shape      6.04e-29   0       0.246  1.00   2400    2451.    2350.\n8 lprior          -1.08e+ 1 -13.9    -7.91   1.00   2400    2276.    2123.\n9 lp__            -3.18e+ 1 -34.9   -30.7    1.00   2400    1955.    2287.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `b_Intercept`: when $x=0$ (its average since it is standardised), the expected value of $y$ is \n  1.927 and we are 95% confident that the true\n  value is between 1.584 and \n  2.208. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `b_x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. Recall that since $x$ is standardised, 1 unit representes a span of 1 standard deviation of $x$. So for every one standard deviation unit change in $x$, $y$ increases by (on\n  average)0.839 units and we are 95%\n  confident that this change is between 0.521\n  and 1.143\n\n##### As draws\n\nAs yet a more flexible alternative, if we first extract the full\nposterior draws (with `as_draws_df()`), we can then use the various\n`tidyverse` functions to focus on just the most ecologically\ninterpretable parameters before summarising.\n\nIn the following, I will use `select()` with a regex (regular\nexpression) to match only the columns that:\n\n- start with (`^`) \"b_\" followed by any amount of (`*`) any character\n  (`.`)\n- start with (`^`) \"sigma\"\n\nIt will also use `mutate()` to transform the results back onto the scale\nof the response (by expoenentiating the the posteriors before summarising.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4c.brm2 |>\n    brms::as_draws_df() |>\n    dplyr::select(matches(\"^b_.*|^sigma\")) |>\n    mutate(across(everything(), exp)) |>\n    posterior::summarise_draws(\n        median,\n        HDInterval::hdi,\n        rhat,\n        length,\n        ess_bulk, ess_tail\n    )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 8\n  variable    median lower upper  rhat length ess_bulk ess_tail\n  <chr>        <dbl> <dbl> <dbl> <dbl>  <dbl>    <dbl>    <dbl>\n1 b_Intercept   6.87  4.84  9.05  1.00   2400    2353.    2230.\n2 b_scalex      2.31  1.67  3.12  1.00   2400    2466.    2173.\n```\n\n\n:::\n:::\n\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `b_Intercept`: when $x=0$ (its average since it is standardised), the expected value of $y$ is \n  6.868 and we are 95% confident that the true\n  value is between 4.873 and \n  9.096. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `b_x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. Recall that since $x$ is standardised, 1 unit representes a span of 1 standard deviation of $x$. So for every one standard deviation unit change in $x$, $y$ increases by (on\n  average) a factor of 2.314 units and we are 95%\n  confident that this change is between 1.684\n  and 3.135. This represents a ((value -1) * 100)\n  131.4% increase in $y$ per \n  unit increase in $x$.\n\n\n##### Density plots (via `gather_draws`)\n\nThe `gather_draws()` function performs the equivalent of an\n`as_draws_df()` followed by a `pivot_longer()` in order to return the\nfull posteriors in long format where they are more suitable for\ngraphing. Note, that the name of the slope parameter gets very awkward\nwhen $x$ is centered, so it is more convenient to refer to this\nparameter via a regular expression.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4c.brm2 |> get_variables()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"     \"b_scalex\"        \"shape\"           \"Intercept\"      \n [5] \"prior_Intercept\" \"prior_b\"         \"prior_shape\"     \"lprior\"         \n [9] \"lp__\"            \"accept_stat__\"   \"stepsize__\"      \"treedepth__\"    \n[13] \"n_leapfrog__\"    \"divergent__\"     \"energy__\"       \n```\n\n\n:::\n\n```{.r .cell-code}\ndat4c.brm2 |>\n  gather_draws(`b_Intercept`,`b_.*x`, regex = TRUE) |> \n  mutate(across(everything(), exp)) |>\n  ggplot() +\n  geom_histogram(aes(x = .value)) +\n  facet_wrap(~.variable, scales = \"free\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/gather_draws4c-1.png){width=768}\n:::\n:::\n\n\n\nAlternatively, there are various other representations supported by\nthe `ggdist` package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4c.brm2 |>\n  gather_draws(`b_Intercept`,`b_.*x`, regex = TRUE) |> \n  mutate(across(everything(), exp)) |>\n  ggplot() +\n  stat_halfeye(aes(x = .value, y = .variable))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/gather_draws4c2-1.png){width=480}\n:::\n:::\n\n\n\n##### R²\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4c.brm2 |>\n    bayes_R2(summary = FALSE) |>\n    median_hdci()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          y      ymin      ymax .width .point .interval\n1 0.8982864 0.6326737 0.9214458   0.95 median      hdci\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- 89.829% of the total variability in $y$ can be\n  explained by its relationship to $x$\n- we are 95% confident that the strength of this relationship is\n  between 63.267% and 92.145%\n  \n:::::\n\n:::::::\n\n### Example 5 (Binary data)\n\n::::::: {.panel-tabset}\n#### Raw predictor\n::::: {.panel-tabset}\n\n##### Summary\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5a.brm2 |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: binomial \n  Links: mu = logit \nFormula: y | trials(1) ~ x \n   Data: dat5 (Number of observations: 10) \n  Draws: 3 chains, each with iter = 5000; warmup = 1000; thin = 5;\n         total post-warmup draws = 2400\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -5.60      3.07   -13.48    -0.98 1.00     2231     2230\nx             1.11      0.56     0.26     2.50 1.00     2230     2157\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n::: {.callout-important}\nThe results presented by the `summary()` function on the link scale \n(in this case, the logit, or log-odds scale).  As such, they can be awkward to \ninterpret and are particularly punishing for your audience.\n:::\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `Intercept`: when $x=0$, the expected value of $y$ is \n  -5.602 and we are 95% confident that the true\n  value is between -13.478 and \n  -0.977. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. So for every one unit change in $x$, $y$ increases by (on\n  average) 1.114 units and we are 95%\n  confident that this change is between 0.256\n  and 2.499\n\n**Note, the estimates are means and quantiles.**\n\n##### Summarise draws\n\nAs an alternative to the regular summary (which is designed to\nresemble that traditionally provided by frequentist analyses - and\nthus be instantly familiar), it is possible to define exactly how each\nof the parameters are summarised.\n\nIn the following, I am nominating that I want to summarise each\nparameter posterior by:\n\n- the median\n- the 95% highest probability density interval (credibility interval)\n- Rhat\n- total number of draws\n- bulk and tail effective sample sizes\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5a.brm2 |>\n  posterior::summarise_draws(\n    median,\n    HDInterval::hdi,\n    rhat,\n    length,\n    ess_bulk, ess_tail\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7 × 8\n  variable         median   lower   upper  rhat length ess_bulk ess_tail\n  <chr>             <dbl>   <dbl>   <dbl> <dbl>  <dbl>    <dbl>    <dbl>\n1 b_Intercept     -5.16   -11.7   -0.0398  1.00   2400    2231.    2230.\n2 b_x              1.02     0.180  2.33    1.00   2400    2230.    2157.\n3 Intercept        0.518   -0.814  2.00    1.00   2400    2385.    2404.\n4 prior_Intercept  0.0137  -1.91   1.96    1.00   2400    2296.    2256.\n5 prior_b         -0.0190  -3.17   2.75    1.00   2400    2424.    2410.\n6 lprior          -2.84    -4.85  -1.92    1.00   2400    2241.    2190.\n7 lp__            -6.01    -8.56  -5.26    1.00   2400    2125.    2339.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n::: {.callout-important}\nThe results presented by the above function on the link scale \n(in this case, the logit, or log-odds  scale).  As such, they can be awkward to \ninterpret and are particularly punishing for your audience.\n:::\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `b_Intercept`: when $x=0$, the expected value of $y$ is \n  0.518 and we are 95% confident that the true\n  value is between -11.682 and \n  -0.04. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `b_x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. So for every one unit change in $x$, $y$ increases by (on\n  average) 1.02 units and we are 95%\n  confident that this change is between 0.18\n  and 2.335\n\n##### As draws\n\nAs yet a more flexible alternative, if we first extract the full\nposterior draws (with `as_draws_df()`), we can then use the various\n`tidyverse` functions to focus on just the most ecologically\ninterpretable parameters before summarising.\n\nIn the following, I will use `select()` with a regex (regular\nexpression) to match only the columns that:\n\n- start with (`^`) \"b_\" followed by any amount of (`*`) any character\n  (`.`)\n- start with (`^`) \"sigma\"\n\nIt will also use `mutate()` to transform the results back onto the scale\nof the response (by expoenentiating the the posteriors before summarising.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5a.brm2 |>\n    brms::as_draws_df() |>\n    dplyr::select(matches(\"^b_.*|^sigma\")) |>\n    mutate(across(everything(), exp)) |>\n    posterior::summarise_draws(\n        median,\n        HDInterval::hdi,\n        rhat,\n        length,\n        ess_bulk, ess_tail\n    )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 8\n  variable     median         lower upper  rhat length ess_bulk ess_tail\n  <chr>         <dbl>         <dbl> <dbl> <dbl>  <dbl>    <dbl>    <dbl>\n1 b_Intercept 0.00572 0.00000000196 0.251  1.00   2400    2214.    2206.\n2 b_x         2.77    0.959         8.85   1.00   2400    2218.    2145.\n```\n\n\n:::\n:::\n\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `b_Intercept`: when $x=0$, the expected value of $y$ is \n  0.006 and we are 95% confident that the true\n  value is between 0 and \n  0.961. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `b_x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. So for every one unit change in $x$, $y$ increases by (on\n  average) a factor of 2.772 and we are 95%\n  confident that this change is between 1.197\n  and 10.328.  This represents a (value -1) * 100\n  177.2 % increase in $y$ per \n  unit increase in $x$.\n\n##### Density plots (via `gather_draws`)\n\nThe `gather_draws()` function performs the equivalent of an\n`as_draws_df()` followed by a `pivot_longer()` in order to return the\nfull posteriors in long format where they are more suitable for\ngraphing.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5a.brm2 |>\n    gather_draws(b_Intercept, b_x) |>\n    mutate(across(everything(), exp)) |>\n    ggplot() +\n    geom_histogram(aes(x = .value)) +\n    facet_wrap(~.variable, scales = \"free\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/gather_draws5a-1.png){width=768}\n:::\n:::\n\n\n\nAlternatively, there are various other representations supported by\nthe `ggdist` package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5a.brm2 |>\n  gather_draws(b_Intercept, b_x) |>\n  mutate(across(everything(), exp)) |>\n  ggplot() +\n  stat_halfeye(aes(x = .value, y = .variable))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/gather_draws5a2-1.png){width=480}\n:::\n:::\n\n\n\n##### R²\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5a.brm2 |>\n    bayes_R2(summary = FALSE) |>\n    median_hdci()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          y      ymin      ymax .width .point .interval\n1 0.6241225 0.2563045 0.6997611   0.95 median      hdci\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- 62.412% of the total variability in $y$ can be\n  explained by its relationship to $x$\n- we are 95% confident that the strength of this relationship is\n  between 25.63% and 69.976%\n  \n:::::\n\n#### Centered predictor\n::::: {.panel-tabset}\n\n##### Summary\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5b.brm2 |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: binomial \n  Links: mu = logit \nFormula: y | trials(1) ~ scale(x, scale = FALSE) \n   Data: dat5 (Number of observations: 10) \n  Draws: 3 chains, each with iter = 5000; warmup = 1000; thin = 5;\n         total post-warmup draws = 2400\n\nRegression Coefficients:\n                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept              0.52      0.73    -0.83     1.97 1.00     2356     2269\nscalexscaleEQFALSE     1.13      0.59     0.24     2.57 1.00     2010     1941\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n::: {.callout-important}\nThe results presented by the `summary()` function on the link scale \n(in this case, the logit, or log-odds  scale).  As such, they can be awkward to \ninterpret and are particularly punishing for your audience.\n:::\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `Intercept`: when $x=0$, the expected value of $y$ is \n  0.523 and we are 95% confident that the true\n  value is between -0.834 and \n  1.971. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. So for every one unit change in $x$, $y$ increases by (on\n  average) 1.13 units and we are 95%\n  confident that this change is between 0.242\n  and 2.575\n\n**Note, the estimates are means and quantiles.**\n\n##### Summarise draws\n\nAs an alternative to the regular summary (which is designed to\nresemble that traditionally provided by frequentist analyses - and\nthus be instantly familiar), it is possible to define exactly how each\nof the parameters are summarised.\n\nIn the following, I am nominating that I want to summarise each\nparameter posterior by:\n\n- the median\n- the 95% highest probability density interval (credibility interval)\n- Rhat\n- total number of draws\n- bulk and tail effective sample sizes\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5b.brm2 |>\n  posterior::summarise_draws(\n    median,\n    HDInterval::hdi,\n    rhat,\n    length,\n    ess_bulk, ess_tail\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7 × 8\n  variable                 median  lower upper  rhat length ess_bulk ess_tail\n  <chr>                     <dbl>  <dbl> <dbl> <dbl>  <dbl>    <dbl>    <dbl>\n1 b_Intercept           0.515     -0.825  1.98 1.00    2400    2356.    2269.\n2 b_scalexscaleEQFALSE  1.03       0.150  2.34 1.00    2400    2010.    1941.\n3 Intercept             0.515     -0.825  1.98 1.00    2400    2356.    2269.\n4 prior_Intercept       0.00929   -1.91   1.95 1.00    2400    2481.    2152.\n5 prior_b              -0.0000282 -2.88   3.74 0.999   2400    2327.    2192.\n6 lprior               -2.86      -4.78  -1.93 1.00    2400    1983.    2062.\n7 lp__                 -6.04      -8.77  -5.26 1.00    2400    2053.    2202.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n::: {.callout-important}\nThe results presented by the above function on the link scale \n(in this case, the logit, or log-odds  scale).  As such, they can be awkward to \ninterpret and are particularly punishing for your audience.\n:::\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `b_Intercept`: when $x=0$, the expected value of $y$ is \n  0.515 and we are 95% confident that the true\n  value is between -0.825 and \n  1.977. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `b_x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. So for every one unit change in $x$, $y$ increases by (on\n  average) 1.026 units and we are 95%\n  confident that this change is between 0.15\n  and 2.337\n\n##### As draws\n\nAs yet a more flexible alternative, if we first extract the full\nposterior draws (with `as_draws_df()`), we can then use the various\n`tidyverse` functions to focus on just the most ecologically\ninterpretable parameters before summarising.\n\nIn the following, I will use `select()` with a regex (regular\nexpression) to match only the columns that:\n\n- start with (`^`) \"b_\" followed by any amount of (`*`) any character\n  (`.`)\n- start with (`^`) \"sigma\"\n\nIt will also use `mutate()` to transform the results back onto the scale\nof the response (by expoenentiating the the posteriors before summarising.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5b.brm2 |>\n    brms::as_draws_df() |>\n    dplyr::select(matches(\"^b_.*|^sigma\")) |>\n    mutate(across(everything(), exp)) |>\n    posterior::summarise_draws(\n        median,\n        HDInterval::hdi,\n        rhat,\n        length,\n        ess_bulk, ess_tail\n    )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 8\n  variable             median lower upper  rhat length ess_bulk ess_tail\n  <chr>                 <dbl> <dbl> <dbl> <dbl>  <dbl>    <dbl>    <dbl>\n1 b_Intercept            1.67 0.226  5.70  1.00   2400    2345.    2253.\n2 b_scalexscaleEQFALSE   2.79 0.869  9.24  1.00   2400    1997.    1918.\n```\n\n\n:::\n:::\n\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `b_Intercept`: when $x=0$, the expected value of $y$ is \n  1.674 and we are 95% confident that the true\n  value is between 0.438 and \n  7.22. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `b_x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. So for every one unit change in $x$, $y$ increases by (on\n  average) a factor of 2.789 and we are 95%\n  confident that this change is between 1.162\n  and 10.351.  This represents a (value -1) * 100\n  178.9 % increase in $y$ per \n  unit increase in $x$.\n\n##### Density plots (via `gather_draws`)\n\nThe `gather_draws()` function performs the equivalent of an\n`as_draws_df()` followed by a `pivot_longer()` in order to return the\nfull posteriors in long format where they are more suitable for\ngraphing.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5b.brm2 |>\n    gather_draws(`b_Intercept`, `b_.*x.*`, regex = TRUE) |>\n    mutate(across(everything(), exp)) |>\n    ggplot() +\n    geom_histogram(aes(x = .value)) +\n    facet_wrap(~.variable, scales = \"free\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/gather_draws5b-1.png){width=768}\n:::\n:::\n\n\n\nAlternatively, there are various other representations supported by\nthe `ggdist` package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5b.brm2 |>\n    gather_draws(`b_Intercept`, `b_.*x.*`, regex = TRUE) |>\n    mutate(across(everything(), exp)) |>\n  ggplot() +\n  stat_halfeye(aes(x = .value, y = .variable))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/gather_draws5b2-1.png){width=480}\n:::\n:::\n\n\n\n##### R²\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5b.brm2 |>\n    bayes_R2(summary = FALSE) |>\n    median_hdci()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          y      ymin      ymax .width .point .interval\n1 0.6238714 0.2277985 0.7055103   0.95 median      hdci\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- 62.387% of the total variability in $y$ can be\n  explained by its relationship to $x$\n- we are 95% confident that the strength of this relationship is\n  between 22.78% and 70.551%\n  \n:::::\n\n#### Standardised predictor\n::::: {.panel-tabset}\n\n##### Summary\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5c.brm2 |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: binomial \n  Links: mu = logit \nFormula: y | trials(1) ~ scale(x) \n   Data: dat5 (Number of observations: 10) \n  Draws: 3 chains, each with iter = 5000; warmup = 1000; thin = 5;\n         total post-warmup draws = 2400\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.42      0.68    -0.83     1.82 1.00     2297     2232\nscalex        2.08      1.28     0.22     5.17 1.00     2331     2289\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n::: {.callout-important}\nThe results presented by the `summary()` function on the link scale \n(in this case, the logit, or log-odds  scale).  As such, they can be awkward to \ninterpret and are particularly punishing for your audience.\n:::\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `Intercept`: when $x=0$ (its average since it is standardised), the expected value of $y$ is \n  0.424 and we are 95% confident that the true\n  value is between -0.831 and \n  1.817. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. Recall that since $x$ is standardised, 1 unit representes a span of 1 standard deviation of $x$. So for every one standard deviation unit change in $x$, $y$ increases by (on\n  average) 2.079 units and we are 95%\n  confident that this change is between 0.215\n  and 5.17\n\n**Note, the estimates are means and quantiles.**\n\n##### Summarise draws\n\nAs an alternative to the regular summary (which is designed to\nresemble that traditionally provided by frequentist analyses - and\nthus be instantly familiar), it is possible to define exactly how each\nof the parameters are summarised.\n\nIn the following, I am nominating that I want to summarise each\nparameter posterior by:\n\n- the median\n- the 95% highest probability density interval (credibility interval)\n- Rhat\n- total number of draws\n- bulk and tail effective sample sizes\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5c.brm2 |>\n  posterior::summarise_draws(\n    median,\n    HDInterval::hdi,\n    rhat,\n    length,\n    ess_bulk, ess_tail\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7 × 8\n  variable         median    lower upper  rhat length ess_bulk ess_tail\n  <chr>             <dbl>    <dbl> <dbl> <dbl>  <dbl>    <dbl>    <dbl>\n1 b_Intercept      0.409   -0.899   1.73  1.00   2400    2297.    2232.\n2 b_scalex         1.84     0.0104  4.69  1.00   2400    2331.    2289.\n3 Intercept        0.409   -0.899   1.73  1.00   2400    2297.    2232.\n4 prior_Intercept -0.0150  -2.01    1.96  1.00   2400    2303.    2410.\n5 prior_b         -0.0234  -3.46    2.82  1.00   2400    2384.    2367.\n6 lprior          -3.73    -6.57   -1.92  1.00   2400    2362.    2214.\n7 lp__            -7.42   -10.1    -6.56  1.00   2400    2418.    2289.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `b_Intercept`: when $x=0$ (its average since it is standardised), the expected value of $y$ is \n  0.409 and we are 95% confident that the true\n  value is between -0.899 and \n  1.734. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `b_x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. Recall that since $x$ is standardised, 1 unit representes a span of 1 standard deviation of $x$. So for every one standard deviation unit change in $x$, $y$ increases by (on\n  average)1.844 units and we are 95%\n  confident that this change is between 0.01\n  and 4.691\n\n##### As draws\n\nAs yet a more flexible alternative, if we first extract the full\nposterior draws (with `as_draws_df()`), we can then use the various\n`tidyverse` functions to focus on just the most ecologically\ninterpretable parameters before summarising.\n\nIn the following, I will use `select()` with a regex (regular\nexpression) to match only the columns that:\n\n- start with (`^`) \"b_\" followed by any amount of (`*`) any character\n  (`.`)\n- start with (`^`) \"sigma\"\n\nIt will also use `mutate()` to transform the results back onto the scale\nof the response (by expoenentiating the the posteriors before summarising.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5c.brm2 |>\n    brms::as_draws_df() |>\n    dplyr::select(matches(\"^b_.*|^sigma\")) |>\n    mutate(across(everything(), exp)) |>\n    posterior::summarise_draws(\n        median,\n        HDInterval::hdi,\n        rhat,\n        length,\n        ess_bulk, ess_tail\n    )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 8\n  variable    median lower upper  rhat length ess_bulk ess_tail\n  <chr>        <dbl> <dbl> <dbl> <dbl>  <dbl>    <dbl>    <dbl>\n1 b_Intercept   1.51 0.176  4.75  1.00   2400    2297.    2198.\n2 b_scalex      6.32 0.767 94.0   1.00   2400    2312.    2283.\n```\n\n\n:::\n:::\n\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `b_Intercept`: when $x=0$ (its average since it is standardised), the expected value of $y$ is \n  1.506 and we are 95% confident that the true\n  value is between 0.407 and \n  5.663. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `b_x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. Recall that since $x$ is standardised, 1 unit representes a span of 1 standard deviation of $x$. So for every one standard deviation unit change in $x$, $y$ increases by (on\n  average) a factor of 6.32 units and we are 95%\n  confident that this change is between 1.01\n  and 108.987. This represents a ((value -1) * 100)\n  532% increase in $y$ per \n  unit increase in $x$.\n\n\n##### Density plots (via `gather_draws`)\n\nThe `gather_draws()` function performs the equivalent of an\n`as_draws_df()` followed by a `pivot_longer()` in order to return the\nfull posteriors in long format where they are more suitable for\ngraphing. Note, that the name of the slope parameter gets very awkward\nwhen $x$ is centered, so it is more convenient to refer to this\nparameter via a regular expression.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5c.brm2 |> get_variables()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"     \"b_scalex\"        \"Intercept\"       \"prior_Intercept\"\n [5] \"prior_b\"         \"lprior\"          \"lp__\"            \"accept_stat__\"  \n [9] \"stepsize__\"      \"treedepth__\"     \"n_leapfrog__\"    \"divergent__\"    \n[13] \"energy__\"       \n```\n\n\n:::\n\n```{.r .cell-code}\ndat5c.brm2 |>\n  gather_draws(`b_Intercept`,`b_.*x`, regex = TRUE) |> \n  mutate(across(everything(), exp)) |>\n  ggplot() +\n  geom_histogram(aes(x = .value)) +\n  facet_wrap(~.variable, scales = \"free\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/gather_draws5c-1.png){width=768}\n:::\n:::\n\n\n\nAlternatively, there are various other representations supported by\nthe `ggdist` package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5c.brm2 |>\n  gather_draws(`b_Intercept`,`b_.*x`, regex = TRUE) |> \n  mutate(across(everything(), exp)) |>\n  ggplot() +\n  stat_halfeye(aes(x = .value, y = .variable))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/gather_draws5c2-1.png){width=480}\n:::\n:::\n\n\n\n##### R²\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5c.brm2 |>\n    bayes_R2(summary = FALSE) |>\n    median_hdci()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          y       ymin      ymax .width .point .interval\n1 0.4896796 0.04655454 0.6721203   0.95 median      hdci\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- 48.968% of the total variability in $y$ can be\n  explained by its relationship to $x$\n- we are 95% confident that the strength of this relationship is\n  between 4.655% and 67.212%\n  \n:::::\n\n:::::::\n\n### Example 6 (Binomial data)\n\n::::::: {.panel-tabset}\n#### Raw predictor\n::::: {.panel-tabset}\n\n##### Summary\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6a.brm2 |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: binomial \n  Links: mu = logit \nFormula: count | trials(total) ~ x \n   Data: dat6 (Number of observations: 10) \n  Draws: 3 chains, each with iter = 5000; warmup = 1000; thin = 5;\n         total post-warmup draws = 2400\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -3.26      0.94    -5.21    -1.56 1.00     2503     2367\nx             0.65      0.17     0.36     1.00 1.00     2470     2347\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n::: {.callout-important}\nThe results presented by the `summary()` function on the link scale \n(in this case, the logit, or log-odds scale).  As such, they can be awkward to \ninterpret and are particularly punishing for your audience.\n:::\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `Intercept`: when $x=0$, the expected value of $y$ is \n  -3.262 and we are 95% confident that the true\n  value is between -5.207 and \n  -1.561. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. So for every one unit change in $x$, $y$ increases by (on\n  average) 0.65 units and we are 95%\n  confident that this change is between 0.355\n  and 0.997\n\n**Note, the estimates are means and quantiles.**\n\n##### Summarise draws\n\nAs an alternative to the regular summary (which is designed to\nresemble that traditionally provided by frequentist analyses - and\nthus be instantly familiar), it is possible to define exactly how each\nof the parameters are summarised.\n\nIn the following, I am nominating that I want to summarise each\nparameter posterior by:\n\n- the median\n- the 95% highest probability density interval (credibility interval)\n- Rhat\n- total number of draws\n- bulk and tail effective sample sizes\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6a.brm2 |>\n  posterior::summarise_draws(\n    median,\n    HDInterval::hdi,\n    rhat,\n    length,\n    ess_bulk, ess_tail\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7 × 8\n  variable           median   lower   upper  rhat length ess_bulk ess_tail\n  <chr>               <dbl>   <dbl>   <dbl> <dbl>  <dbl>    <dbl>    <dbl>\n1 b_Intercept      -3.20     -5.11   -1.52  1.00    2400    2503.    2367.\n2 b_x               0.640     0.346   0.982 1.00    2400    2470.    2347.\n3 Intercept         0.317    -0.163   0.791 1.00    2400    2453.    2543.\n4 prior_Intercept  -0.233    -0.869   0.402 1.00    2400    2390.    2225.\n5 prior_b          -0.00697  -1.65    1.70  0.999   2400    2442.    2287.\n6 lprior           -2.37     -5.29   -0.417 1.00    2400    2447.    2412.\n7 lp__            -14.4     -16.8   -13.7   1.00    2400    2354.    2168.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n::: {.callout-important}\nThe results presented by the above function on the link scale \n(in this case, the logit, or log-odds  scale).  As such, they can be awkward to \ninterpret and are particularly punishing for your audience.\n:::\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `b_Intercept`: when $x=0$, the expected value of $y$ is \n  0.317 and we are 95% confident that the true\n  value is between -5.111 and \n  -1.516. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `b_x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. So for every one unit change in $x$, $y$ increases by (on\n  average) 0.64 units and we are 95%\n  confident that this change is between 0.346\n  and 0.982\n\n##### As draws\n\nAs yet a more flexible alternative, if we first extract the full\nposterior draws (with `as_draws_df()`), we can then use the various\n`tidyverse` functions to focus on just the most ecologically\ninterpretable parameters before summarising.\n\nIn the following, I will use `select()` with a regex (regular\nexpression) to match only the columns that:\n\n- start with (`^`) \"b_\" followed by any amount of (`*`) any character\n  (`.`)\n- start with (`^`) \"sigma\"\n\nIt will also use `mutate()` to transform the results back onto the scale\nof the response (by expoenentiating the the posteriors before summarising.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6a.brm2 |>\n    brms::as_draws_df() |>\n    dplyr::select(matches(\"^b_.*|^sigma\")) |>\n    mutate(across(everything(), exp)) |>\n    posterior::summarise_draws(\n        median,\n        HDInterval::hdi,\n        rhat,\n        length,\n        ess_bulk, ess_tail\n    )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 8\n  variable    median    lower upper  rhat length ess_bulk ess_tail\n  <chr>        <dbl>    <dbl> <dbl> <dbl>  <dbl>    <dbl>    <dbl>\n1 b_Intercept 0.0406 0.000717 0.164  1.00   2400    2491.    2362.\n2 b_x         1.90   1.37     2.62   1.00   2400    2459.    2329.\n```\n\n\n:::\n:::\n\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `b_Intercept`: when $x=0$, the expected value of $y$ is \n  0.041 and we are 95% confident that the true\n  value is between 0.006 and \n  0.22. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `b_x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. So for every one unit change in $x$, $y$ increases by (on\n  average) a factor of 1.896 and we are 95%\n  confident that this change is between 1.414\n  and 2.67.  This represents a (value -1) * 100\n  89.6 % increase in $y$ per \n  unit increase in $x$.\n\n##### Density plots (via `gather_draws`)\n\nThe `gather_draws()` function performs the equivalent of an\n`as_draws_df()` followed by a `pivot_longer()` in order to return the\nfull posteriors in long format where they are more suitable for\ngraphing.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6a.brm2 |>\n    gather_draws(b_Intercept, b_x) |>\n    mutate(across(everything(), exp)) |>\n    ggplot() +\n    geom_histogram(aes(x = .value)) +\n    facet_wrap(~.variable, scales = \"free\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/gather_draws6a-1.png){width=768}\n:::\n:::\n\n\n\nAlternatively, there are various other representations supported by\nthe `ggdist` package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6a.brm2 |>\n  gather_draws(b_Intercept, b_x) |>\n  mutate(across(everything(), exp)) |>\n  ggplot() +\n  stat_halfeye(aes(x = .value, y = .variable))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/gather_draws6a2-1.png){width=480}\n:::\n:::\n\n\n\n##### R²\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6a.brm2 |>\n    bayes_R2(summary = FALSE) |>\n    median_hdci()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          y      ymin      ymax .width .point .interval\n1 0.7603549 0.5651125 0.8234649   0.95 median      hdci\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- 76.035% of the total variability in $y$ can be\n  explained by its relationship to $x$\n- we are 95% confident that the strength of this relationship is\n  between 56.511% and 82.346%\n  \n:::::\n\n#### Centered predictor\n::::: {.panel-tabset}\n\n##### Summary\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6b.brm2 |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: binomial \n  Links: mu = logit \nFormula: count | trials(total) ~ scale(x, scale = FALSE) \n   Data: dat6 (Number of observations: 10) \n  Draws: 3 chains, each with iter = 5000; warmup = 1000; thin = 5;\n         total post-warmup draws = 2400\n\nRegression Coefficients:\n                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept              0.31      0.25    -0.19     0.80 1.00     2202     2411\nscalexscaleEQFALSE     0.66      0.17     0.37     1.02 1.00     2344     2208\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n::: {.callout-important}\nThe results presented by the `summary()` function on the link scale \n(in this case, the logit, or log-odds  scale).  As such, they can be awkward to \ninterpret and are particularly punishing for your audience.\n:::\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `Intercept`: when $x=0$, the expected value of $y$ is \n  0.308 and we are 95% confident that the true\n  value is between -0.191 and \n  0.797. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. So for every one unit change in $x$, $y$ increases by (on\n  average) 0.659 units and we are 95%\n  confident that this change is between 0.367\n  and 1.024\n\n**Note, the estimates are means and quantiles.**\n\n##### Summarise draws\n\nAs an alternative to the regular summary (which is designed to\nresemble that traditionally provided by frequentist analyses - and\nthus be instantly familiar), it is possible to define exactly how each\nof the parameters are summarised.\n\nIn the following, I am nominating that I want to summarise each\nparameter posterior by:\n\n- the median\n- the 95% highest probability density interval (credibility interval)\n- Rhat\n- total number of draws\n- bulk and tail effective sample sizes\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6b.brm2 |>\n  posterior::summarise_draws(\n    median,\n    HDInterval::hdi,\n    rhat,\n    length,\n    ess_bulk, ess_tail\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7 × 8\n  variable                median   lower   upper  rhat length ess_bulk ess_tail\n  <chr>                    <dbl>   <dbl>   <dbl> <dbl>  <dbl>    <dbl>    <dbl>\n1 b_Intercept            0.310    -0.174   0.805  1.00   2400    2202.    2411.\n2 b_scalexscaleEQFALSE   0.647     0.366   1.01   1.00   2400    2344.    2208.\n3 Intercept              0.310    -0.174   0.805  1.00   2400    2202.    2411.\n4 prior_Intercept       -0.218    -0.864   0.403  1.00   2400    2308.    2427.\n5 prior_b                0.00307  -1.83    1.59   1.00   2400    2471.    2498.\n6 lprior                -2.34     -5.30   -0.580  1.00   2400    2316.    2445.\n7 lp__                 -14.4     -16.9   -13.7    1.00   2400    2339.    2368.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n::: {.callout-important}\nThe results presented by the above function on the link scale \n(in this case, the logit, or log-odds  scale).  As such, they can be awkward to \ninterpret and are particularly punishing for your audience.\n:::\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `b_Intercept`: when $x=0$, the expected value of $y$ is \n  0.31 and we are 95% confident that the true\n  value is between -0.174 and \n  0.805. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `b_x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. So for every one unit change in $x$, $y$ increases by (on\n  average) 0.647 units and we are 95%\n  confident that this change is between 0.366\n  and 1.015\n\n##### As draws\n\nAs yet a more flexible alternative, if we first extract the full\nposterior draws (with `as_draws_df()`), we can then use the various\n`tidyverse` functions to focus on just the most ecologically\ninterpretable parameters before summarising.\n\nIn the following, I will use `select()` with a regex (regular\nexpression) to match only the columns that:\n\n- start with (`^`) \"b_\" followed by any amount of (`*`) any character\n  (`.`)\n- start with (`^`) \"sigma\"\n\nIt will also use `mutate()` to transform the results back onto the scale\nof the response (by expoenentiating the the posteriors before summarising.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6b.brm2 |>\n    brms::as_draws_df() |>\n    dplyr::select(matches(\"^b_.*|^sigma\")) |>\n    mutate(across(everything(), exp)) |>\n    posterior::summarise_draws(\n        median,\n        HDInterval::hdi,\n        rhat,\n        length,\n        ess_bulk, ess_tail\n    )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 8\n  variable             median lower upper  rhat length ess_bulk ess_tail\n  <chr>                 <dbl> <dbl> <dbl> <dbl>  <dbl>    <dbl>    <dbl>\n1 b_Intercept            1.36 0.730  2.07  1.00   2400    2178.    2383.\n2 b_scalexscaleEQFALSE   1.91 1.37   2.65  1.00   2400    2336.    2153.\n```\n\n\n:::\n:::\n\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `b_Intercept`: when $x=0$, the expected value of $y$ is \n  1.363 and we are 95% confident that the true\n  value is between 0.84 and \n  2.236. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `b_x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. So for every one unit change in $x$, $y$ increases by (on\n  average) a factor of 1.91 and we are 95%\n  confident that this change is between 1.442\n  and 2.759.  This represents a (value -1) * 100\n  91 % increase in $y$ per \n  unit increase in $x$.\n\n##### Density plots (via `gather_draws`)\n\nThe `gather_draws()` function performs the equivalent of an\n`as_draws_df()` followed by a `pivot_longer()` in order to return the\nfull posteriors in long format where they are more suitable for\ngraphing.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6b.brm2 |>\n    gather_draws(`b_Intercept`, `b_.*x.*`, regex = TRUE) |>\n    mutate(across(everything(), exp)) |>\n    ggplot() +\n    geom_histogram(aes(x = .value)) +\n    facet_wrap(~.variable, scales = \"free\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/gather_draws6b-1.png){width=768}\n:::\n:::\n\n\n\nAlternatively, there are various other representations supported by\nthe `ggdist` package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6b.brm2 |>\n    gather_draws(`b_Intercept`, `b_.*x.*`, regex = TRUE) |>\n    mutate(across(everything(), exp)) |>\n  ggplot() +\n  stat_halfeye(aes(x = .value, y = .variable))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/gather_draws6b2-1.png){width=480}\n:::\n:::\n\n\n\n##### R²\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6b.brm2 |>\n    bayes_R2(summary = FALSE) |>\n    median_hdci()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          y      ymin      ymax .width .point .interval\n1 0.7621002 0.5838024 0.8209675   0.95 median      hdci\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- 76.21% of the total variability in $y$ can be\n  explained by its relationship to $x$\n- we are 95% confident that the strength of this relationship is\n  between 58.38% and 82.097%\n  \n:::::\n\n#### Standardised predictor\n::::: {.panel-tabset}\n\n##### Summary\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6c.brm2 |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: binomial \n  Links: mu = logit \nFormula: count | trials(total) ~ scale(x) \n   Data: dat6 (Number of observations: 10) \n  Draws: 3 chains, each with iter = 5000; warmup = 1000; thin = 5;\n         total post-warmup draws = 2400\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.28      0.25    -0.19     0.76 1.00     2332     1948\nscalex        1.62      0.51     0.70     2.72 1.00     2297     2293\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n::: {.callout-important}\nThe results presented by the `summary()` function on the link scale \n(in this case, the logit, or log-odds  scale).  As such, they can be awkward to \ninterpret and are particularly punishing for your audience.\n:::\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `Intercept`: when $x=0$ (its average since it is standardised), the expected value of $y$ is \n  0.28 and we are 95% confident that the true\n  value is between -0.189 and \n  0.765. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. Recall that since $x$ is standardised, 1 unit representes a span of 1 standard deviation of $x$. So for every one standard deviation unit change in $x$, $y$ increases by (on\n  average) 1.622 units and we are 95%\n  confident that this change is between 0.701\n  and 2.721\n\n**Note, the estimates are means and quantiles.**\n\n##### Summarise draws\n\nAs an alternative to the regular summary (which is designed to\nresemble that traditionally provided by frequentist analyses - and\nthus be instantly familiar), it is possible to define exactly how each\nof the parameters are summarised.\n\nIn the following, I am nominating that I want to summarise each\nparameter posterior by:\n\n- the median\n- the 95% highest probability density interval (credibility interval)\n- Rhat\n- total number of draws\n- bulk and tail effective sample sizes\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6c.brm2 |>\n  posterior::summarise_draws(\n    median,\n    HDInterval::hdi,\n    rhat,\n    length,\n    ess_bulk, ess_tail\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7 × 8\n  variable           median   lower   upper  rhat length ess_bulk ess_tail\n  <chr>               <dbl>   <dbl>   <dbl> <dbl>  <dbl>    <dbl>    <dbl>\n1 b_Intercept       0.279    -0.189   0.765 1.00    2400    2332.    1948.\n2 b_scalex          1.61      0.642   2.59  0.999   2400    2298.    2293.\n3 Intercept         0.279    -0.189   0.765 1.00    2400    2332.    1948.\n4 prior_Intercept  -0.234    -0.855   0.410 1.00    2400    2359.    2366.\n5 prior_b          -0.00711  -0.923   1.01  1.00    2400    2094.    2290.\n6 lprior           -5.26     -8.95   -2.08  1.00    2400    2217.    2188.\n7 lp__            -17.9     -20.3   -17.1   0.999   2400    2155.    2148.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `b_Intercept`: when $x=0$ (its average since it is standardised), the expected value of $y$ is \n  0.279 and we are 95% confident that the true\n  value is between -0.189 and \n  0.765. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `b_x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. Recall that since $x$ is standardised, 1 unit representes a span of 1 standard deviation of $x$. So for every one standard deviation unit change in $x$, $y$ increases by (on\n  average)1.609 units and we are 95%\n  confident that this change is between 0.642\n  and 2.59\n\n##### As draws\n\nAs yet a more flexible alternative, if we first extract the full\nposterior draws (with `as_draws_df()`), we can then use the various\n`tidyverse` functions to focus on just the most ecologically\ninterpretable parameters before summarising.\n\nIn the following, I will use `select()` with a regex (regular\nexpression) to match only the columns that:\n\n- start with (`^`) \"b_\" followed by any amount of (`*`) any character\n  (`.`)\n- start with (`^`) \"sigma\"\n\nIt will also use `mutate()` to transform the results back onto the scale\nof the response (by expoenentiating the the posteriors before summarising.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6c.brm2 |>\n    brms::as_draws_df() |>\n    dplyr::select(matches(\"^b_.*|^sigma\")) |>\n    mutate(across(everything(), exp)) |>\n    posterior::summarise_draws(\n        median,\n        HDInterval::hdi,\n        rhat,\n        length,\n        ess_bulk, ess_tail\n    )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 8\n  variable    median lower upper  rhat length ess_bulk ess_tail\n  <chr>        <dbl> <dbl> <dbl> <dbl>  <dbl>    <dbl>    <dbl>\n1 b_Intercept   1.32 0.756  2.07  1.00   2400    2318.    1883.\n2 b_scalex      5.00 1.47  12.3   1.00   2400    2272.    2260.\n```\n\n\n:::\n:::\n\n\n\n**Conclusions**: \n\n- in the initial block of information, we are reminded on the formula\n  as well as chain fitting characteristics. We are also informed that\n  the total number of post-warmup MCMC samples is 2400.\n- the `Rhat` values for each parameter are >1.01 confirming\n  convergence of the parameter estimates across the chains\n- `Bulk_ESS`: effective sample sizes for the bulk of the posterior\n  range are a high fraction (< 0.5) of the total number of draws. This\n  suggests that the sampling was reasonably efficient and therefore\n  likely to be reliable\n- `Tail_ESS`: effective sample sizes of estimates in the tail of the\n  posterior. The tails (more extreme regions of the posterior) are\n  where the sampler is most likely to get stuck (have divergent\n  transitions etc). The fraction of effective samples from this region\n  is relatively high implying that the sampler did not get stuck in\n  such areas that are unsupported by data.\n- `b_Intercept`: when $x=0$ (its average since it is standardised), the expected value of $y$ is \n  1.322 and we are 95% confident that the true\n  value is between 0.828 and \n  2.149. Since $x=0$ is outside the observed\n  range of $x$, this y-intercept is of very limited value.\n- `b_x`: (the slope) - the rate of change in $y$ per unit (=1) change in\n  $x$. Recall that since $x$ is standardised, 1 unit representes a span of 1 standard deviation of $x$. So for every one standard deviation unit change in $x$, $y$ increases by (on\n  average) a factor of 4.999 units and we are 95%\n  confident that this change is between 1.901\n  and 13.335. This represents a ((value -1) * 100)\n  399.9% increase in $y$ per \n  unit increase in $x$.\n\n\n##### Density plots (via `gather_draws`)\n\nThe `gather_draws()` function performs the equivalent of an\n`as_draws_df()` followed by a `pivot_longer()` in order to return the\nfull posteriors in long format where they are more suitable for\ngraphing. Note, that the name of the slope parameter gets very awkward\nwhen $x$ is centered, so it is more convenient to refer to this\nparameter via a regular expression.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6c.brm2 |> get_variables()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"     \"b_scalex\"        \"Intercept\"       \"prior_Intercept\"\n [5] \"prior_b\"         \"lprior\"          \"lp__\"            \"accept_stat__\"  \n [9] \"stepsize__\"      \"treedepth__\"     \"n_leapfrog__\"    \"divergent__\"    \n[13] \"energy__\"       \n```\n\n\n:::\n\n```{.r .cell-code}\ndat6c.brm2 |>\n  gather_draws(`b_Intercept`,`b_.*x`, regex = TRUE) |> \n  mutate(across(everything(), exp)) |>\n  ggplot() +\n  geom_histogram(aes(x = .value)) +\n  facet_wrap(~.variable, scales = \"free\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/gather_draws6c-1.png){width=768}\n:::\n:::\n\n\n\nAlternatively, there are various other representations supported by\nthe `ggdist` package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6c.brm2 |>\n  gather_draws(`b_Intercept`,`b_.*x`, regex = TRUE) |> \n  mutate(across(everything(), exp)) |>\n  ggplot() +\n  stat_halfeye(aes(x = .value, y = .variable))\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/gather_draws6c2-1.png){width=480}\n:::\n:::\n\n\n\n##### R²\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6c.brm2 |>\n    bayes_R2(summary = FALSE) |>\n    median_hdci()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         y      ymin      ymax .width .point .interval\n1 0.710588 0.3429293 0.8207865   0.95 median      hdci\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- 71.059% of the total variability in $y$ can be\n  explained by its relationship to $x$\n- we are 95% confident that the strength of this relationship is\n  between 34.293% and 82.079%\n  \n:::::\n\n:::::::\n::::\n\n# Predictions\n\nWhilst linear models are useful for estimating effects (relative\ndifferences), because they are low dimensional (only focus on a small\nnumber of covariates) they are not good at absolute predictions.\nNevertheless, predicting values from linear models provides the basis\nfor investigating/estimating additional effects and generating various\ngraphics to visualise the estimates.\n\nThere are a large number of candidate routines for performing\nprediction. We will go through some of these. It is worth noting that\nin this context prediction is technically the act of estimating what\nwe expect to get if we were to collect a **single** new observation\nfrom a particular population (e.g. a specific level of fertilizer\nconcentration). Often this is not what we want. Often we want the\n**fitted** values - estimates of what we expect to get if we were to\ncollect **multiple** new observations and average them.\n\nSo while fitted values represent the expected underlying processes\noccurring in the system, predicted values represent our expectations\nfrom sampling from such processes.\n\n| Package         | Function              | Description                                                                                                | Summarise with  |\n| ---             | ---                   | ------                                                                                                     | ---             |\n| `emmeans`       | `emmeans`             | Estimated marginal means from which posteriors can be drawn (via `tidy_draws` or `gather_emmeans_draws()`)                              | `median_hdci()` |\n| `rstantools`    | `posterior_predict`   | Draw from the posterior of a prediction (includes sigma) - _predicts single observations_                  | `summarise_draws()`    |\n| `rstantools`    | `posterior_linpred`   | Draw from the posterior of the fitted values (**on the link scale**) - _predicts average observations_     | `summarise_draws()`    |\n| `rstantools`    | `posterior_epred`     | Draw from the posterior of the fitted values (**on the response scale**) - _predicts average observations_ | `summarise_draws()`    |\n| `tidybayes`     | `predicted_draws`     | Extract the posterior of _prediction_ values                                                               | `median_hdci()` |\n| `tidybayes`     | `epred_draws`         | Extract the posterior of _expected_ values                                                                 | `median_hdci()` |\n| ~~`tidybayes`~~ | ~~`fitted_draws`~~    | ~~Extract the posterior of _fitted_ values~~                                                               | `median_hdci()` |\n| `tidybayes`     | `add_predicted_draws` | Adds draws from the posterior of _predictions_ to a data frame (of prediction data)                        | `median_hdci()` |\n| `tidybayes`     | `add_fitted_draws`    | Adds draws from the posterior of _fitted_ values to a data frame (of prediction data)                      | `median_hdci()` |\n\n: {.primary .bordered .sm .paramsTable tbl-colwidths=\"[20, 20, 50, 10]\"}\n\n\nFor simple models prediction is essentially taking the model formula\ncomplete with parameter (coefficient) estimates and solving for new\nvalues of the predictor. To explore this, we will use the fitted model\nto predict Yield for a Fertilizer concentration of 110.\n\nWe will therefore start by establishing this prediction domain as a\ndata frame to use across all of the prediction routines.\n\n\n\n:::: {.panel-tabset}\n\n### Example 1 (Gaussian data)\n\n::::::: {.panel-tabset}\n\n#### Raw predictor\n\nIn each case, we will predict $y$ when $x$ is 2.5 and 5\n\nNote, for a Gaussian model `emmeans`, `posterior_epred` and\n`posterior_linpred` will all yield the same outputs.\n`posterior_predict` will yield wider credible intervals as it is\npredicting for individual observations rather than predicting means of\nlarge samples. \n\n::::: {.panel-tabset}\n\n##### Estimated marginal means\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1a.brm2 |> emmeans(~x, at = list(x = c(2.5, 5)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   x  emmean lower.HPD upper.HPD\n 2.5  0.0595     -2.59      2.55\n 5.0 -0.1437     -4.81      4.67\n\nPoint estimate displayed: median \nHPD interval probability: 0.95 \n```\n\n\n:::\n\n```{.r .cell-code}\n# OR with more control over the way posteriors are summarised\ndat1a.brm2 |>\n    emmeans(~x, at = list(x = c(2.5, 5))) |>\n    gather_emmeans_draws() |>\n    median_hdci()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 7\n      x  .value .lower .upper .width .point .interval\n  <dbl>   <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1   2.5  0.0595  -2.59   2.55   0.95 median hdci     \n2   5   -0.144   -4.81   4.67   0.95 median hdci     \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1a.emmeans <- dat1a.brm2 |>\n    emmeans(~x, at = list(x = c(2.5, 5))) |>\n    as.data.frame()\n```\n:::\n\n\n\n**Conclusions**:\n\n- the predicted (estimated) mean $y$ associated with an $x$ of 2.5 is\n  0.06\n- the predicted (estimated) mean $y$ associated with an $x$ of 5 is\n  -0.144\n- 95% HPD intervals also given\n\n##### Posterior predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1a.brm2 |>\n    posterior_predict(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable  median lower upper\n  <chr>      <dbl> <dbl> <dbl>\n1 ...1      0.0827 -3.51  3.56\n2 ...2     -0.119  -5.50  5.22\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat1a.brm2 |>\n    add_predicted_draws(newdata = data.frame(x = c(2.5, 5))) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n# Groups:   x, .row [2]\n      x  .row variable     median lower upper\n  <dbl> <int> <chr>         <dbl> <dbl> <dbl>\n1   2.5     1 .prediction  0.0771 -3.28  3.41\n2   5       2 .prediction -0.134  -5.14  5.44\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1a.pred <- dat1a.brm2 |>\n    posterior_predict(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n:::\n\n\n\n**Conclusions**:\n\n- the predicted (estimated) mean $y$ associated with an $x$ of 2.5 is\n  0.079\n- the predicted (estimated) mean $y$ associated with an $x$ of 5 is\n  -0.131\n- 95% HPD intervals also given\n\n\n##### Posterior expected values\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1a.brm2 |>\n    posterior_epred(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable  median lower upper\n  <chr>      <dbl> <dbl> <dbl>\n1 ...1      0.0595 -2.59  2.55\n2 ...2     -0.144  -4.81  4.67\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat1a.brm2 |>\n    add_epred_draws(newdata = data.frame(x = c(2.5, 5))) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n# Groups:   x, .row [2]\n      x  .row variable  median lower upper\n  <dbl> <int> <chr>      <dbl> <dbl> <dbl>\n1   2.5     1 .epred    0.0595 -2.59  2.55\n2   5       2 .epred   -0.144  -4.81  4.67\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1a.epred <- dat1a.brm2 |>\n    posterior_epred(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n:::\n\n\n\n**Conclusions**:\n\n- the fitted mean $y$ associated with an $x$ of 2.5 is\n  0.06\n- the fitted mean $y$ associated with an $x$ of 5 is\n  -0.144\n- 95% HPD intervals also given\n\n##### Posterior linear predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1a.brm2 |>\n    posterior_linpred(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable  median lower upper\n  <chr>      <dbl> <dbl> <dbl>\n1 ...1      0.0595 -2.59  2.55\n2 ...2     -0.144  -4.81  4.67\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat1a.brm2 |>\n    add_linpred_draws(newdata = data.frame(x = c(2.5, 5))) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n# Groups:   x, .row [2]\n      x  .row variable  median lower upper\n  <dbl> <int> <chr>      <dbl> <dbl> <dbl>\n1   2.5     1 .linpred  0.0595 -2.59  2.55\n2   5       2 .linpred -0.144  -4.81  4.67\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1a.linpred <- dat1a.brm2 |>\n    posterior_linpred(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n:::\n\n\n\n**Conclusions**:\n\n- the fitted mean $y$ associated with an $x$ of 2.5 is\n  0.06\n- the fitted mean $y$ associated with an $x$ of 5 is\n  -0.144\n- 95% HPD intervals also given\n\n:::::\n\n#### Centered predictor\n\nIn each case, we will predict $y$ when $x$ is 2.5 and 5\n\nNote, for a Gaussian model `emmeans`, `posterior_epred` and\n`posterior_linpred` will all yield the same outputs.\n`posterior_predict` will yield wider credible intervals as it is\npredicting for individual observations rather than predicting means of\nlarge samples. \n\n::::: {.panel-tabset}\n\n##### Estimated marginal means\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1b.brm2 |> emmeans(~x, at = list(x = c(2.5, 5)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   x  emmean lower.HPD upper.HPD\n 2.5  0.0433     -2.59      2.51\n 5.0 -0.1834     -4.86      4.57\n\nPoint estimate displayed: median \nHPD interval probability: 0.95 \n```\n\n\n:::\n\n```{.r .cell-code}\n# OR with more control over the way posteriors are summarised\ndat1b.brm2 |>\n    emmeans(~x, at = list(x = c(2.5, 5))) |>\n    gather_emmeans_draws() |>\n    median_hdci()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 7\n      x  .value .lower .upper .width .point .interval\n  <dbl>   <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1   2.5  0.0433  -2.59   2.51   0.95 median hdci     \n2   5   -0.183   -4.86   4.57   0.95 median hdci     \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1b.emmeans <- dat1b.brm2 |>\n    emmeans(~x, at = list(x = c(2.5, 5))) |>\n    as.data.frame()\n```\n:::\n\n\n\n**Conclusions**:\n\n- the predicted (estimated) mean $y$ associated with an $x$ of 2.5 is\n  0.043\n- the predicted (estimated) mean $y$ associated with an $x$ of 5 is\n  -0.183\n- 95% HPD intervals also given\n\n##### Posterior predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1b.brm2 |>\n    posterior_predict(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable  median lower upper\n  <chr>      <dbl> <dbl> <dbl>\n1 ...1      0.0357 -3.53  3.43\n2 ...2     -0.256  -5.44  5.02\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat1b.brm2 |>\n    add_predicted_draws(newdata = data.frame(x = c(2.5, 5))) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n# Groups:   x, .row [2]\n      x  .row variable     median lower upper\n  <dbl> <int> <chr>         <dbl> <dbl> <dbl>\n1   2.5     1 .prediction  0.0552 -3.45  3.34\n2   5       2 .prediction -0.188  -5.47  4.97\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1b.pred <- dat1b.brm2 |>\n    posterior_predict(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n:::\n\n\n\n**Conclusions**:\n\n- the predicted (estimated) mean $y$ associated with an $x$ of 2.5 is\n  0.059\n- the predicted (estimated) mean $y$ associated with an $x$ of 5 is\n  -0.148\n- 95% HPD intervals also given\n\n\n##### Posterior expected values\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1b.brm2 |>\n    posterior_epred(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable  median lower upper\n  <chr>      <dbl> <dbl> <dbl>\n1 ...1      0.0433 -2.59  2.51\n2 ...2     -0.183  -4.86  4.57\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat1b.brm2 |>\n    add_epred_draws(newdata = data.frame(x = c(2.5, 5))) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n# Groups:   x, .row [2]\n      x  .row variable  median lower upper\n  <dbl> <int> <chr>      <dbl> <dbl> <dbl>\n1   2.5     1 .epred    0.0433 -2.59  2.51\n2   5       2 .epred   -0.183  -4.86  4.57\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1b.epred <- dat1b.brm2 |>\n    posterior_epred(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n:::\n\n\n\n**Conclusions**:\n\n- the fitted mean $y$ associated with an $x$ of 2.5 is\n  0.043\n- the fitted mean $y$ associated with an $x$ of 5 is\n  -0.183\n- 95% HPD intervals also given\n\n##### Posterior linear predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1b.brm2 |>\n    posterior_linpred(newdata = data.frame(x = c(2.5, 5))) |>\n    median_hdci()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            y      ymin     ymax .width .point .interval\n1 -0.05408044 -4.058454 3.885879   0.95 median      hdci\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat1b.brm2 |>\n    add_linpred_draws(newdata = data.frame(x = c(2.5, 5))) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n# Groups:   x, .row [2]\n      x  .row variable  median lower upper\n  <dbl> <int> <chr>      <dbl> <dbl> <dbl>\n1   2.5     1 .linpred  0.0433 -2.59  2.51\n2   5       2 .linpred -0.183  -4.86  4.57\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1b.linpred <- dat1b.brm2 |>\n    posterior_linpred(newdata = data.frame(x = c(2.5, 5))) |>\n    median_hdci()\n```\n:::\n\n\n\n**Conclusions**:\n\n- the fitted mean $y$ associated with an $x$ of 2.5 is\n  -4.058\n- the fitted mean $y$ associated with an $x$ of 5 is\n  NA\n- 95% HPD intervals also given\n\n:::::\n\n#### Standardised predictor\n\nIn each case, we will predict $y$ when $x$ is 2.5 and 5\n\nNote, for a Gaussian model `emmeans`, `posterior_epred` and\n`posterior_linpred` will all yield the same outputs.\n`posterior_predict` will yield wider credible intervals as it is\npredicting for individual observations rather than predicting means of\nlarge samples. \n\n::::: {.panel-tabset}\n\n##### Estimated marginal means\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1c.brm2 |> emmeans(~x, at = list(x = c(2.5, 5)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   x emmean lower.HPD upper.HPD\n 2.5  0.037     -2.48      2.74\n 5.0 -0.181     -5.10      4.80\n\nPoint estimate displayed: median \nHPD interval probability: 0.95 \n```\n\n\n:::\n\n```{.r .cell-code}\n# OR with more control over the way posteriors are summarised\ndat1c.brm2 |>\n    emmeans(~x, at = list(x = c(2.5, 5))) |>\n    gather_emmeans_draws() |>\n    median_hdci()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 7\n      x  .value .lower .upper .width .point .interval\n  <dbl>   <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1   2.5  0.0370  -2.48   2.74   0.95 median hdci     \n2   5   -0.181   -5.06   4.85   0.95 median hdci     \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1c.emmeans <- dat1c.brm2 |>\n    emmeans(~x, at = list(x = c(2.5, 5))) |>\n    as.data.frame()\n```\n:::\n\n\n\n**Conclusions**:\n\n- the predicted (estimated) mean $y$ associated with an $x$ of 2.5 is\n  0.037\n- the predicted (estimated) mean $y$ associated with an $x$ of 5 is\n  -0.181\n- 95% HPD intervals also given\n\n##### Posterior predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1c.brm2 |>\n    posterior_predict(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable  median lower upper\n  <chr>      <dbl> <dbl> <dbl>\n1 ...1     -0.0127 -3.39  3.59\n2 ...2     -0.170  -5.19  5.95\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat1c.brm2 |>\n    add_predicted_draws(newdata = data.frame(x = c(2.5, 5))) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n# Groups:   x, .row [2]\n      x  .row variable     median lower upper\n  <dbl> <int> <chr>         <dbl> <dbl> <dbl>\n1   2.5     1 .prediction  0.0638 -3.27  3.63\n2   5       2 .prediction -0.152  -6.34  5.02\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1c.pred <- dat1c.brm2 |>\n    posterior_predict(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n:::\n\n\n\n**Conclusions**:\n\n- the predicted (estimated) mean $y$ associated with an $x$ of 2.5 is\n  0.031\n- the predicted (estimated) mean $y$ associated with an $x$ of 5 is\n  -0.193\n- 95% HPD intervals also given\n\n\n##### Posterior expected values\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1c.brm2 |>\n    posterior_epred(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable  median lower upper\n  <chr>      <dbl> <dbl> <dbl>\n1 ...1      0.0370 -2.48  2.74\n2 ...2     -0.181  -5.10  4.80\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat1c.brm2 |>\n    add_epred_draws(newdata = data.frame(x = c(2.5, 5))) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n# Groups:   x, .row [2]\n      x  .row variable  median lower upper\n  <dbl> <int> <chr>      <dbl> <dbl> <dbl>\n1   2.5     1 .epred    0.0370 -2.48  2.74\n2   5       2 .epred   -0.181  -5.10  4.80\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1c.epred <- dat1c.brm2 |>\n    posterior_epred(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n:::\n\n\n\n**Conclusions**:\n\n- the fitted mean $y$ associated with an $x$ of 2.5 is\n  0.037\n- the fitted mean $y$ associated with an $x$ of 5 is\n  -0.181\n- 95% HPD intervals also given\n\n##### Posterior linear predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1c.brm2 |>\n    posterior_linpred(newdata = data.frame(x = c(2.5, 5))) |>\n    median_hdci()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            y     ymin     ymax .width .point .interval\n1 -0.04678397 -4.02588 4.194853   0.95 median      hdci\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat1c.brm2 |>\n    add_linpred_draws(newdata = data.frame(x = c(2.5, 5))) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n# Groups:   x, .row [2]\n      x  .row variable  median lower upper\n  <dbl> <int> <chr>      <dbl> <dbl> <dbl>\n1   2.5     1 .linpred  0.0370 -2.48  2.74\n2   5       2 .linpred -0.181  -5.10  4.80\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1c.linpred <- dat1c.brm2 |>\n    posterior_linpred(newdata = data.frame(x = c(2.5, 5))) |>\n    median_hdci()\n```\n:::\n\n\n\n**Conclusions**:\n\n- the fitted mean $y$ associated with an $x$ of 2.5 is\n  -4.026\n- the fitted mean $y$ associated with an $x$ of 5 is\n  NA\n- 95% HPD intervals also given\n\n:::::\n:::::::\n\n### Example 2 (categorical predictor)\n\n::::::: {.panel-tabset}\n\n#### Treatment contrasts\n\nIn each case, we will predict $y$ when $x$ is \"control\", \"medium\" and \"high\"\n\nNote, for a Gaussian model `emmeans`, `posterior_epred` and\n`posterior_linpred` will all yield the same outputs.\n`posterior_predict` will yield wider credible intervals as it is\npredicting for individual observations rather than predicting means of\nlarge samples. \n\n::::: {.panel-tabset}\n\n##### Estimated marginal means\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2a.brm2 |> emmeans(~x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n x       emmean lower.HPD upper.HPD\n control   20.9     16.99      25.1\n medium    20.0     16.08      24.2\n high      12.1      7.95      16.4\n\nPoint estimate displayed: median \nHPD interval probability: 0.95 \n```\n\n\n:::\n\n```{.r .cell-code}\n# OR with more control over the way posteriors are summarised\ndat2a.brm2 |>\n    emmeans(~x) |>\n    gather_emmeans_draws() |>\n    median_hdci()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 7\n  x       .value .lower .upper .width .point .interval\n  <fct>    <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1 control   20.9  17.0    25.1   0.95 median hdci     \n2 medium    20.0  16.3    24.4   0.95 median hdci     \n3 high      12.1   7.95   16.4   0.95 median hdci     \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2a.emmeans <- dat2a.brm2 |>\n    emmeans(~x) |>\n    as.data.frame()\n```\n:::\n\n\n\n**Conclusions**:\n\n- the predicted (estimated) mean $y$ associated with an $x$ of\n  \"control\" is 20.875\n- the predicted (estimated) mean $y$ associated with an $x$ of\n  \"medium\" is 20.007\n- the predicted (estimated) mean $y$ associated with an $x$ of\n  \"high\" is 12.094\n- 95% HPD intervals also given\n\n##### Posterior predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2a.brm2 |>\n  posterior_predict(newdata = data.frame(x =\n          c(\"control\", \"medium\", \"high\"))) |>\n  summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 4\n  variable median lower upper\n  <chr>     <dbl> <dbl> <dbl>\n1 ...1       20.8 10.9   30.6\n2 ...2       19.8  9.89  30.3\n3 ...3       11.9  2.12  21.7\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat2a.brm2 |>\n    add_predicted_draws(newdata = data.frame(x = c(\"control\", \"medium\", \"high\"))) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 6\n# Groups:   x, .row [3]\n  x        .row variable    median lower upper\n  <chr>   <int> <chr>        <dbl> <dbl> <dbl>\n1 control     1 .prediction   20.9 10.6   30.8\n2 high        3 .prediction   12.0  2.42  22.1\n3 medium      2 .prediction   19.9  9.54  29.7\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2a.pred <- dat2a.brm2 |>\n    posterior_predict(newdata = data.frame(x = c(\"control\", \"medium\", \"high\"))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n:::\n\n\n\n**Conclusions**:\n\n- the predicted (estimated) mean $y$ associated with an $x$ of\n  \"control\" is 20.899\n- the predicted (estimated) mean $y$ associated with an $x$ of\n  \"medium\" is 20.061\n- the predicted (estimated) mean $y$ associated with an $x$ of\n  \"high\" is 12.205\n- 95% HPD intervals also given\n\n##### Posterior expected values\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2a.brm2 |>\n    posterior_epred(newdata = data.frame(x = c(\"control\", \"medium\", \"high\"))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 4\n  variable median lower upper\n  <chr>     <dbl> <dbl> <dbl>\n1 ...1       20.9 17.0   25.1\n2 ...2       20.0 16.1   24.2\n3 ...3       12.1  7.95  16.4\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat2a.brm2 |>\n    add_epred_draws(newdata = data.frame(x = c(\"control\", \"medium\", \"high\"))) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 6\n# Groups:   x, .row [3]\n  x        .row variable median lower upper\n  <chr>   <int> <chr>     <dbl> <dbl> <dbl>\n1 control     1 .epred     20.9 17.0   25.1\n2 high        3 .epred     12.1  7.95  16.4\n3 medium      2 .epred     20.0 16.1   24.2\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2a.epred <- dat2a.brm2 |>\n    posterior_epred(newdata = data.frame(x = c(\"control\", \"medium\", \"high\"))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n:::\n\n\n\n**Conclusions**:\n\n- the predicted (estimated) mean $y$ associated with an $x$ of\n  \"control\" is 20.875\n- the predicted (estimated) mean $y$ associated with an $x$ of\n  \"medium\" is 20.007\n- the predicted (estimated) mean $y$ associated with an $x$ of\n  \"high\" is 12.094\n- 95% HPD intervals also given\n\n##### Posterior linear predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2a.brm2 |>\n    posterior_linpred(newdata = data.frame(x = c(\"control\", \"medium\", \"high\"))) |>\n    median_hdci()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         y     ymin     ymax .width .point .interval\n1 19.13564 9.279738 24.10563   0.95 median      hdci\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat2a.brm2 |>\n    add_linpred_draws(newdata = data.frame(x = c(\"control\", \"medium\", \"high\"))) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 6\n# Groups:   x, .row [3]\n  x        .row variable median lower upper\n  <chr>   <int> <chr>     <dbl> <dbl> <dbl>\n1 control     1 .linpred   20.9 17.0   25.1\n2 high        3 .linpred   12.1  7.95  16.4\n3 medium      2 .linpred   20.0 16.1   24.2\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2a.linpred <- dat2a.brm2 |>\n    posterior_linpred(newdata = data.frame(x = c(\"control\", \"medium\", \"high\"))) |>\n    median_hdci()\n```\n:::\n\n\n\n**Conclusions**:\n\n- the predicted (estimated) mean $y$ associated with an $x$ of\n  \"control\" is 9.28\n- the predicted (estimated) mean $y$ associated with an $x$ of\n  \"medium\" is NA\n- the predicted (estimated) mean $y$ associated with an $x$ of\n  \"high\" is NA\n- 95% HPD intervals also given\n:::::\n\n#### Means parameterisation\n\nIn each case, we will predict $y$ when $x$ is \"control\", \"medium\" and \"high\"\n\nNote, for a Gaussian model `emmeans`, `posterior_epred` and\n`posterior_linpred` will all yield the same outputs.\n`posterior_predict` will yield wider credible intervals as it is\npredicting for individual observations rather than predicting means of\nlarge samples. \n\n::::: {.panel-tabset}\n\n##### Estimated marginal means\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2b.brm2 |> emmeans(~x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n x       emmean lower.HPD upper.HPD\n control   20.3     15.85      24.5\n medium    18.6     14.74      23.4\n high      11.0      6.49      15.6\n\nPoint estimate displayed: median \nHPD interval probability: 0.95 \n```\n\n\n:::\n\n```{.r .cell-code}\n# OR with more control over the way posteriors are summarised\ndat2b.brm2 |>\n    emmeans(~x) |>\n    gather_emmeans_draws() |>\n    median_hdci()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 7\n  x       .value .lower .upper .width .point .interval\n  <fct>    <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1 control   20.3  15.8    24.5   0.95 median hdci     \n2 medium    18.6  14.5    23.1   0.95 median hdci     \n3 high      11.0   6.29   15.5   0.95 median hdci     \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2b.emmeans <- dat2b.brm2 |>\n    emmeans(~x) |>\n    as.data.frame()\n```\n:::\n\n\n\n**Conclusions**:\n\n- the predicted (estimated) mean $y$ associated with an $x$ of\n  \"control\" is 20.276\n- the predicted (estimated) mean $y$ associated with an $x$ of\n  \"medium\" is 18.635\n- the predicted (estimated) mean $y$ associated with an $x$ of\n  \"high\" is 11.028\n- 95% HPD intervals also given\n\n##### Posterior predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2b.brm2 |>\n  posterior_predict(newdata = data.frame(x =\n          c(\"control\", \"medium\", \"high\"))) |>\n  summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 4\n  variable median  lower upper\n  <chr>     <dbl>  <dbl> <dbl>\n1 ...1       20.4 10.8    30.4\n2 ...2       18.6  8.26   28.1\n3 ...3       11.1  0.832  21.9\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat2b.brm2 |>\n    add_predicted_draws(newdata = data.frame(x = c(\"control\", \"medium\", \"high\"))) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 6\n# Groups:   x, .row [3]\n  x        .row variable    median lower upper\n  <chr>   <int> <chr>        <dbl> <dbl> <dbl>\n1 control     1 .prediction   20.2 10.9   30.5\n2 high        3 .prediction   11.1  1.33  21.6\n3 medium      2 .prediction   18.7  8.66  28.6\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2b.pred <- dat2b.brm2 |>\n    posterior_predict(newdata = data.frame(x = c(\"control\", \"medium\", \"high\"))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n:::\n\n\n\n**Conclusions**:\n\n- the predicted (estimated) mean $y$ associated with an $x$ of\n  \"control\" is 20.18\n- the predicted (estimated) mean $y$ associated with an $x$ of\n  \"medium\" is 18.863\n- the predicted (estimated) mean $y$ associated with an $x$ of\n  \"high\" is 10.819\n- 95% HPD intervals also given\n\n##### Posterior expected values\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2b.brm2 |>\n    posterior_epred(newdata = data.frame(x = c(\"control\", \"medium\", \"high\"))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 4\n  variable median lower upper\n  <chr>     <dbl> <dbl> <dbl>\n1 ...1       20.3 15.8   24.5\n2 ...2       18.6 14.7   23.4\n3 ...3       11.0  6.49  15.6\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat2b.brm2 |>\n    add_epred_draws(newdata = data.frame(x = c(\"control\", \"medium\", \"high\"))) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 6\n# Groups:   x, .row [3]\n  x        .row variable median lower upper\n  <chr>   <int> <chr>     <dbl> <dbl> <dbl>\n1 control     1 .epred     20.3 15.8   24.5\n2 high        3 .epred     11.0  6.49  15.6\n3 medium      2 .epred     18.6 14.7   23.4\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2b.epred <- dat2b.brm2 |>\n    posterior_epred(newdata = data.frame(x = c(\"control\", \"medium\", \"high\"))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n:::\n\n\n\n**Conclusions**:\n\n- the predicted (estimated) mean $y$ associated with an $x$ of\n  \"control\" is 20.276\n- the predicted (estimated) mean $y$ associated with an $x$ of\n  \"medium\" is 18.635\n- the predicted (estimated) mean $y$ associated with an $x$ of\n  \"high\" is 11.028\n- 95% HPD intervals also given\n\n##### Posterior linear predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2b.brm2 |>\n    posterior_linpred(newdata = data.frame(x = c(\"control\", \"medium\", \"high\"))) |>\n    median_hdci()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         y     ymin     ymax .width .point .interval\n1 17.98865 8.309788 23.74624   0.95 median      hdci\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat2b.brm2 |>\n    add_linpred_draws(newdata = data.frame(x = c(\"control\", \"medium\", \"high\"))) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 6\n# Groups:   x, .row [3]\n  x        .row variable median lower upper\n  <chr>   <int> <chr>     <dbl> <dbl> <dbl>\n1 control     1 .linpred   20.3 15.8   24.5\n2 high        3 .linpred   11.0  6.49  15.6\n3 medium      2 .linpred   18.6 14.7   23.4\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2b.linpred <- dat2b.brm2 |>\n    posterior_linpred(newdata = data.frame(x = c(\"control\", \"medium\", \"high\"))) |>\n    median_hdci()\n```\n:::\n\n\n\n**Conclusions**:\n\n- the predicted (estimated) mean $y$ associated with an $x$ of\n  \"control\" is 8.31\n- the predicted (estimated) mean $y$ associated with an $x$ of\n  \"medium\" is NA\n- the predicted (estimated) mean $y$ associated with an $x$ of\n  \"high\" is NA\n- 95% HPD intervals also given\n\n:::::\n:::::::\n\n### Example 3 (Poisson data)\n\n::::::: {.panel-tabset}\n\n#### Raw predictor\n\nIn each case, we will predict $y$ when $x$ is 2.5 and 5\n\nNote, for a Gaussian model `emmeans`, `posterior_epred` and\n`posterior_linpred` will all yield the same outputs.\n`posterior_predict` will yield wider credible intervals as it is\npredicting for individual observations rather than predicting means of\nlarge samples. \n\n::::: {.panel-tabset}\n\n##### Estimated marginal means\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3a.brm2 |> emmeans(~x, at = list(x = c(2.5, 5)), type = \"response\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   x rate lower.HPD upper.HPD\n 2.5 2.45      1.41      3.73\n 5.0 5.79      4.07      7.56\n\nPoint estimate displayed: median \nResults are back-transformed from the log scale \nHPD interval probability: 0.95 \n```\n\n\n:::\n\n```{.r .cell-code}\n# OR with more control over the way posteriors are summarised\ndat3a.brm2 |>\n    emmeans(~x, at = list(x = c(2.5, 5))) |>\n    gather_emmeans_draws() |>\n    mutate(.value = exp(.value)) |> \n    median_hdci()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 7\n      x .value .lower .upper .width .point .interval\n  <dbl>  <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1   2.5   2.45   1.41   3.73   0.95 median hdci     \n2   5     5.79   4.07   7.56   0.95 median hdci     \n```\n\n\n:::\n\n```{.r .cell-code}\n# OR with yet more control over the way posteriors are summarised\ndat3a.brm2 |>\n    emmeans(~x, at = list(x = c(2.5, 5))) |>\n    gather_emmeans_draws() |>\n    dplyr::select(-.chain) |> \n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n# Groups:   x [2]\n      x variable median lower upper\n  <dbl> <chr>     <dbl> <dbl> <dbl>\n1   2.5 .value    0.894 0.391  1.36\n2   5   .value    1.76  1.42   2.04\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- the predicted (estimated) mean $y$ associated with an $x$ of 2.5 is\n  2.445\n- the predicted (estimated) mean $y$ associated with an $x$ of 5 is\n  5.794\n- 95% HPD intervals also given\n\n##### Posterior predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3a.brm2 |>\n    posterior_predict(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable median lower upper\n  <chr>     <dbl> <dbl> <dbl>\n1 ...1          2     0     6\n2 ...2          6     0    10\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat3a.brm2 |>\n    add_predicted_draws(newdata = data.frame(x = c(2.5, 5))) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n# Groups:   x, .row [2]\n      x  .row variable    median lower upper\n  <dbl> <int> <chr>        <dbl> <dbl> <dbl>\n1   2.5     1 .prediction      2     0     6\n2   5       2 .prediction      6     0    10\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3a.pred <- dat3a.brm2 |>\n    posterior_predict(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n:::\n\n\n\n**Conclusions**:\n\n- the predicted (estimated) mean $y$ associated with an $x$ of 2.5 is\n  2\n- the predicted (estimated) mean $y$ associated with an $x$ of 5 is\n  6\n- 95% HPD intervals also given\n\n\n##### Posterior expected values\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3a.brm2 |>\n    posterior_epred(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable median lower upper\n  <chr>     <dbl> <dbl> <dbl>\n1 ...1       2.45  1.41  3.73\n2 ...2       5.79  4.07  7.56\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat3a.brm2 |>\n    add_epred_draws(newdata = data.frame(x = c(2.5, 5))) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n# Groups:   x, .row [2]\n      x  .row variable median lower upper\n  <dbl> <int> <chr>     <dbl> <dbl> <dbl>\n1   2.5     1 .epred     2.45  1.41  3.73\n2   5       2 .epred     5.79  4.07  7.56\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- the fitted mean $y$ associated with an $x$ of 2.5 is\n  2.445\n- the fitted mean $y$ associated with an $x$ of 5 is\n  5.794\n- 95% HPD intervals also given\n\n##### Posterior linear predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3a.brm2 |>\n    posterior_linpred(newdata = data.frame(x = c(2.5, 5))) |>\n    exp() |> \n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable median lower upper\n  <chr>     <dbl> <dbl> <dbl>\n1 ...1       2.45  1.41  3.73\n2 ...2       5.79  4.07  7.56\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat3a.brm2 |>\n    add_linpred_draws(newdata = data.frame(x = c(2.5, 5))) |>\n    mutate(.linpred = exp(.linpred)) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n# Groups:   x, .row [2]\n      x  .row variable median lower upper\n  <dbl> <int> <chr>     <dbl> <dbl> <dbl>\n1   2.5     1 .linpred   2.45  1.41  3.73\n2   5       2 .linpred   5.79  4.07  7.56\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- the fitted mean $y$ associated with an $x$ of 2.5 is\n  2.445\n- the fitted mean $y$ associated with an $x$ of 5 is\n  5.794\n- 95% HPD intervals also given\n\n:::::\n\n#### Centered predictor\n\nIn each case, we will predict $y$ when $x$ is 2.5 and 5\n\nNote, for a Gaussian model `emmeans`, `posterior_epred` and\n`posterior_linpred` will all yield the same outputs.\n`posterior_predict` will yield wider credible intervals as it is\npredicting for individual observations rather than predicting means of\nlarge samples. \n\n::::: {.panel-tabset}\n\n##### Estimated marginal means\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3b.brm2 |> emmeans(~x, at = list(x = c(2.5, 5)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   x emmean lower.HPD upper.HPD\n 2.5  0.901     0.377      1.37\n 5.0  1.757     1.450      2.05\n\nPoint estimate displayed: median \nResults are given on the log (not the response) scale. \nHPD interval probability: 0.95 \n```\n\n\n:::\n\n```{.r .cell-code}\n# OR with more control over the way posteriors are summarised\ndat3b.brm2 |>\n    emmeans(~x, at = list(x = c(2.5, 5))) |>\n    gather_emmeans_draws() |>\n    median_hdci()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 7\n      x .value .lower .upper .width .point .interval\n  <dbl>  <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1   2.5  0.901  0.388   1.38   0.95 median hdci     \n2   5    1.76   1.45    2.06   0.95 median hdci     \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3b.emmeans <- dat3b.brm2 |>\n    emmeans(~x, at = list(x = c(2.5, 5))) |>\n    as.data.frame()\n```\n:::\n\n\n\n**Conclusions**:\n\n- the predicted (estimated) mean $y$ associated with an $x$ of 2.5 is\n  0.901\n- the predicted (estimated) mean $y$ associated with an $x$ of 5 is\n  1.757\n- 95% HPD intervals also given\n\n##### Posterior predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3b.brm2 |>\n    posterior_predict(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable median lower upper\n  <chr>     <dbl> <dbl> <dbl>\n1 ...1          2     0     6\n2 ...2          6     1    11\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat3b.brm2 |>\n    add_predicted_draws(newdata = data.frame(x = c(2.5, 5))) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n# Groups:   x, .row [2]\n      x  .row variable    median lower upper\n  <dbl> <int> <chr>        <dbl> <dbl> <dbl>\n1   2.5     1 .prediction      2     0     6\n2   5       2 .prediction      6     1    10\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3b.pred <- dat3b.brm2 |>\n    posterior_predict(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n:::\n\n\n\n**Conclusions**:\n\n- the predicted (estimated) mean $y$ associated with an $x$ of 2.5 is\n  2\n- the predicted (estimated) mean $y$ associated with an $x$ of 5 is\n  6\n- 95% HPD intervals also given\n\n\n##### Posterior expected values\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3b.brm2 |>\n    posterior_epred(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable median lower upper\n  <chr>     <dbl> <dbl> <dbl>\n1 ...1       2.46  1.40  3.84\n2 ...2       5.80  4.26  7.79\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat3b.brm2 |>\n    add_epred_draws(newdata = data.frame(x = c(2.5, 5))) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n# Groups:   x, .row [2]\n      x  .row variable median lower upper\n  <dbl> <int> <chr>     <dbl> <dbl> <dbl>\n1   2.5     1 .epred     2.46  1.40  3.84\n2   5       2 .epred     5.80  4.26  7.79\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3b.epred <- dat3b.brm2 |>\n    posterior_epred(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n:::\n\n\n\n**Conclusions**:\n\n- the fitted mean $y$ associated with an $x$ of 2.5 is\n  2.463\n- the fitted mean $y$ associated with an $x$ of 5 is\n  5.795\n- 95% HPD intervals also given\n\n##### Posterior linear predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3b.brm2 |>\n    posterior_linpred(newdata = data.frame(x = c(2.5, 5))) |>\n    exp() |> \n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable median lower upper\n  <chr>     <dbl> <dbl> <dbl>\n1 ...1       2.46  1.40  3.84\n2 ...2       5.80  4.26  7.79\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat3b.brm2 |>\n    add_linpred_draws(newdata = data.frame(x = c(2.5, 5))) |>\n    mutate(.linpred = exp(.linpred)) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n# Groups:   x, .row [2]\n      x  .row variable median lower upper\n  <dbl> <int> <chr>     <dbl> <dbl> <dbl>\n1   2.5     1 .linpred   2.46  1.40  3.84\n2   5       2 .linpred   5.80  4.26  7.79\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3b.linpred <- dat3b.brm2 |>\n    posterior_linpred(newdata = data.frame(x = c(2.5, 5))) |>\n    exp() |> \n    summarise_draws(median, HDInterval::hdi)\n```\n:::\n\n\n\n**Conclusions**:\n\n- the fitted mean $y$ associated with an $x$ of 2.5 is\n  2.463\n- the fitted mean $y$ associated with an $x$ of 5 is\n  5.795\n- 95% HPD intervals also given\n\n:::::\n\n#### Standardised predictor\n\nIn each case, we will predict $y$ when $x$ is 2.5 and 5\n\nNote, for a Gaussian model `emmeans`, `posterior_epred` and\n`posterior_linpred` will all yield the same outputs.\n`posterior_predict` will yield wider credible intervals as it is\npredicting for individual observations rather than predicting means of\nlarge samples. \n\n::::: {.panel-tabset}\n\n##### Estimated marginal means\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3c.brm2 |> emmeans(~x, at = list(x = c(2.5, 5)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   x emmean lower.HPD upper.HPD\n 2.5   0.91     0.421      1.40\n 5.0   1.76     1.442      2.06\n\nPoint estimate displayed: median \nResults are given on the log (not the response) scale. \nHPD interval probability: 0.95 \n```\n\n\n:::\n\n```{.r .cell-code}\n# OR with more control over the way posteriors are summarised\ndat3c.brm2 |>\n    emmeans(~x, at = list(x = c(2.5, 5))) |>\n    gather_emmeans_draws() |>\n    median_hdci()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 7\n      x .value .lower .upper .width .point .interval\n  <dbl>  <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1   2.5  0.910  0.421   1.40   0.95 median hdci     \n2   5    1.76   1.44    2.06   0.95 median hdci     \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3c.emmeans <- dat3c.brm2 |>\n    emmeans(~x, at = list(x = c(2.5, 5))) |>\n    as.data.frame()\n```\n:::\n\n\n\n**Conclusions**:\n\n- the predicted (estimated) mean $y$ associated with an $x$ of 2.5 is\n  0.91\n- the predicted (estimated) mean $y$ associated with an $x$ of 5 is\n  1.761\n- 95% HPD intervals also given\n\n##### Posterior predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3c.brm2 |>\n    posterior_predict(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable median lower upper\n  <chr>     <dbl> <dbl> <dbl>\n1 ...1          2     0     6\n2 ...2          6     2    11\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat3c.brm2 |>\n    add_predicted_draws(newdata = data.frame(x = c(2.5, 5))) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n# Groups:   x, .row [2]\n      x  .row variable    median lower upper\n  <dbl> <int> <chr>        <dbl> <dbl> <dbl>\n1   2.5     1 .prediction      2     0     6\n2   5       2 .prediction      6     1    11\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3c.pred <- dat3c.brm2 |>\n    posterior_predict(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n:::\n\n\n\n**Conclusions**:\n\n- the predicted (estimated) mean $y$ associated with an $x$ of 2.5 is\n  2\n- the predicted (estimated) mean $y$ associated with an $x$ of 5 is\n  6\n- 95% HPD intervals also given\n\n\n##### Posterior expected values\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3c.brm2 |>\n    posterior_epred(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable median lower upper\n  <chr>     <dbl> <dbl> <dbl>\n1 ...1       2.48  1.45  3.88\n2 ...2       5.82  4.17  7.76\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat3c.brm2 |>\n    add_epred_draws(newdata = data.frame(x = c(2.5, 5))) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n# Groups:   x, .row [2]\n      x  .row variable median lower upper\n  <dbl> <int> <chr>     <dbl> <dbl> <dbl>\n1   2.5     1 .epred     2.48  1.45  3.88\n2   5       2 .epred     5.82  4.17  7.76\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3c.epred <- dat3c.brm2 |>\n    posterior_epred(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n:::\n\n\n\n**Conclusions**:\n\n- the fitted mean $y$ associated with an $x$ of 2.5 is\n  2.483\n- the fitted mean $y$ associated with an $x$ of 5 is\n  5.818\n- 95% HPD intervals also given\n\n##### Posterior linear predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3c.brm2 |>\n    posterior_linpred(newdata = data.frame(x = c(2.5, 5))) |>\n    exp() |> \n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable median lower upper\n  <chr>     <dbl> <dbl> <dbl>\n1 ...1       2.48  1.45  3.88\n2 ...2       5.82  4.17  7.76\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat3c.brm2 |>\n    add_linpred_draws(newdata = data.frame(x = c(2.5, 5))) |>\n    mutate(.linpred = exp(.linpred)) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n# Groups:   x, .row [2]\n      x  .row variable median lower upper\n  <dbl> <int> <chr>     <dbl> <dbl> <dbl>\n1   2.5     1 .linpred   2.48  1.45  3.88\n2   5       2 .linpred   5.82  4.17  7.76\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat3c.linpred <- dat3c.brm2 |>\n    posterior_linpred(newdata = data.frame(x = c(2.5, 5))) |>\n    exp() |> \n    summarise_draws(median, HDInterval::hdi)\n```\n:::\n\n\n\n**Conclusions**:\n\n- the fitted mean $y$ associated with an $x$ of 2.5 is\n  2.483\n- the fitted mean $y$ associated with an $x$ of 5 is\n  5.818\n- 95% HPD intervals also given\n\n:::::\n:::::::\n\n### Example 4 (NB data)\n\n::::::: {.panel-tabset}\n\n#### Raw predictor\n\nIn each case, we will predict $y$ when $x$ is 2.5 and 5\n\nNote, for a Gaussian model `emmeans`, `posterior_epred` and\n`posterior_linpred` will all yield the same outputs.\n`posterior_predict` will yield wider credible intervals as it is\npredicting for individual observations rather than predicting means of\nlarge samples. \n\n::::: {.panel-tabset}\n\n##### Estimated marginal means\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4a.brm2 |> emmeans(~x, at = list(x = c(2.5, 5)), type = \"response\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   x prob lower.HPD upper.HPD\n 2.5 2.93      1.50      4.68\n 5.0 5.92      4.02      8.11\n\nPoint estimate displayed: median \nResults are back-transformed from the log scale \nHPD interval probability: 0.95 \n```\n\n\n:::\n\n```{.r .cell-code}\n# OR with more control over the way posteriors are summarised\ndat4a.brm2 |>\n    emmeans(~x, at = list(x = c(2.5, 5))) |>\n    gather_emmeans_draws() |>\n    mutate(.value = exp(.value)) |> \n    median_hdci()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 7\n      x .value .lower .upper .width .point .interval\n  <dbl>  <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1   2.5   2.93   1.50   4.68   0.95 median hdci     \n2   5     5.92   4.02   8.11   0.95 median hdci     \n```\n\n\n:::\n\n```{.r .cell-code}\n# OR with yet more control over the way posteriors are summarised\ndat4a.brm2 |>\n    emmeans(~x, at = list(x = c(2.5, 5))) |>\n    gather_emmeans_draws() |>\n    dplyr::select(-.chain) |> \n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n# Groups:   x [2]\n      x variable median lower upper\n  <dbl> <chr>     <dbl> <dbl> <dbl>\n1   2.5 .value     1.07 0.563  1.63\n2   5   .value     1.78 1.46   2.14\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- the predicted (estimated) mean $y$ associated with an $x$ of 2.5 is\n  2.928\n- the predicted (estimated) mean $y$ associated with an $x$ of 5 is\n  5.92\n- 95% HPD intervals also given\n\n##### Posterior predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4a.brm2 |>\n    posterior_predict(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable median lower upper\n  <chr>     <dbl> <dbl> <dbl>\n1 ...1          3     0     7\n2 ...2          6     0    11\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat4a.brm2 |>\n    add_predicted_draws(newdata = data.frame(x = c(2.5, 5))) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n# Groups:   x, .row [2]\n      x  .row variable    median lower upper\n  <dbl> <int> <chr>        <dbl> <dbl> <dbl>\n1   2.5     1 .prediction      3     0     7\n2   5       2 .prediction      6     0    11\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4a.pred <- dat4a.brm2 |>\n    posterior_predict(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n:::\n\n\n\n**Conclusions**:\n\n- the predicted (estimated) mean $y$ associated with an $x$ of 2.5 is\n  3\n- the predicted (estimated) mean $y$ associated with an $x$ of 5 is\n  6\n- 95% HPD intervals also given\n\n\n##### Posterior expected values\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4a.brm2 |>\n    posterior_epred(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable median lower upper\n  <chr>     <dbl> <dbl> <dbl>\n1 ...1       2.93  1.50  4.68\n2 ...2       5.92  4.02  8.11\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat4a.brm2 |>\n    add_epred_draws(newdata = data.frame(x = c(2.5, 5))) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n# Groups:   x, .row [2]\n      x  .row variable median lower upper\n  <dbl> <int> <chr>     <dbl> <dbl> <dbl>\n1   2.5     1 .epred     2.93  1.50  4.68\n2   5       2 .epred     5.92  4.02  8.11\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- the fitted mean $y$ associated with an $x$ of 2.5 is\n  2.928\n- the fitted mean $y$ associated with an $x$ of 5 is\n  5.92\n- 95% HPD intervals also given\n\n##### Posterior linear predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4a.brm2 |>\n    posterior_linpred(newdata = data.frame(x = c(2.5, 5))) |>\n    exp() |> \n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable median lower upper\n  <chr>     <dbl> <dbl> <dbl>\n1 ...1       2.93  1.50  4.68\n2 ...2       5.92  4.02  8.11\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat4a.brm2 |>\n    add_linpred_draws(newdata = data.frame(x = c(2.5, 5))) |>\n    mutate(.linpred = exp(.linpred)) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n# Groups:   x, .row [2]\n      x  .row variable median lower upper\n  <dbl> <int> <chr>     <dbl> <dbl> <dbl>\n1   2.5     1 .linpred   2.93  1.50  4.68\n2   5       2 .linpred   5.92  4.02  8.11\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- the fitted mean $y$ associated with an $x$ of 2.5 is\n  2.928\n- the fitted mean $y$ associated with an $x$ of 5 is\n  5.92\n- 95% HPD intervals also given\n\n:::::\n\n#### Centered predictor\n\nIn each case, we will predict $y$ when $x$ is 2.5 and 5\n\nNote, for a Gaussian model `emmeans`, `posterior_epred` and\n`posterior_linpred` will all yield the same outputs.\n`posterior_predict` will yield wider credible intervals as it is\npredicting for individual observations rather than predicting means of\nlarge samples. \n\n::::: {.panel-tabset}\n\n##### Estimated marginal means\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4b.brm2 |> emmeans(~x, at = list(x = c(2.5, 5)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   x emmean lower.HPD upper.HPD\n 2.5   1.07      0.54      1.61\n 5.0   1.78      1.44      2.09\n\nPoint estimate displayed: median \nResults are given on the log (not the response) scale. \nHPD interval probability: 0.95 \n```\n\n\n:::\n\n```{.r .cell-code}\n# OR with more control over the way posteriors are summarised\ndat4b.brm2 |>\n    emmeans(~x, at = list(x = c(2.5, 5))) |>\n    gather_emmeans_draws() |>\n    median_hdci()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 7\n      x .value .lower .upper .width .point .interval\n  <dbl>  <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1   2.5   1.07  0.533   1.60   0.95 median hdci     \n2   5     1.78  1.44    2.09   0.95 median hdci     \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4b.emmeans <- dat4b.brm2 |>\n    emmeans(~x, at = list(x = c(2.5, 5))) |>\n    as.data.frame()\n```\n:::\n\n\n\n**Conclusions**:\n\n- the predicted (estimated) mean $y$ associated with an $x$ of 2.5 is\n  1.068\n- the predicted (estimated) mean $y$ associated with an $x$ of 5 is\n  1.78\n- 95% HPD intervals also given\n\n##### Posterior predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4b.brm2 |>\n    posterior_predict(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable median lower upper\n  <chr>     <dbl> <dbl> <dbl>\n1 ...1          3     0     7\n2 ...2          6     0    11\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat4b.brm2 |>\n    add_predicted_draws(newdata = data.frame(x = c(2.5, 5))) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n# Groups:   x, .row [2]\n      x  .row variable    median lower upper\n  <dbl> <int> <chr>        <dbl> <dbl> <dbl>\n1   2.5     1 .prediction      3     0     7\n2   5       2 .prediction      6     1    12\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4b.pred <- dat4b.brm2 |>\n    posterior_predict(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n:::\n\n\n\n**Conclusions**:\n\n- the predicted (estimated) mean $y$ associated with an $x$ of 2.5 is\n  3\n- the predicted (estimated) mean $y$ associated with an $x$ of 5 is\n  6\n- 95% HPD intervals also given\n\n\n##### Posterior expected values\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4b.brm2 |>\n    posterior_epred(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable median lower upper\n  <chr>     <dbl> <dbl> <dbl>\n1 ...1       2.91  1.50  4.55\n2 ...2       5.93  4.08  7.97\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat4b.brm2 |>\n    add_epred_draws(newdata = data.frame(x = c(2.5, 5))) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n# Groups:   x, .row [2]\n      x  .row variable median lower upper\n  <dbl> <int> <chr>     <dbl> <dbl> <dbl>\n1   2.5     1 .epred     2.91  1.50  4.55\n2   5       2 .epred     5.93  4.08  7.97\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4b.epred <- dat4b.brm2 |>\n    posterior_epred(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n:::\n\n\n\n**Conclusions**:\n\n- the fitted mean $y$ associated with an $x$ of 2.5 is\n  2.911\n- the fitted mean $y$ associated with an $x$ of 5 is\n  5.929\n- 95% HPD intervals also given\n\n##### Posterior linear predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4b.brm2 |>\n    posterior_linpred(newdata = data.frame(x = c(2.5, 5))) |>\n    exp() |> \n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable median lower upper\n  <chr>     <dbl> <dbl> <dbl>\n1 ...1       2.91  1.50  4.55\n2 ...2       5.93  4.08  7.97\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat4b.brm2 |>\n    add_linpred_draws(newdata = data.frame(x = c(2.5, 5))) |>\n    mutate(.linpred = exp(.linpred)) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n# Groups:   x, .row [2]\n      x  .row variable median lower upper\n  <dbl> <int> <chr>     <dbl> <dbl> <dbl>\n1   2.5     1 .linpred   2.91  1.50  4.55\n2   5       2 .linpred   5.93  4.08  7.97\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4b.linpred <- dat4b.brm2 |>\n    posterior_linpred(newdata = data.frame(x = c(2.5, 5))) |>\n    exp() |> \n    summarise_draws(median, HDInterval::hdi)\n```\n:::\n\n\n\n**Conclusions**:\n\n- the fitted mean $y$ associated with an $x$ of 2.5 is\n  2.911\n- the fitted mean $y$ associated with an $x$ of 5 is\n  5.929\n- 95% HPD intervals also given\n\n:::::\n\n#### Standardised predictor\n\nIn each case, we will predict $y$ when $x$ is 2.5 and 5\n\nNote, for a Gaussian model `emmeans`, `posterior_epred` and\n`posterior_linpred` will all yield the same outputs.\n`posterior_predict` will yield wider credible intervals as it is\npredicting for individual observations rather than predicting means of\nlarge samples. \n\n::::: {.panel-tabset}\n\n##### Estimated marginal means\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4c.brm2 |> emmeans(~x, at = list(x = c(2.5, 5)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   x emmean lower.HPD upper.HPD\n 2.5   1.10     0.584      1.61\n 5.0   1.79     1.421      2.09\n\nPoint estimate displayed: median \nResults are given on the log (not the response) scale. \nHPD interval probability: 0.95 \n```\n\n\n:::\n\n```{.r .cell-code}\n# OR with more control over the way posteriors are summarised\ndat4c.brm2 |>\n    emmeans(~x, at = list(x = c(2.5, 5))) |>\n    gather_emmeans_draws() |>\n    median_hdci()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 7\n      x .value .lower .upper .width .point .interval\n  <dbl>  <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1   2.5   1.10  0.587   1.61   0.95 median hdci     \n2   5     1.79  1.43    2.11   0.95 median hdci     \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4c.emmeans <- dat4c.brm2 |>\n    emmeans(~x, at = list(x = c(2.5, 5))) |>\n    as.data.frame()\n```\n:::\n\n\n\n**Conclusions**:\n\n- the predicted (estimated) mean $y$ associated with an $x$ of 2.5 is\n  1.1\n- the predicted (estimated) mean $y$ associated with an $x$ of 5 is\n  1.79\n- 95% HPD intervals also given\n\n##### Posterior predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4c.brm2 |>\n    posterior_predict(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable median lower upper\n  <chr>     <dbl> <dbl> <dbl>\n1 ...1          3     0     7\n2 ...2          6     0    11\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat4c.brm2 |>\n    add_predicted_draws(newdata = data.frame(x = c(2.5, 5))) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n# Groups:   x, .row [2]\n      x  .row variable    median lower upper\n  <dbl> <int> <chr>        <dbl> <dbl> <dbl>\n1   2.5     1 .prediction      3     0     7\n2   5       2 .prediction      6     0    11\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4c.pred <- dat4c.brm2 |>\n    posterior_predict(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n:::\n\n\n\n**Conclusions**:\n\n- the predicted (estimated) mean $y$ associated with an $x$ of 2.5 is\n  3\n- the predicted (estimated) mean $y$ associated with an $x$ of 5 is\n  6\n- 95% HPD intervals also given\n\n\n##### Posterior expected values\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4c.brm2 |>\n    posterior_epred(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable median lower upper\n  <chr>     <dbl> <dbl> <dbl>\n1 ...1       3.00  1.68  4.83\n2 ...2       5.99  4.14  8.11\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat4c.brm2 |>\n    add_epred_draws(newdata = data.frame(x = c(2.5, 5))) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n# Groups:   x, .row [2]\n      x  .row variable median lower upper\n  <dbl> <int> <chr>     <dbl> <dbl> <dbl>\n1   2.5     1 .epred     3.00  1.68  4.83\n2   5       2 .epred     5.99  4.14  8.11\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4c.epred <- dat4c.brm2 |>\n    posterior_epred(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n:::\n\n\n\n**Conclusions**:\n\n- the fitted mean $y$ associated with an $x$ of 2.5 is\n  3.004\n- the fitted mean $y$ associated with an $x$ of 5 is\n  5.989\n- 95% HPD intervals also given\n\n##### Posterior linear predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4c.brm2 |>\n    posterior_linpred(newdata = data.frame(x = c(2.5, 5))) |>\n    exp() |> \n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable median lower upper\n  <chr>     <dbl> <dbl> <dbl>\n1 ...1       3.00  1.68  4.83\n2 ...2       5.99  4.14  8.11\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat4c.brm2 |>\n    add_linpred_draws(newdata = data.frame(x = c(2.5, 5))) |>\n    mutate(.linpred = exp(.linpred)) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n# Groups:   x, .row [2]\n      x  .row variable median lower upper\n  <dbl> <int> <chr>     <dbl> <dbl> <dbl>\n1   2.5     1 .linpred   3.00  1.68  4.83\n2   5       2 .linpred   5.99  4.14  8.11\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat4c.linpred <- dat4c.brm2 |>\n    posterior_linpred(newdata = data.frame(x = c(2.5, 5))) |>\n    exp() |> \n    summarise_draws(median, HDInterval::hdi)\n```\n:::\n\n\n\n**Conclusions**:\n\n- the fitted mean $y$ associated with an $x$ of 2.5 is\n  3.004\n- the fitted mean $y$ associated with an $x$ of 5 is\n  5.989\n- 95% HPD intervals also given\n\n:::::\n:::::::\n\n### Example 5 (Binary data)\n\n::::::: {.panel-tabset}\n\n#### Raw predictor\n\nIn each case, we will predict $y$ when $x$ is 2.5 and 5\n\nNote, for a Gaussian model `emmeans`, `posterior_epred` and\n`posterior_linpred` will all yield the same outputs.\n`posterior_predict` will yield wider credible intervals as it is\npredicting for individual observations rather than predicting means of\nlarge samples. \n\n::::: {.panel-tabset}\n\n##### Estimated marginal means\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5a.brm2 |> emmeans(~x, at = list(x = c(2.5, 5)), type = \"response\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   x   prob lower.HPD upper.HPD\n 2.5 0.0704  2.59e-05     0.387\n 5.0 0.4975  1.78e-01     0.783\n\nPoint estimate displayed: median \nResults are back-transformed from the logit scale \nHPD interval probability: 0.95 \n```\n\n\n:::\n\n```{.r .cell-code}\n# OR with more control over the way posteriors are summarised\ndat5a.brm2 |>\n    emmeans(~x, at = list(x = c(2.5, 5))) |>\n    gather_emmeans_draws() |>\n    mutate(.value = plogis(.value)) |> \n    median_hdci()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 7\n      x .value    .lower .upper .width .point .interval\n  <dbl>  <dbl>     <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1   2.5 0.0704 0.0000258  0.386   0.95 median hdci     \n2   5   0.498  0.179      0.785   0.95 median hdci     \n```\n\n\n:::\n\n```{.r .cell-code}\n# OR with yet more control over the way posteriors are summarised\ndat5a.brm2 |>\n    emmeans(~x, at = list(x = c(2.5, 5))) |>\n    gather_emmeans_draws() |>\n    dplyr::select(-.chain) |> \n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n# Groups:   x [2]\n      x variable   median lower  upper\n  <dbl> <chr>       <dbl> <dbl>  <dbl>\n1   2.5 .value   -2.58    -6.73 0.0680\n2   5   .value   -0.00980 -1.53 1.28  \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- the predicted (estimated) mean $y$ associated with an $x$ of 2.5 is\n  0.07\n- the predicted (estimated) mean $y$ associated with an $x$ of 5 is\n  0.498\n- 95% HPD intervals also given\n\n##### Posterior predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5a.brm2 |>\n    posterior_predict(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable median lower upper\n  <chr>     <dbl> <dbl> <dbl>\n1 ...1          0     0     1\n2 ...2          0     0     1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat5a.brm2 |>\n    add_predicted_draws(newdata = data.frame(x = c(2.5, 5))) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n# Groups:   x, .row [2]\n      x  .row variable    median lower upper\n  <dbl> <int> <chr>        <dbl> <dbl> <dbl>\n1   2.5     1 .prediction      0     0     1\n2   5       2 .prediction      1     0     1\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5a.pred <- dat5a.brm2 |>\n    posterior_predict(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n:::\n\n\n\n**Conclusions**:\n\n- the predicted (estimated) mean $y$ associated with an $x$ of 2.5 is\n  0\n- the predicted (estimated) mean $y$ associated with an $x$ of 5 is\n  1\n- 95% HPD intervals also given\n\n\n##### Posterior expected values\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5a.brm2 |>\n    posterior_epred(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable median     lower upper\n  <chr>     <dbl>     <dbl> <dbl>\n1 ...1     0.0704 0.0000258 0.387\n2 ...2     0.498  0.178     0.783\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat5a.brm2 |>\n    add_epred_draws(newdata = data.frame(x = c(2.5, 5))) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n# Groups:   x, .row [2]\n      x  .row variable median     lower upper\n  <dbl> <int> <chr>     <dbl>     <dbl> <dbl>\n1   2.5     1 .epred   0.0704 0.0000258 0.387\n2   5       2 .epred   0.498  0.178     0.783\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- the fitted mean $y$ associated with an $x$ of 2.5 is\n  0.07\n- the fitted mean $y$ associated with an $x$ of 5 is\n  0.498\n- 95% HPD intervals also given\n\n##### Posterior linear predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5a.brm2 |>\n    posterior_linpred(newdata = data.frame(x = c(2.5, 5))) |>\n    plogis() |> \n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable median     lower upper\n  <chr>     <dbl>     <dbl> <dbl>\n1 ...1     0.0704 0.0000258 0.387\n2 ...2     0.498  0.178     0.783\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat5a.brm2 |>\n    add_linpred_draws(newdata = data.frame(x = c(2.5, 5))) |>\n    mutate(.linpred = plogis(.linpred)) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n# Groups:   x, .row [2]\n      x  .row variable median     lower upper\n  <dbl> <int> <chr>     <dbl>     <dbl> <dbl>\n1   2.5     1 .linpred 0.0704 0.0000258 0.387\n2   5       2 .linpred 0.498  0.178     0.783\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- the fitted mean $y$ associated with an $x$ of 2.5 is\n  0.07\n- the fitted mean $y$ associated with an $x$ of 5 is\n  0.498\n- 95% HPD intervals also given\n\n:::::\n\n#### Centered predictor\n\nIn each case, we will predict $y$ when $x$ is 2.5 and 5\n\nNote, for a Gaussian model `emmeans`, `posterior_epred` and\n`posterior_linpred` will all yield the same outputs.\n`posterior_predict` will yield wider credible intervals as it is\npredicting for individual observations rather than predicting means of\nlarge samples. \n\n::::: {.panel-tabset}\n\n##### Estimated marginal means\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5b.brm2 |> emmeans(~x, at = list(x = c(2.5, 5)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   x  emmean lower.HPD upper.HPD\n 2.5 -2.6167     -6.70     0.244\n 5.0 -0.0311     -1.45     1.501\n\nPoint estimate displayed: median \nResults are given on the logit (not the response) scale. \nHPD interval probability: 0.95 \n```\n\n\n:::\n\n```{.r .cell-code}\n# OR with more control over the way posteriors are summarised\ndat5b.brm2 |>\n    emmeans(~x, at = list(x = c(2.5, 5))) |>\n    gather_emmeans_draws() |>\n    median_hdci()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 7\n      x  .value .lower .upper .width .point .interval\n  <dbl>   <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1   2.5 -2.62    -6.53  0.429   0.95 median hdci     \n2   5   -0.0311  -1.58  1.41    0.95 median hdci     \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5b.emmeans <- dat5b.brm2 |>\n    emmeans(~x, at = list(x = c(2.5, 5))) |>\n    as.data.frame()\n```\n:::\n\n\n\n**Conclusions**:\n\n- the predicted (estimated) mean $y$ associated with an $x$ of 2.5 is\n  -2.617\n- the predicted (estimated) mean $y$ associated with an $x$ of 5 is\n  -0.031\n- 95% HPD intervals also given\n\n##### Posterior predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5b.brm2 |>\n    posterior_predict(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable median lower upper\n  <chr>     <dbl> <dbl> <dbl>\n1 ...1          0     0     1\n2 ...2          0     0     1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat5b.brm2 |>\n    add_predicted_draws(newdata = data.frame(x = c(2.5, 5))) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n# Groups:   x, .row [2]\n      x  .row variable    median lower upper\n  <dbl> <int> <chr>        <dbl> <dbl> <dbl>\n1   2.5     1 .prediction      0     0     1\n2   5       2 .prediction      0     0     1\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5b.pred <- dat5b.brm2 |>\n    posterior_predict(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n:::\n\n\n\n**Conclusions**:\n\n- the predicted (estimated) mean $y$ associated with an $x$ of 2.5 is\n  0\n- the predicted (estimated) mean $y$ associated with an $x$ of 5 is\n  0\n- 95% HPD intervals also given\n\n\n##### Posterior expected values\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5b.brm2 |>\n    posterior_epred(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable median      lower upper\n  <chr>     <dbl>      <dbl> <dbl>\n1 ...1     0.0681 0.00000402 0.396\n2 ...2     0.492  0.189      0.818\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat5b.brm2 |>\n    add_epred_draws(newdata = data.frame(x = c(2.5, 5))) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n# Groups:   x, .row [2]\n      x  .row variable median      lower upper\n  <dbl> <int> <chr>     <dbl>      <dbl> <dbl>\n1   2.5     1 .epred   0.0681 0.00000402 0.396\n2   5       2 .epred   0.492  0.189      0.818\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5b.epred <- dat5b.brm2 |>\n    posterior_epred(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n:::\n\n\n\n**Conclusions**:\n\n- the fitted mean $y$ associated with an $x$ of 2.5 is\n  0.068\n- the fitted mean $y$ associated with an $x$ of 5 is\n  0.492\n- 95% HPD intervals also given\n\n##### Posterior linear predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5b.brm2 |>\n    posterior_linpred(newdata = data.frame(x = c(2.5, 5))) |>\n    plogis() |> \n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable median      lower upper\n  <chr>     <dbl>      <dbl> <dbl>\n1 ...1     0.0681 0.00000402 0.396\n2 ...2     0.492  0.189      0.818\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat5b.brm2 |>\n    add_linpred_draws(newdata = data.frame(x = c(2.5, 5))) |>\n    mutate(.linpred = plogis(.linpred)) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n# Groups:   x, .row [2]\n      x  .row variable median      lower upper\n  <dbl> <int> <chr>     <dbl>      <dbl> <dbl>\n1   2.5     1 .linpred 0.0681 0.00000402 0.396\n2   5       2 .linpred 0.492  0.189      0.818\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5b.linpred <- dat5b.brm2 |>\n    posterior_linpred(newdata = data.frame(x = c(2.5, 5))) |>\n    plogis() |> \n    summarise_draws(median, HDInterval::hdi)\n```\n:::\n\n\n\n**Conclusions**:\n\n- the fitted mean $y$ associated with an $x$ of 2.5 is\n  0.068\n- the fitted mean $y$ associated with an $x$ of 5 is\n  0.492\n- 95% HPD intervals also given\n\n:::::\n\n#### Standardised predictor\n\nIn each case, we will predict $y$ when $x$ is 2.5 and 5\n\nNote, for a Gaussian model `emmeans`, `posterior_epred` and\n`posterior_linpred` will all yield the same outputs.\n`posterior_predict` will yield wider credible intervals as it is\npredicting for individual observations rather than predicting means of\nlarge samples. \n\n::::: {.panel-tabset}\n\n##### Estimated marginal means\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5c.brm2 |> emmeans(~x, at = list(x = c(2.5, 5)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   x  emmean lower.HPD upper.HPD\n 2.5 -1.4481     -4.37     0.737\n 5.0  0.0908     -1.29     1.378\n\nPoint estimate displayed: median \nResults are given on the logit (not the response) scale. \nHPD interval probability: 0.95 \n```\n\n\n:::\n\n```{.r .cell-code}\n# OR with more control over the way posteriors are summarised\ndat5c.brm2 |>\n    emmeans(~x, at = list(x = c(2.5, 5))) |>\n    gather_emmeans_draws() |>\n    median_hdci()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 7\n      x  .value .lower .upper .width .point .interval\n  <dbl>   <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1   2.5 -1.45    -4.37  0.737   0.95 median hdci     \n2   5    0.0908  -1.29  1.38    0.95 median hdci     \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5c.emmeans <- dat5c.brm2 |>\n    emmeans(~x, at = list(x = c(2.5, 5))) |>\n    as.data.frame()\n```\n:::\n\n\n\n**Conclusions**:\n\n- the predicted (estimated) mean $y$ associated with an $x$ of 2.5 is\n  -1.448\n- the predicted (estimated) mean $y$ associated with an $x$ of 5 is\n  0.091\n- 95% HPD intervals also given\n\n##### Posterior predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5c.brm2 |>\n    posterior_predict(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable median lower upper\n  <chr>     <dbl> <dbl> <dbl>\n1 ...1          0     0     1\n2 ...2          1     0     1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat5c.brm2 |>\n    add_predicted_draws(newdata = data.frame(x = c(2.5, 5))) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n# Groups:   x, .row [2]\n      x  .row variable    median lower upper\n  <dbl> <int> <chr>        <dbl> <dbl> <dbl>\n1   2.5     1 .prediction      0     0     1\n2   5       2 .prediction      1     0     1\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5c.pred <- dat5c.brm2 |>\n    posterior_predict(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n:::\n\n\n\n**Conclusions**:\n\n- the predicted (estimated) mean $y$ associated with an $x$ of 2.5 is\n  0\n- the predicted (estimated) mean $y$ associated with an $x$ of 5 is\n  1\n- 95% HPD intervals also given\n\n\n##### Posterior expected values\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5c.brm2 |>\n    posterior_epred(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable median    lower upper\n  <chr>     <dbl>    <dbl> <dbl>\n1 ...1      0.190 0.000832 0.551\n2 ...2      0.523 0.232    0.815\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat5c.brm2 |>\n    add_epred_draws(newdata = data.frame(x = c(2.5, 5))) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n# Groups:   x, .row [2]\n      x  .row variable median    lower upper\n  <dbl> <int> <chr>     <dbl>    <dbl> <dbl>\n1   2.5     1 .epred    0.190 0.000832 0.551\n2   5       2 .epred    0.523 0.232    0.815\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5c.epred <- dat5c.brm2 |>\n    posterior_epred(newdata = data.frame(x = c(2.5, 5))) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n:::\n\n\n\n**Conclusions**:\n\n- the fitted mean $y$ associated with an $x$ of 2.5 is\n  0.19\n- the fitted mean $y$ associated with an $x$ of 5 is\n  0.523\n- 95% HPD intervals also given\n\n##### Posterior linear predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5c.brm2 |>\n    posterior_linpred(newdata = data.frame(x = c(2.5, 5))) |>\n    plogis() |> \n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable median    lower upper\n  <chr>     <dbl>    <dbl> <dbl>\n1 ...1      0.190 0.000832 0.551\n2 ...2      0.523 0.232    0.815\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat5c.brm2 |>\n    add_linpred_draws(newdata = data.frame(x = c(2.5, 5))) |>\n    mutate(.linpred = plogis(.linpred)) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n# Groups:   x, .row [2]\n      x  .row variable median    lower upper\n  <dbl> <int> <chr>     <dbl>    <dbl> <dbl>\n1   2.5     1 .linpred  0.190 0.000832 0.551\n2   5       2 .linpred  0.523 0.232    0.815\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat5c.linpred <- dat5c.brm2 |>\n    posterior_linpred(newdata = data.frame(x = c(2.5, 5))) |>\n    plogis() |> \n    summarise_draws(median, HDInterval::hdi)\n```\n:::\n\n\n\n**Conclusions**:\n\n- the fitted mean $y$ associated with an $x$ of 2.5 is\n  0.19\n- the fitted mean $y$ associated with an $x$ of 5 is\n  0.523\n- 95% HPD intervals also given\n\n:::::\n:::::::\n\n### Example 6 (Binomial data)\n\n::::::: {.panel-tabset}\n\n#### Raw predictor\n\nIn each case, we will predict $y$ when $x$ is 2.5 and 5\n\nNote, for a Gaussian model `emmeans`, `posterior_epred` and\n`posterior_linpred` will all yield the same outputs.\n`posterior_predict` will yield wider credible intervals as it is\npredicting for individual observations rather than predicting means of\nlarge samples. \n\n::::: {.panel-tabset}\n\n##### Estimated marginal means\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6a.brm2 |> emmeans(~x, at = list(x = c(2.5, 5)), type = \"response\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   x  prob lower.HPD upper.HPD\n 2.5 0.167    0.0482     0.326\n 5.0 0.499    0.3747     0.613\n\nPoint estimate displayed: median \nResults are back-transformed from the logit scale \nHPD interval probability: 0.95 \n```\n\n\n:::\n\n```{.r .cell-code}\n# OR with more control over the way posteriors are summarised\ndat6a.brm2 |>\n    emmeans(~x, at = list(x = c(2.5, 5))) |>\n    gather_emmeans_draws() |>\n    mutate(.value = plogis(.value)) |> \n    median_hdci()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 7\n      x .value .lower .upper .width .point .interval\n  <dbl>  <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1   2.5  0.167 0.0482  0.326   0.95 median hdci     \n2   5    0.499 0.375   0.613   0.95 median hdci     \n```\n\n\n:::\n\n```{.r .cell-code}\n# OR with yet more control over the way posteriors are summarised\ndat6a.brm2 |>\n    emmeans(~x, at = list(x = c(2.5, 5))) |>\n    gather_emmeans_draws() |>\n    dplyr::select(-.chain) |> \n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n# Groups:   x [2]\n      x variable   median  lower  upper\n  <dbl> <chr>       <dbl>  <dbl>  <dbl>\n1   2.5 .value   -1.61    -2.76  -0.659\n2   5   .value   -0.00525 -0.512  0.459\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- the predicted (estimated) mean $y$ associated with an $x$ of 2.5 is\n  0.167\n- the predicted (estimated) mean $y$ associated with an $x$ of 5 is\n  0.499\n- 95% HPD intervals also given\n\n##### Posterior predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6a.brm2 |>\n    posterior_predict(newdata = data.frame(x = c(2.5, 5), total = 1)) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable median lower upper\n  <chr>     <dbl> <dbl> <dbl>\n1 ...1          0     0     1\n2 ...2          0     0     1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat6a.brm2 |>\n    add_predicted_draws(newdata = data.frame(x = c(2.5, 5), total = 1)) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 7\n# Groups:   x, total, .row [2]\n      x total  .row variable    median lower upper\n  <dbl> <dbl> <int> <chr>        <dbl> <dbl> <dbl>\n1   2.5     1     1 .prediction      0     0     1\n2   5       1     2 .prediction      0     0     1\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6a.pred <- dat6a.brm2 |>\n    posterior_predict(newdata = data.frame(x = c(2.5, 5), total = 1)) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n:::\n\n\n\n**Conclusions**:\n\n- the predicted (estimated) mean $y$ associated with an $x$ of 2.5 is\n  0\n- the predicted (estimated) mean $y$ associated with an $x$ of 5 is\n  0\n- 95% HPD intervals also given\n\n\n##### Posterior expected values\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6a.brm2 |>\n    posterior_epred(newdata = data.frame(x = c(2.5, 5), total = 1)) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable median  lower upper\n  <chr>     <dbl>  <dbl> <dbl>\n1 ...1      0.167 0.0482 0.326\n2 ...2      0.499 0.375  0.613\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat6a.brm2 |>\n    add_epred_draws(newdata = data.frame(x = c(2.5, 5), total = 1)) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 7\n# Groups:   x, total, .row [2]\n      x total  .row variable median  lower upper\n  <dbl> <dbl> <int> <chr>     <dbl>  <dbl> <dbl>\n1   2.5     1     1 .epred    0.167 0.0482 0.326\n2   5       1     2 .epred    0.499 0.375  0.613\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- the fitted mean $y$ associated with an $x$ of 2.5 is\n  0.167\n- the fitted mean $y$ associated with an $x$ of 5 is\n  0.499\n- 95% HPD intervals also given\n\n##### Posterior linear predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6a.brm2 |>\n    posterior_linpred(newdata = data.frame(x = c(2.5, 5), total = 1)) |>\n    plogis() |> \n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable median  lower upper\n  <chr>     <dbl>  <dbl> <dbl>\n1 ...1      0.167 0.0482 0.326\n2 ...2      0.499 0.375  0.613\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat6a.brm2 |>\n    add_linpred_draws(newdata = data.frame(x = c(2.5, 5), total = 1)) |>\n    mutate(.linpred = plogis(.linpred)) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 7\n# Groups:   x, total, .row [2]\n      x total  .row variable median  lower upper\n  <dbl> <dbl> <int> <chr>     <dbl>  <dbl> <dbl>\n1   2.5     1     1 .linpred  0.167 0.0482 0.326\n2   5       1     2 .linpred  0.499 0.375  0.613\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- the fitted mean $y$ associated with an $x$ of 2.5 is\n  0.167\n- the fitted mean $y$ associated with an $x$ of 5 is\n  0.499\n- 95% HPD intervals also given\n\n:::::\n\n#### Centered predictor\n\nIn each case, we will predict $y$ when $x$ is 2.5 and 5\n\nNote, for a Gaussian model `emmeans`, `posterior_epred` and\n`posterior_linpred` will all yield the same outputs.\n`posterior_predict` will yield wider credible intervals as it is\npredicting for individual observations rather than predicting means of\nlarge samples. \n\n::::: {.panel-tabset}\n\n##### Estimated marginal means\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6b.brm2 |> emmeans(~x, at = list(x = c(2.5, 5)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   x  emmean lower.HPD upper.HPD\n 2.5 -1.6495    -2.746    -0.650\n 5.0 -0.0187    -0.521     0.481\n\nPoint estimate displayed: median \nResults are given on the logit (not the response) scale. \nHPD interval probability: 0.95 \n```\n\n\n:::\n\n```{.r .cell-code}\n# OR with more control over the way posteriors are summarised\ndat6b.brm2 |>\n    emmeans(~x, at = list(x = c(2.5, 5))) |>\n    gather_emmeans_draws() |>\n    median_hdci()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 7\n      x  .value .lower .upper .width .point .interval\n  <dbl>   <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1   2.5 -1.65   -2.75  -0.650   0.95 median hdci     \n2   5   -0.0187 -0.555  0.452   0.95 median hdci     \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6b.emmeans <- dat6b.brm2 |>\n    emmeans(~x, at = list(x = c(2.5, 5))) |>\n    as.data.frame()\n```\n:::\n\n\n\n**Conclusions**:\n\n- the predicted (estimated) mean $y$ associated with an $x$ of 2.5 is\n  -1.65\n- the predicted (estimated) mean $y$ associated with an $x$ of 5 is\n  -0.019\n- 95% HPD intervals also given\n\n##### Posterior predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6b.brm2 |>\n    posterior_predict(newdata = data.frame(x = c(2.5, 5), total = 1)) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable median lower upper\n  <chr>     <dbl> <dbl> <dbl>\n1 ...1          0     0     1\n2 ...2          1     0     1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat6b.brm2 |>\n    add_predicted_draws(newdata = data.frame(x = c(2.5, 5), total = 1)) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 7\n# Groups:   x, total, .row [2]\n      x total  .row variable    median lower upper\n  <dbl> <dbl> <int> <chr>        <dbl> <dbl> <dbl>\n1   2.5     1     1 .prediction      0     0     1\n2   5       1     2 .prediction      0     0     1\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6b.pred <- dat6b.brm2 |>\n    posterior_predict(newdata = data.frame(x = c(2.5, 5), total = 1)) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n:::\n\n\n\n**Conclusions**:\n\n- the predicted (estimated) mean $y$ associated with an $x$ of 2.5 is\n  0\n- the predicted (estimated) mean $y$ associated with an $x$ of 5 is\n  1\n- 95% HPD intervals also given\n\n\n##### Posterior expected values\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6b.brm2 |>\n    posterior_epred(newdata = data.frame(x = c(2.5, 5), total = 1)) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable median  lower upper\n  <chr>     <dbl>  <dbl> <dbl>\n1 ...1      0.161 0.0480 0.315\n2 ...2      0.495 0.373  0.618\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat6b.brm2 |>\n    add_epred_draws(newdata = data.frame(x = c(2.5, 5), total = 1)) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 7\n# Groups:   x, total, .row [2]\n      x total  .row variable median  lower upper\n  <dbl> <dbl> <int> <chr>     <dbl>  <dbl> <dbl>\n1   2.5     1     1 .epred    0.161 0.0480 0.315\n2   5       1     2 .epred    0.495 0.373  0.618\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6b.epred <- dat6b.brm2 |>\n    posterior_epred(newdata = data.frame(x = c(2.5, 5), total = 1)) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n:::\n\n\n\n**Conclusions**:\n\n- the fitted mean $y$ associated with an $x$ of 2.5 is\n  0.161\n- the fitted mean $y$ associated with an $x$ of 5 is\n  0.495\n- 95% HPD intervals also given\n\n##### Posterior linear predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6b.brm2 |>\n    posterior_linpred(newdata = data.frame(x = c(2.5, 5), total = 1)) |>\n    plogis() |> \n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable median  lower upper\n  <chr>     <dbl>  <dbl> <dbl>\n1 ...1      0.161 0.0480 0.315\n2 ...2      0.495 0.373  0.618\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat6b.brm2 |>\n    add_linpred_draws(newdata = data.frame(x = c(2.5, 5), total = 1)) |>\n    mutate(.linpred = plogis(.linpred)) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 7\n# Groups:   x, total, .row [2]\n      x total  .row variable median  lower upper\n  <dbl> <dbl> <int> <chr>     <dbl>  <dbl> <dbl>\n1   2.5     1     1 .linpred  0.161 0.0480 0.315\n2   5       1     2 .linpred  0.495 0.373  0.618\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6b.linpred <- dat6b.brm2 |>\n    posterior_linpred(newdata = data.frame(x = c(2.5, 5), total = 1)) |>\n    plogis() |> \n    summarise_draws(median, HDInterval::hdi)\n```\n:::\n\n\n\n**Conclusions**:\n\n- the fitted mean $y$ associated with an $x$ of 2.5 is\n  0.161\n- the fitted mean $y$ associated with an $x$ of 5 is\n  0.495\n- 95% HPD intervals also given\n\n:::::\n\n#### Standardised predictor\n\nIn each case, we will predict $y$ when $x$ is 2.5 and 5\n\nNote, for a Gaussian model `emmeans`, `posterior_epred` and\n`posterior_linpred` will all yield the same outputs.\n`posterior_predict` will yield wider credible intervals as it is\npredicting for individual observations rather than predicting means of\nlarge samples. \n\n::::: {.panel-tabset}\n\n##### Estimated marginal means\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6c.brm2 |> emmeans(~x, at = list(x = c(2.5, 5)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   x  emmean lower.HPD upper.HPD\n 2.5 -1.3104    -2.459    -0.322\n 5.0  0.0105    -0.485     0.502\n\nPoint estimate displayed: median \nResults are given on the logit (not the response) scale. \nHPD interval probability: 0.95 \n```\n\n\n:::\n\n```{.r .cell-code}\n# OR with more control over the way posteriors are summarised\ndat6c.brm2 |>\n    emmeans(~x, at = list(x = c(2.5, 5))) |>\n    gather_emmeans_draws() |>\n    median_hdci()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 7\n      x  .value .lower .upper .width .point .interval\n  <dbl>   <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1   2.5 -1.31   -2.43  -0.279   0.95 median hdci     \n2   5    0.0105 -0.485  0.502   0.95 median hdci     \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6c.emmeans <- dat6c.brm2 |>\n    emmeans(~x, at = list(x = c(2.5, 5))) |>\n    as.data.frame()\n```\n:::\n\n\n\n**Conclusions**:\n\n- the predicted (estimated) mean $y$ associated with an $x$ of 2.5 is\n  -1.31\n- the predicted (estimated) mean $y$ associated with an $x$ of 5 is\n  0.011\n- 95% HPD intervals also given\n\n##### Posterior predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6c.brm2 |>\n    posterior_predict(newdata = data.frame(x = c(2.5, 5), total = 1)) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable median lower upper\n  <chr>     <dbl> <dbl> <dbl>\n1 ...1          0     0     1\n2 ...2          1     0     1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat6c.brm2 |>\n    add_predicted_draws(newdata = data.frame(x = c(2.5, 5), total = 1)) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 7\n# Groups:   x, total, .row [2]\n      x total  .row variable    median lower upper\n  <dbl> <dbl> <int> <chr>        <dbl> <dbl> <dbl>\n1   2.5     1     1 .prediction      0     0     1\n2   5       1     2 .prediction      1     0     1\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6c.pred <- dat6c.brm2 |>\n    posterior_predict(newdata = data.frame(x = c(2.5, 5), total = 1)) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n:::\n\n\n\n**Conclusions**:\n\n- the predicted (estimated) mean $y$ associated with an $x$ of 2.5 is\n  0\n- the predicted (estimated) mean $y$ associated with an $x$ of 5 is\n  1\n- 95% HPD intervals also given\n\n\n##### Posterior expected values\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6c.brm2 |>\n    posterior_epred(newdata = data.frame(x = c(2.5, 5), total = 1)) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable median  lower upper\n  <chr>     <dbl>  <dbl> <dbl>\n1 ...1      0.212 0.0560 0.388\n2 ...2      0.503 0.381  0.623\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat6c.brm2 |>\n    add_epred_draws(newdata = data.frame(x = c(2.5, 5), total = 1)) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 7\n# Groups:   x, total, .row [2]\n      x total  .row variable median  lower upper\n  <dbl> <dbl> <int> <chr>     <dbl>  <dbl> <dbl>\n1   2.5     1     1 .epred    0.212 0.0560 0.388\n2   5       1     2 .epred    0.503 0.381  0.623\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6c.epred <- dat6c.brm2 |>\n    posterior_epred(newdata = data.frame(x = c(2.5, 5), total = 1)) |>\n    summarise_draws(median, HDInterval::hdi)\n```\n:::\n\n\n\n**Conclusions**:\n\n- the fitted mean $y$ associated with an $x$ of 2.5 is\n  0.212\n- the fitted mean $y$ associated with an $x$ of 5 is\n  0.503\n- 95% HPD intervals also given\n\n##### Posterior linear predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6c.brm2 |>\n    posterior_linpred(newdata = data.frame(x = c(2.5, 5), total = 1)) |>\n    plogis() |> \n    summarise_draws(median, HDInterval::hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  variable median  lower upper\n  <chr>     <dbl>  <dbl> <dbl>\n1 ...1      0.212 0.0560 0.388\n2 ...2      0.503 0.381  0.623\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or for even more control and ability to add other summaries\ndat6c.brm2 |>\n    add_linpred_draws(newdata = data.frame(x = c(2.5, 5), total = 1)) |>\n    mutate(.linpred = plogis(.linpred)) |>\n    dplyr::select(-.chain) |> # need to exclude this field as it interfers with as_draws_df()\n    summarise_draws(\n      median,\n      HDInterval::hdi\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 7\n# Groups:   x, total, .row [2]\n      x total  .row variable median  lower upper\n  <dbl> <dbl> <int> <chr>     <dbl>  <dbl> <dbl>\n1   2.5     1     1 .linpred  0.212 0.0560 0.388\n2   5       1     2 .linpred  0.503 0.381  0.623\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat6c.linpred <- dat6c.brm2 |>\n    posterior_linpred(newdata = data.frame(x = c(2.5, 5), total = 1)) |>\n    plogis() |> \n    summarise_draws(median, HDInterval::hdi)\n```\n:::\n\n\n\n**Conclusions**:\n\n- the fitted mean $y$ associated with an $x$ of 2.5 is\n  0.212\n- the fitted mean $y$ associated with an $x$ of 5 is\n  0.503\n- 95% HPD intervals also given\n\n:::::\n:::::::\n::::\n\n# Further investigations\n\nSince we have the entire posterior, we are able to make probability\nstatements. We simply count up the number of MCMC sample draws that\nsatisfy a condition (e.g represent a slope greater than 0) and then\ndivide by the total number of MCMC samples.\n\n:::: {.panel-tabset}\n\n### Example 1 (Gaussian data)\n\n::::::: {.panel-tabset}\n#### Raw predictor\n::::: {.panel-tabset}\n\n##### Specific hypotheses\n\nNow that we have the full posteriors, we are free to use these to\ngarner evidence on a range of hypotheses. To demonstrate, we will\nconsider the following hypotheses:\n\n1. a change in $x$ is associated with an increase in $y$\n2. a doubling of $x$ (from 2.5 to 5) is associated with an increase in $y$ of > 50%\n\n::: {.panel-tabset .nav-pills}\n##### 1. Positive relationship\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1a.brm2 |> hypothesis(\"x > 0\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1    (x) > 0    -0.08      0.46    -0.84     0.66       0.74      0.42     \n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1a_hyp <- dat1a.brm2 |>\n    hypothesis(\"x > 0\") |>\n    _[[\"hypothesis\"]]\n```\n:::\n\n\n\n**Conclusions**:\n\n- the parameter (`b_x`) minus 0 is -0.08\n- `Evid.Ratio`: the ratio of evidence for the hypothesis vs the\n  evidence against it.  In this case, the evidence ratio is \n  0.735 - `Inf` is because the divisor was 0 \n  (no evidence against the hypothesis).\n- `Post.Prob`: the probability of the hypothesis is 0.424\n- there is very high evidence for this hypothesis\n\nAlternatively, we could use `gather_draws` to achieve a similar\noutcome.\n\nIn the following, in addition to median and HPD intervals, we will\ncalculate the probability that the slope (`b_x`) is greater than 0. To\ncalculate such a probability, we could simply count up the number of\nposterior `b_x` values that are greater than zero and then divide by\nthe total number of posterior `b_x` values. In R, we could do this as\n`sum(b_x > 0)/length(b_x)` (where `b_x > 0` will return either a 1 for\neach case it is true and a 0 when it is false, and thus summing is\nlike counting). Dividing a sum by its length equates to a mean and\nthus we can achieve the probability by calcualing the mean of `b_x >\n0`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1a.brm2 |>\n  gather_draws(b_x) |>\n  summarise_draws(median,\n    HDInterval::hdi,\n    P = ~mean(. > 0)\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 6\n# Groups:   .variable [1]\n  .variable variable  median  lower upper     P\n  <chr>     <chr>      <dbl>  <dbl> <dbl> <dbl>\n1 b_x       .value   -0.0803 -0.973 0.806 0.424\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1a_hyp2 <- dat1a.brm2 |>\n    gather_draws(b_x) |>\n    summarise_draws(median,\n        HDInterval::hdi,\n        P = ~mean(. > 0)\n    )\n```\n:::\n\n\n::: {.callout-tip}\n\nThe `summarise_draws()` function expects a set of one or more summary\nor diagnostic functions (such as `median` etc). These can be supplied\neither as the name of the function (as in the case for `median` in the\nexample above) or if more arguments or information is required by the\nfunction, the function can be written out in full. in this case, the\nfunction must be proceeded with a `~` and the variable is denoted a\n`.` (such as in `P = ~mean(. > 0)` above).\n\n:::\n\n**Conclusions**:\n\n- the parameter (`b_x`) minus 0 is -0.08\n- `P`: the probability of the hypothesis is 0.735\n- there is very high evidence for this hypothesis\n\n\n\n##### 2. >50% increase when $x$ doubles\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1a.brm2 |>\n    emmeans::emmeans(~x, at = list(x = c(2.5, 5))) |>\n    gather_emmeans_draws() |>\n    ungroup() |>\n    group_by(.draw) |>\n  summarise(ES = 100 * diff(.value) / .value[1]) |>\n  hypothesis(\"ES > 50\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nHypothesis Tests for class :\n     Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1 (ES)-(50) > 0    22.54   2159.24  -178.62   257.92       2.94      0.75     \n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- the difference between the percentage change in estimated $y$ as $x$\n  increases from 2.5 to 5 and 50% is 22.54\n- the evidence ratio in support of the hypothesis that the percentage\n  change exceeds 50% is 2.941\n- the probability that the change in $y$ exceeds 50% is \n  0.746\n- the evidence for such a change is very weak\n  \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1a.brm2 |>\n  emmeans::emmeans(~x, at = list(x = c(2.5, 5))) |>\n  gather_emmeans_draws() |>\n  ungroup() |>\n  group_by(.draw) |>\n  summarise(ES = 100 * diff(.value) / .value[1]) |>\n  summarise_draws(\n    mean, median,\n    HDInterval::hdi,\n    P = ~ mean(. > 50)\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 6\n  variable  mean median lower upper     P\n  <chr>    <dbl>  <dbl> <dbl> <dbl> <dbl>\n1 ES        72.5   84.5 -352.  502. 0.746\n```\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: `as_data_frame()` was deprecated in tibble 2.0.0.\nℹ Please use `as_tibble()` (with slightly different semantics) to convert to a\n  tibble, or `as.data.frame()` to convert to a data frame.\n```\n\n\n:::\n:::\n\n\n\n**Conclusions**:\n\n- the percentage change in estimated $y$ as $x$\n  increases from 2.5 to 5 is 72.54\n- the probability that the change in $y$ exceeds 50% is \n  0.746\n- the evidence for such a change is very weak\n\n\n:::\n\n\n##### ROPE\n\nThe procedure highlighted above for calculating excedence\nprobabilities evaluates the degree of evidence for a effect in a\nparticular direction. However, there are other instances where there\nis a desire to evaluate the evidence that something has change (either\nincreased or decreased). Such purposes are similar to the Frequentist\npursuit of testing a null hypothesis (e.g. effect = 0).\n\nThe Region of Practical Equivalence (ROPE) evaluates evidence that an\neffect is \"practically equivalent\" to a value (e.g. 0) by calculating\nthe proportion of effects that are within a nominated range.\n@Kruschke-2018-270 argued that for standardized parameters, the range\nof -0.1 to 0.1 would envelop a negligible effect based on @Cohen-1988.\n@Kruschke-2018-270 also suggested that this range could be extended to\nnon-standardized parameters by multiplying by the standard deviation\nof the response. Accordingly, calculating the proportion of posterior\ndensity within this ROPE could act as a form of \"null-hypothesis\"\ntesting in a Bayesian framework.\n\n- if the HDI of the focal parameter falls completely **outside** the\n  ROPE, there is strong evidence that there **is** an effect\n- if the HDI of the focal parameter falls completely **inside** the\n  ROPE, there is strong evidence that there **is not** an effect\n- otherwise there is not clear evidence either way\n\n::: {.callout-note}\n\nROPE and equivalence tests are of most use when you decide that there\nis not enough evidence to support an hypothesis that there is an\neffect. Such a \"non-significant\" result may be because there genuinely\nis not effect OR you do not have enough power to detect the effect.\nPerforming an equivalence test provides a mechanism to tease these two\nappart.\n\nI provide the following example purely to illustrate how such a test\nwould be performed. In this case, as we have already demonstrated\nstrong evidence for an effect, the equivalence test does not yield any\nadditional insights.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Calculate ROPE range manually\nROPE <- c(-0.1, 0.1) * with(dat, sd(y))\n## OR calculate ROPE range via rope_range function\nROPE <- bayestestR::rope_range(dat1a.brm2)\n\ndat1a.brm2 |> bayestestR::equivalence_test(parameters = \"b_x\", range = ROPE, ci_method = \"HDI\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Test for Practical Equivalence\n\n  ROPE: [-0.09 0.09]\n\nParameter |        H0 | inside ROPE |      95% HDI\n--------------------------------------------------\nx         | Undecided |     15.88 % | [-0.97 0.82]\n```\n\n\n:::\n\n```{.r .cell-code}\ndat1a.brm2 |>\n    bayestestR::equivalence_test(parameters = \"b_x\", range = ROPE, ci_method = \"HDI\") |>\n    plot()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nPicking joint bandwidth of 0.0734\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/rope1a_1-1.png){width=480}\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- the percentage of the HPD for the slope that is inside the ROPE is\n  0.159\n- there is strong evidence for an effect\n\nOR using the `rope` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Calculate ROPE range manually\ndat1a.brm2 |> bayestestR::rope(parameters = \"x\", range = ROPE, ci_method = \"HDI\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Proportion of samples inside the ROPE [-0.09, 0.09]:\n\nParameter | inside ROPE\n-----------------------\nx         |     15.87 %\n```\n\n\n:::\n\n```{.r .cell-code}\ndat1a.brm2 |>\n    bayestestR::rope(parameters = \"x\", range = ROPE, ci_method = \"HDI\") |>\n    plot()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/rope1a_2-1.png){width=480}\n:::\n:::\n\n\n\nThe above demonstration, was applied to the simple comparison that the\nslope was not equal to 0, however, it can similarly be applied to any\nhypothesis (although typically only if there is no evidence of an\neffect)\n\n:::::\n\n#### Centered predictor\n::::: {.panel-tabset}\n\n##### Specific hypotheses\n\nNow that we have the full posteriors, we are free to use these to\ngarner evidence on a range of hypotheses. To demonstrate, we will\nconsider the following hypotheses:\n\n1. a change in $x$ is associated with an increase in $y$\n2. a doubling of $x$ (from 2.5 to 5) is associated with an increase in $y$ of > 50%\n\n::: {.panel-tabset .nav-pills}\n###### 1. Positive relationship\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1b.brm2 |> get_variables()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"          \"b_scalexscaleEQFALSE\" \"sigma\"               \n [4] \"Intercept\"            \"prior_Intercept\"      \"prior_b\"             \n [7] \"prior_sigma\"          \"lprior\"               \"lp__\"                \n[10] \"accept_stat__\"        \"stepsize__\"           \"treedepth__\"         \n[13] \"n_leapfrog__\"         \"divergent__\"          \"energy__\"            \n```\n\n\n:::\n\n```{.r .cell-code}\ndat1b.brm2 |> hypothesis(\"scalexscaleEQFALSE > 0\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nHypothesis Tests for class b:\n                Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio\n1 (scalexscaleEQFALSE) > 0    -0.09      0.46    -0.81     0.65       0.68\n  Post.Prob Star\n1      0.41     \n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1b_hyp <- dat1b.brm2 |>\n    hypothesis(\"scalexscaleEQFALSE > 0\") |>\n    _[[\"hypothesis\"]]\n```\n:::\n\n\n\n**Conclusions**:\n\n- the parameter (`b_scalexscaledEQFALSE`) minus 0 is -0.089\n- `Evid.Ratio`: the ratio of evidence for the hypothesis vs the\n  evidence against it.  In this case, the evidence ratio is \n  0.683 - `Inf` is because the divisor was 0 \n  (no evidence against the hypothesis).\n- `Post.Prob`: the probability of the hypothesis is 0.406\n- there is very high evidence for this hypothesis\n\nAlternatively, we could use `gather_draws` to achieve a similar\noutcome.\n\nIn the following, in addition to median and HPD intervals, we will\ncalculate the probability that the slope (`b_x`) is greater than 0. To\ncalculate such a probability, we could simply count up the number of\nposterior `b_x` values that are greater than zero and then divide by\nthe total number of posterior `b_x` values. In R, we could do this as\n`sum(b_x > 0)/length(b_x)` (where `b_x > 0` will return either a 1 for\neach case it is true and a 0 when it is false, and thus summing is\nlike counting). Dividing a sum by its length equates to a mean and\nthus we can achieve the probability by calcualing the mean of `b_x >\n0`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1b.brm2 |>\n  gather_draws(`b_.*x.*`, regex = TRUE) |>\n  summarise_draws(median,\n    HDInterval::hdi,\n    P = ~mean(. > 0)\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 6\n# Groups:   .variable [1]\n  .variable            variable  median  lower upper     P\n  <chr>                <chr>      <dbl>  <dbl> <dbl> <dbl>\n1 b_scalexscaleEQFALSE .value   -0.0841 -0.997 0.803 0.406\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1b_hyp2 <- dat1b.brm2 |>\n    gather_draws(`b_.*x.*`, regex = TRUE) |>\n    summarise_draws(median,\n        HDInterval::hdi,\n        P = ~mean(. > 0)\n    )\n```\n:::\n\n\n::: {.callout-tip}\n\nThe `summarise_draws()` function expects a set of one or more summary\nor diagnostic functions (such as `median` etc). These can be supplied\neither as the name of the function (as in the case for `median` in the\nexample above) or if more arguments or information is required by the\nfunction, the function can be written out in full. in this case, the\nfunction must be proceeded with a `~` and the variable is denoted a\n`.` (such as in `P = ~mean(. > 0)` above).\n\n:::\n\n**Conclusions**:\n\n- the parameter (`b_scalexscaleEQFALSE`) minus 0 is -0.084\n- `P`: the probability of the hypothesis is 0.683\n- there is very high evidence for this hypothesis\n\n\n\n###### 2. >50% increase when $x$ doubles\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1b.brm2 |>\n    emmeans::emmeans(~x, at = list(x = c(2.5, 5))) |>\n    gather_emmeans_draws() |>\n    ungroup() |>\n    group_by(.draw) |>\n  summarise(ES = 100 * diff(.value) / .value[1]) |>\n  hypothesis(\"ES > 50\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nHypothesis Tests for class :\n     Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1 (ES)-(50) > 0   -34.88   1950.39  -205.63   207.74       2.85      0.74     \n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- the difference between the percentage change in estimated $y$ as $x$\n  increases from 2.5 to 5 and 50% is -34.885\n- the evidence ratio in support of the hypothesis that the percentage\n  change exceeds 50% is 2.852\n- the probability that the change in $y$ exceeds 50% is \n  0.74\n- the evidence for such a change is very weak\n  \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1b.brm2 |>\n  emmeans::emmeans(~x, at = list(x = c(2.5, 5))) |>\n  gather_emmeans_draws() |>\n  ungroup() |>\n  group_by(.draw) |>\n  summarise(ES = 100 * diff(.value) / .value[1]) |>\n  summarise_draws(\n    mean, median,\n    HDInterval::hdi,\n    P = ~ mean(. > 50)\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 6\n  variable  mean median lower upper     P\n  <chr>    <dbl>  <dbl> <dbl> <dbl> <dbl>\n1 ES        15.1   84.2 -430.  389. 0.740\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- the percentage change in estimated $y$ as $x$\n  increases from 2.5 to 5 is 15.115\n- the probability that the change in $y$ exceeds 50% is \n  0.74\n- the evidence for such a change is very weak\n\n\n:::\n\n\n##### ROPE\n\nThe procedure highlighted above for calculating excedence\nprobabilities evaluates the degree of evidence for a effect in a\nparticular direction. However, there are other instances where there\nis a desire to evaluate the evidence that something has change (either\nincreased or decreased). Such purposes are similar to the Frequentist\npursuit of testing a null hypothesis (e.g. effect = 0).\n\nThe Region of Practical Equivalence (ROPE) evaluates evidence that an\neffect is \"practically equivalent\" to a value (e.g. 0) by calculating\nthe proportion of effects that are within a nominated range.\n@Kruschke-2018-270 argued that for standardized parameters, the range\nof -0.1 to 0.1 would envelop a negligible effect based on @Cohen-1988.\n@Kruschke-2018-270 also suggested that this range could be extended to\nnon-standardized parameters by multiplying by the standard deviation\nof the response. Accordingly, calculating the proportion of posterior\ndensity within this ROPE could act as a form of \"null-hypothesis\"\ntesting in a Bayesian framework.\n\n- if the HDI of the focal parameter falls completely **outside** the\n  ROPE, there is strong evidence that there **is** an effect\n- if the HDI of the focal parameter falls completely **inside** the\n  ROPE, there is strong evidence that there **is not** an effect\n- otherwise there is not clear evidence either way\n\n::: {.callout-note}\n\nROPE and equivalence tests are of most use when you decide that there\nis not enough evidence to support an hypothesis that there is an\neffect. Such a \"non-significant\" result may be because there genuinely\nis not effect OR you do not have enough power to detect the effect.\nPerforming an equivalence test provides a mechanism to tease these two\nappart.\n\nI provide the following example purely to illustrate how such a test\nwould be performed. In this case, as we have already demonstrated\nstrong evidence for an effect, the equivalence test does not yield any\nadditional insights.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Calculate ROPE range manually\nROPE <- c(-0.1, 0.1) * with(dat, sd(y))\n## OR calculate ROPE range via rope_range function\nROPE <- bayestestR::rope_range(dat1b.brm2)\n\ndat1b.brm2 |> bayestestR::equivalence_test(parameters = \"b_.*x.*\", range = ROPE, ci_method = \"HDI\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Test for Practical Equivalence\n\n  ROPE: [-0.09 0.09]\n\nParameter          |        H0 | inside ROPE |      95% HDI\n-----------------------------------------------------------\nscalexscaleEQFALSE | Undecided |     18.33 % | [-0.97 0.85]\n```\n\n\n:::\n\n```{.r .cell-code}\ndat1b.brm2 |>\n    bayestestR::equivalence_test(parameters = \"b_.*x.*\", range = ROPE, ci_method = \"HDI\") |>\n    plot()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nPicking joint bandwidth of 0.0727\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/rope1b_1-1.png){width=480}\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- the percentage of the HPD for the slope that is inside the ROPE is\n  0.183\n- there is strong evidence for an effect\n\nOR using the `rope` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Calculate ROPE range manually\ndat1b.brm2 |> bayestestR::rope(parameters = \"x\", range = ROPE, ci_method = \"HDI\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Proportion of samples inside the ROPE [-0.09, 0.09]:\n\nParameter          | inside ROPE\n--------------------------------\nscalexscaleEQFALSE |     18.33 %\n```\n\n\n:::\n\n```{.r .cell-code}\ndat1b.brm2 |>\n    bayestestR::rope(parameters = \"x\", range = ROPE, ci_method = \"HDI\") |>\n    plot()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/rope1b_2-1.png){width=480}\n:::\n:::\n\n\n\nThe above demonstration, was applied to the simple comparison that the\nslope was not equal to 0, however, it can similarly be applied to any\nhypothesis (although typically only if there is no evidence of an\neffect)\n\n:::::\n\n#### Standardised predictor\n::::: {.panel-tabset}\n\n##### Specific hypotheses\n\nNow that we have the full posteriors, we are free to use these to\ngarner evidence on a range of hypotheses. To demonstrate, we will\nconsider the following hypotheses:\n\n1. a change in $x$ is associated with an increase in $y$\n2. a doubling of $x$ (from 2.5 to 5) is associated with an increase in $y$ of > 50%\n\n::: {.panel-tabset .nav-pills}\n###### 1. Positive relationship\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1c.brm2 |> get_variables()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"b_Intercept\"     \"b_scalex\"        \"sigma\"           \"Intercept\"      \n [5] \"prior_Intercept\" \"prior_b\"         \"prior_sigma\"     \"lprior\"         \n [9] \"lp__\"            \"accept_stat__\"   \"stepsize__\"      \"treedepth__\"    \n[13] \"n_leapfrog__\"    \"divergent__\"     \"energy__\"       \n```\n\n\n:::\n\n```{.r .cell-code}\ndat1c.brm2 |> hypothesis(\"scalex > 0\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nHypothesis Tests for class b:\n    Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1 (scalex) > 0    -0.07      0.41    -0.71     0.58       0.72      0.42     \n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1c_hyp <- dat1c.brm2 |>\n    hypothesis(\"scalex > 0\") |>\n    _[[\"hypothesis\"]]\n```\n:::\n\n\n\n**Conclusions**:\n\n- the parameter (`b_scalex`) minus 0 is -0.069\n- `Evid.Ratio`: the ratio of evidence for the hypothesis vs the\n  evidence against it.  In this case, the evidence ratio is \n  0.72 - `Inf` is because the divisor was 0 \n  (no evidence against the hypothesis).\n- `Post.Prob`: the probability of the hypothesis is 0.419\n- there is very high evidence for this hypothesis\n\nAlternatively, we could use `gather_draws` to achieve a similar\noutcome.\n\nIn the following, in addition to median and HPD intervals, we will\ncalculate the probability that the slope (`b_x`) is greater than 0. To\ncalculate such a probability, we could simply count up the number of\nposterior `b_x` values that are greater than zero and then divide by\nthe total number of posterior `b_x` values. In R, we could do this as\n`sum(b_x > 0)/length(b_x)` (where `b_x > 0` will return either a 1 for\neach case it is true and a 0 when it is false, and thus summing is\nlike counting). Dividing a sum by its length equates to a mean and\nthus we can achieve the probability by calcualing the mean of `b_x >\n0`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1c.brm2 |>\n  gather_draws(`b_.*x`, regex = TRUE) |>\n  summarise_draws(median,\n    HDInterval::hdi,\n    P = ~mean(. > 0)\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 6\n# Groups:   .variable [1]\n  .variable variable  median  lower upper     P\n  <chr>     <chr>      <dbl>  <dbl> <dbl> <dbl>\n1 b_scalex  .value   -0.0750 -0.813 0.752 0.419\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1c_hyp2 <- dat1c.brm2 |>\n    gather_draws(`b_.*x`, regex = TRUE) |>\n    summarise_draws(median,\n        HDInterval::hdi,\n        P = ~mean(. > 0)\n    )\n```\n:::\n\n\n::: {.callout-tip}\n\nThe `summarise_draws()` function expects a set of one or more summary\nor diagnostic functions (such as `median` etc). These can be supplied\neither as the name of the function (as in the case for `median` in the\nexample above) or if more arguments or information is required by the\nfunction, the function can be written out in full. in this case, the\nfunction must be proceeded with a `~` and the variable is denoted a\n`.` (such as in `P = ~mean(. > 0)` above).\n\n:::\n\n**Conclusions**:\n\n- the parameter (`b_scalex`) minus 0 is -0.075\n- `P`: the probability of the hypothesis is 0.72\n- there is very high evidence for this hypothesis\n\n\n\n###### 2. >50% increase when $x$ doubles\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1c.brm2 |>\n    emmeans::emmeans(~x, at = list(x = c(2.5, 5))) |>\n    gather_emmeans_draws() |>\n    ungroup() |>\n    group_by(.draw) |>\n  summarise(ES = 100 * diff(.value) / .value[1]) |>\n  hypothesis(\"ES > 50\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nHypothesis Tests for class :\n     Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1 (ES)-(50) > 0  -214.24  10928.71  -188.13   241.73       3.02      0.75     \n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- the difference between the percentage change in estimated $y$ as $x$\n  increases from 2.5 to 5 and 50% is -214.236\n- the evidence ratio in support of the hypothesis that the percentage\n  change exceeds 50% is 3.02\n- the probability that the change in $y$ exceeds 50% is \n  0.751\n- the evidence for such a change is very weak\n  \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1c.brm2 |>\n  emmeans::emmeans(~x, at = list(x = c(2.5, 5))) |>\n  gather_emmeans_draws() |>\n  ungroup() |>\n  group_by(.draw) |>\n  summarise(ES = 100 * diff(.value) / .value[1]) |>\n  summarise_draws(\n    mean, median,\n    HDInterval::hdi,\n    P = ~ mean(. > 50)\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 6\n  variable  mean median lower upper     P\n  <chr>    <dbl>  <dbl> <dbl> <dbl> <dbl>\n1 ES       -164.   84.8 -312.  498. 0.751\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- the percentage change in estimated $y$ as $x$\n  increases from 2.5 to 5 is -164.236\n- the probability that the change in $y$ exceeds 50% is \n  0.751\n- the evidence for such a change is very weak\n\n\n:::\n\n\n##### ROPE\n\nThe procedure highlighted above for calculating excedence\nprobabilities evaluates the degree of evidence for a effect in a\nparticular direction. However, there are other instances where there\nis a desire to evaluate the evidence that something has change (either\nincreased or decreased). Such purposes are similar to the Frequentist\npursuit of testing a null hypothesis (e.g. effect = 0).\n\nThe Region of Practical Equivalence (ROPE) evaluates evidence that an\neffect is \"practically equivalent\" to a value (e.g. 0) by calculating\nthe proportion of effects that are within a nominated range.\n@Kruschke-2018-270 argued that for standardized parameters, the range\nof -0.1 to 0.1 would envelop a negligible effect based on @Cohen-1988.\n@Kruschke-2018-270 also suggested that this range could be extended to\nnon-standardized parameters by multiplying by the standard deviation\nof the response. Accordingly, calculating the proportion of posterior\ndensity within this ROPE could act as a form of \"null-hypothesis\"\ntesting in a Bayesian framework.\n\n- if the HDI of the focal parameter falls completely **outside** the\n  ROPE, there is strong evidence that there **is** an effect\n- if the HDI of the focal parameter falls completely **inside** the\n  ROPE, there is strong evidence that there **is not** an effect\n- otherwise there is not clear evidence either way\n\n::: {.callout-note}\n\nROPE and equivalence tests are of most use when you decide that there\nis not enough evidence to support an hypothesis that there is an\neffect. Such a \"non-significant\" result may be because there genuinely\nis not effect OR you do not have enough power to detect the effect.\nPerforming an equivalence test provides a mechanism to tease these two\nappart.\n\nI provide the following example purely to illustrate how such a test\nwould be performed. In this case, as we have already demonstrated\nstrong evidence for an effect, the equivalence test does not yield any\nadditional insights.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Calculate ROPE range manually\nROPE <- c(-0.1, 0.1) * with(dat, sd(y))\n## OR calculate ROPE range via rope_range function\nROPE <- bayestestR::rope_range(dat1c.brm2)\n\ndat1c.brm2 |> bayestestR::equivalence_test(parameters = \"b_.*x.*\", range = ROPE, ci_method = \"HDI\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Test for Practical Equivalence\n\n  ROPE: [-0.09 0.09]\n\nParameter |        H0 | inside ROPE |      95% HDI\n--------------------------------------------------\nscalex    | Undecided |     19.43 % | [-0.87 0.71]\n```\n\n\n:::\n\n```{.r .cell-code}\ndat1c.brm2 |>\n    bayestestR::equivalence_test(parameters = \"b_.*x.*\", range = ROPE, ci_method = \"HDI\") |>\n    plot()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nPicking joint bandwidth of 0.0637\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/rope1c_1-1.png){width=480}\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- the percentage of the HPD for the slope that is inside the ROPE is\n  0.194\n- there is strong evidence for an effect\n\nOR using the `rope` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Calculate ROPE range manually\ndat1c.brm2 |> bayestestR::rope(parameters = \"x\", range = ROPE, ci_method = \"HDI\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Proportion of samples inside the ROPE [-0.09, 0.09]:\n\nParameter | inside ROPE\n-----------------------\nscalex    |     19.42 %\n```\n\n\n:::\n\n```{.r .cell-code}\ndat1c.brm2 |>\n    bayestestR::rope(parameters = \"x\", range = ROPE, ci_method = \"HDI\") |>\n    plot()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/rope1c_2-1.png){width=480}\n:::\n:::\n\n\n\nThe above demonstration, was applied to the simple comparison that the\nslope was not equal to 0, however, it can similarly be applied to any\nhypothesis (although typically only if there is no evidence of an\neffect)\n\n:::::\n\n:::::::\n\n### Example 2 (categorical predictor)\n\n::::::: {.panel-tabset}\n#### Treatment contrasts\n::::: {.panel-tabset}\n\n##### Specific hypotheses\n\nNow that we have the full posteriors, we are free to use these to\ngarner evidence on a range of hypotheses. To demonstrate, we will\nconsider the following hypotheses:\n\n1. all pairwise comparisons (compare each level of $x$ to each other\n2. define a specific set of contrasts that include comparing the\n   average of medium and high treatments to the control treatment.\n\n::: {.panel-tabset .nav-pills}\n\n###### 1. pairwise contrasts\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2a.brm2 |>\n    emmeans(~x) |>\n    pairs()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n contrast         estimate lower.HPD upper.HPD\n control - medium    0.889     -4.89      6.44\n control - high      8.726      2.38     14.76\n medium - high       7.980      1.84     14.10\n\nPoint estimate displayed: median \nHPD interval probability: 0.95 \n```\n\n\n:::\n:::\n\n\n\nOr if we want the full posteriors... This option allows us to\ncalculate exceedence probabilities. That is, we can calculate the\nproportion of contrast posteriors that exceed a specific value (as a\nhypothesis). In this case, we will calculate two exceedence\nprobabilities:\n\n1. probability that the effect is negative (e.g. proportion of\n   probabilities that are less than 0)\n2. probability that the effect is positive (e.g. proportion of\n   probabilities that are greater than 0)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2a.brm2 |>\n    emmeans(~x) |>\n    pairs() |>\n    gather_emmeans_draws() |>\n    dplyr::select(-.chain) |> \n    summarise_draws(median,\n        HDInterval::hdi,\n        Pl = ~ mean(.x < 0),\n        Pg = ~ mean(.x > 0)\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 7\n# Groups:   contrast [3]\n  contrast         variable median lower upper      Pl    Pg\n  <chr>            <chr>     <dbl> <dbl> <dbl>   <dbl> <dbl>\n1 control - high   .value    8.73   2.38 14.8  0.00417 0.996\n2 control - medium .value    0.889 -4.89  6.44 0.373   0.627\n3 medium - high    .value    7.98   1.84 14.1  0.0075  0.992\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- the difference in $y$ between \"control\" and \"medium\" is \n  0.89, however there is no evidence of this\n  effect (exceedence probability)\n- the difference in $y$ between \"control\" and \"high\" is\n  8.73, and there is very strong evidence for this effect\n- the difference in $y$ between \"medium\" and \"high\" is\n  7.98, and there is very strong evidence for this effect\n\nIt is also possible to express the magnitude of effect in percentage\nchange. The trick is to put the emmeans parameters onto a logarithmic\nscale so that the pairwise comparisons (which are a subtraction)\neffectively are treated as divisions (due to log laws).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2a.brm2 |>\n    emmeans(~x) |>\n    regrid(transform = \"log\") |>\n    pairs() |>\n    regrid()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n contrast       ratio lower.HPD upper.HPD\n control/medium  1.04     0.755      1.34\n control/high    1.72     1.078      2.55\n medium/high     1.66     0.996      2.43\n\nPoint estimate displayed: median \nHPD interval probability: 0.95 \n```\n\n\n:::\n:::\n\n\n\n::: {.collout-info}\n\nThe estimates are expressed as fractional changes. A \"ratio\" of 1\nindicates parity, since if you multiply something by 1, it does not\nchange. A value of 1.5, indicates a 50% increase and a value of 0.5\nindicates a 50% decline.\n\nTo calculate percentage change from a fractional value, subtract 1 and\nmultiply the result by 100. E.g.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(1.19 - 1) * 100\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 19\n```\n\n\n:::\n:::\n\n\n\n:::\n\nIf we get the full posteriors, we can also explore whether the change\nexceeds some ecologically important change (such as 20%)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2a.brm2 |>\n  emmeans(~x) |>\n  regrid(transform = \"log\") |>\n  pairs() |>\n  regrid() |> \n  gather_emmeans_draws() |>\n  dplyr::select(-.chain) |>\n  summarise_draws(median,\n    HDInterval::hdi,\n    Pl = ~ mean(.x < 1),\n    Pg = ~ mean(.x > 1),\n    Pl50 = ~ mean(.x < 0.8),\n    Pg50 = ~ mean(.x > 1.2)\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 9\n# Groups:   contrast [3]\n  contrast       variable median lower upper      Pl    Pg     Pl50  Pg50\n  <chr>          <chr>     <dbl> <dbl> <dbl>   <dbl> <dbl>    <dbl> <dbl>\n1 control/high   .value     1.72 1.08   2.55 0.00417 0.996 0.000417 0.962\n2 control/medium .value     1.04 0.755  1.34 0.373   0.627 0.0383   0.145\n3 medium/high    .value     1.66 0.996  2.43 0.0075  0.992 0.000417 0.944\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- the response $y$ is 72.25%\n  higher in the control group over the high group.\n- there is strong evidence (P = 1) of the\n  above\n- there is also strong evidence (P = 1)\n  that $y$ is at least 20% higher in the control group over the high\n  group\n- the response $y$ is 4.378%\n  higher in the control group over the medium group.\n- there is no evidence (P = 0.627) of the\n  above\n- there is no evidence (P = 0.145) that $y$\n  is at least 20% higher in the control group over the medium group\n- the response $y$ is 65.861%\n  higher in the medium group over the high group.\n- there is strong evidence (P = 0.993) of\n  the above\n- there is no evidence (P = 0.944) that $y$\n  is at least 20% higher in the medium group over the high group\n\n\n###### 2. planned contrasts\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncmat <- cbind(\n  \"Contrast vs Medium/High\" = c(1, -0.5, -0.5),\n  \"Medium vs High\" = c(0, 1, -1)\n)\ndat2a.brm2 |>\n    emmeans(~x) |>\n    contrast(method = list(x = cmat))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n contrast                  estimate lower.HPD upper.HPD\n x.Contrast vs Medium/High     4.79    -0.215      9.82\n x.Medium vs High              7.98     1.840     14.10\n\nPoint estimate displayed: median \nHPD interval probability: 0.95 \n```\n\n\n:::\n:::\n\n\n\nOr with full posteriors and exceedance probabilities...\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2a.brm2 |>\n    emmeans(~x) |>\n    contrast(method = list(x = cmat)) |> \n    gather_emmeans_draws() |>\n    dplyr::select(-.chain) |> \n    summarise_draws(median,\n        HDInterval::hdi,\n        Pl = ~ mean(.x < 0),\n        Pg = ~ mean(.x > 0)\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 7\n# Groups:   contrast [2]\n  contrast                  variable median  lower upper     Pl    Pg\n  <chr>                     <chr>     <dbl>  <dbl> <dbl>  <dbl> <dbl>\n1 x.Contrast vs Medium/High .value     4.79 -0.215  9.82 0.035  0.965\n2 x.Medium vs High          .value     7.98  1.84  14.1  0.0075 0.992\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusions**:\n\n- on average, $y$ is 4.79 higher in the\n  \"control\" group than the \"mediun\" and \"high\" groups.\n- the evidence for this effect is very strong 0.965\n\n\n:::\n\n##### ROPE\n\nWe have already seen that there is no evidence of a difference in $y$\nbetween the \"control\" and \"medium\" groups. This could be because\neither there is not enough power to detect the difference or that the\npopulations are not different. It would be nice to be able to gain\nsome insights into which of these is most likely. And we can. If we\nestablish the range of values that represent an insubstantial effect,\nwe can then quantify the proportion of the posterior that falls inside\nthis Region of Practical Equivalence (ROPE).\n\nConventionally, the ROPE represents within 10% - that is, if the\neffect is less than 10% change, then we might consider it\ninsubstantial.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Calculate ROPE range manually\nROPE <- c(-0.1, 0.1) * with(dat2, sd(y))\n## OR calculate ROPE range via rope_range function\nROPE <- bayestestR::rope_range(dat2a.brm2)\n\ndat2a.brm2 |> bayestestR::equivalence_test(parameters = \"b_x\", range = ROPE, ci_method = \"HDI\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Test for Practical Equivalence\n\n  ROPE: [-0.60 0.60]\n\nParameter |        H0 | inside ROPE |        95% HDI\n----------------------------------------------------\nxmedium   | Undecided |     17.50 % | [ -6.31  5.12]\nxhigh     |  Rejected |      0.00 % | [-14.74 -2.34]\n```\n\n\n:::\n\n```{.r .cell-code}\ndat2a.brm2 |>\n    bayestestR::equivalence_test(parameters = \"b_x\", range = ROPE, ci_method = \"HDI\") |>\n    plot()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nPicking joint bandwidth of 0.484\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/rope2a_1-1.png){width=480}\n:::\n:::\n\n\n\n**Conclusions**:\n\n- there is insufficient evidence to conclude that there is a\n  difference in $y$ between \"control\" and \"medium\" groups\n- we cannot conclude that there is evidence of no effect\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n## Calculate ROPE range manually\ndat2a.brm2 |> bayestestR::rope(parameters = \"x\", range = ROPE, ci_method = \"HDI\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Proportion of samples inside the ROPE [-0.60, 0.60]:\n\nParameter | inside ROPE\n-----------------------\nxmedium   |     17.49 %\nxhigh     |      0.00 %\n```\n\n\n:::\n\n```{.r .cell-code}\ndat2a.brm2 |>\n    bayestestR::rope(parameters = \"x\", range = ROPE, ci_method = \"HDI\") |>\n    plot()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/rope2a_2-1.png){width=480}\n:::\n:::\n\n\n\n\n\n:::::\n:::::::\n::::\n\n# Summary plots\n\n:::: {.panel-tabset}\n\n### Example 1 (Gaussian data)\n\n::::::: {.panel-tabset}\n#### Raw predictor\n::::: {.panel-tabset}\n\n##### Partial plot\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1a.grid <- list(x = modelr::seq_range(dat$x, n = 100))\ndat1a.brm2 |>\n    emmeans(~x, at = dat1a.grid) |>\n    as.data.frame() |>\n    ggplot(aes(y = emmean, x = x)) +\n    geom_ribbon(aes(ymin = lower.HPD, ymax = upper.HPD), fill = \"orange\", alpha = 0.3) +\n    geom_point(data = dat, aes(y = y)) +\n    geom_line() +\n    theme_classic()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/summary_plot1a-1.png){width=480}\n:::\n:::\n\n\n\nAs a spaghetti plot\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1a.grid <- list(x = modelr::seq_range(dat$x, n = 100))\ndat1a.brm2 |>\n    emmeans(~x, at = dat1a.grid) |>\n    gather_emmeans_draws() |> \n    ggplot(aes(y = .value, x = x)) +\n    geom_line(aes(group=.draw),  colour = 'orange', alpha=0.01) +\n    geom_point(data = dat, aes(y = y)) +\n    theme_classic()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/summary_plot1a_2-1.png){width=480}\n:::\n:::\n\n\n\n\n:::::\n\n#### Centered predictor\n::::: {.panel-tabset}\n\n##### Partial plot\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1b.grid <- list(x = modelr::seq_range(dat$x, n = 100))\ndat1b.brm2 |>\n    emmeans(~x, at = dat1b.grid) |>\n    as.data.frame() |>\n    ggplot(aes(y = emmean, x = x)) +\n    geom_ribbon(aes(ymin = lower.HPD, ymax = upper.HPD), fill = \"orange\", alpha = 0.3) +\n    geom_point(data = dat, aes(y = y)) +\n    geom_line() +\n    theme_classic()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/summary_plot1b-1.png){width=480}\n:::\n:::\n\n\n\nAs a spaghetti plot\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1b.grid <- list(x = modelr::seq_range(dat$x, n = 100))\ndat1b.brm2 |>\n    emmeans(~x, at = dat1b.grid) |>\n    gather_emmeans_draws() |> \n    ggplot(aes(y = .value, x = x)) +\n    geom_line(aes(group=.draw),  colour = 'orange', alpha=0.01) +\n    geom_point(data = dat, aes(y = y)) +\n    theme_classic()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/summary_plot1b_2-1.png){width=480}\n:::\n:::\n\n\n\n\n:::::\n\n#### Standardised predictor\n::::: {.panel-tabset}\n\n##### Partial plot\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1c.grid <- list(x = modelr::seq_range(dat$x, n = 100))\ndat1c.brm2 |>\n    emmeans(~x, at = dat1c.grid) |>\n    as.data.frame() |>\n    ggplot(aes(y = emmean, x = x)) +\n    geom_ribbon(aes(ymin = lower.HPD, ymax = upper.HPD), fill = \"orange\", alpha = 0.3) +\n    geom_point(data = dat, aes(y = y)) +\n    geom_line() +\n    theme_classic()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/summary_plot1c-1.png){width=480}\n:::\n:::\n\n\n\nAs a spaghetti plot\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1c.grid <- list(x = modelr::seq_range(dat$x, n = 100))\ndat1c.brm2 |>\n    emmeans(~x, at = dat1c.grid) |>\n    gather_emmeans_draws() |> \n    ggplot(aes(y = .value, x = x)) +\n    geom_line(aes(group=.draw),  colour = 'orange', alpha=0.01) +\n    geom_point(data = dat, aes(y = y)) +\n    theme_classic()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/summary_plot1c_2-1.png){width=480}\n:::\n:::\n\n\n\n\n:::::\n\n:::::::\n\n### Example 2 (categorical predictor)\n\n::::::: {.panel-tabset}\n#### Treatment contrasts\n::::: {.panel-tabset}\n\n##### Partial plot\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2a.brm2 |>\n  emmeans(~x) |>\n  as.data.frame() |>\n  ggplot(aes(y = emmean, x = x)) +\n  geom_pointrange(aes(ymin = lower.HPD, ymax = upper.HPD)) +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](35_bayesian_glm_files/figure-html/summary_plot2a-1.png){width=480}\n:::\n:::\n\n\n\n:::::\n\n:::::::\n\n::::\n\n\n# References\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}